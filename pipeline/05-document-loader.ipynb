{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Be careful not to import anything related to Snorkel here as it will crash pyarrow on staging step *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dotenv\n",
    "import shutil\n",
    "import glob\n",
    "import tqdm\n",
    "from tcre.env import *\n",
    "import os.path as osp\n",
    "# Flag for whether new docs should be added to existing database or \n",
    "# if all documents within database should be removed first\n",
    "clear_docs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy.orm import sessionmaker\n",
    "# from snorkel import SnorkelSession\n",
    "# session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 69135 entries, 0 to 48693\n",
      "Data columns (total 18 columns):\n",
      "abstract          67211 non-null object\n",
      "arch_archive      48685 non-null object\n",
      "arch_id           48685 non-null object\n",
      "arch_name         48685 non-null object\n",
      "arch_path         48685 non-null object\n",
      "arch_venue        48685 non-null object\n",
      "body              58252 non-null object\n",
      "date_accepted     69135 non-null object\n",
      "date_pub          69135 non-null object\n",
      "date_received     69135 non-null object\n",
      "id_doi            65905 non-null object\n",
      "id_pmc            69135 non-null object\n",
      "id_pmid           68306 non-null object\n",
      "journal_ids       69135 non-null object\n",
      "journal_titles    69135 non-null object\n",
      "src               69135 non-null object\n",
      "text              20450 non-null object\n",
      "title             69133 non-null object\n",
      "dtypes: object(18)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    pd.read_parquet(osp.join(IMPORT_DATA_DIR_02, 'corpus_01.parquet')).assign(src='entrez'),\n",
    "    pd.read_parquet(osp.join(IMPORT_DATA_DIR_03, 'corpus_03.parquet')).assign(src='pmcoa')\n",
    "], sort=True)\n",
    "assert df['id_pmc'].notnull().all()\n",
    "df = df.drop_duplicates(subset=['src', 'id_pmc'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entrez</th>\n",
       "      <th>pmcoa</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entrez  pmcoa  count\n",
       "0       0      1  43824\n",
       "1       1      0  15589\n",
       "2       1      1   4861"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show document intersection\n",
    "cts = df.assign(ind=1).pivot(index='id_pmc', columns='src', values='ind').fillna(0).astype(int)\n",
    "cts.groupby(['entrez', 'pmcoa']).size().rename('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    59413\n",
       "True      4861\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cts > 0).all(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupe_ids = cts[(cts > 0).all(axis=1)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 64274 entries, 0 to 48693\n",
      "Data columns (total 18 columns):\n",
      "abstract          62360 non-null object\n",
      "arch_archive      43824 non-null object\n",
      "arch_id           43824 non-null object\n",
      "arch_name         43824 non-null object\n",
      "arch_path         43824 non-null object\n",
      "arch_venue        43824 non-null object\n",
      "body              53411 non-null object\n",
      "date_accepted     64274 non-null object\n",
      "date_pub          64274 non-null object\n",
      "date_received     64274 non-null object\n",
      "id_doi            61191 non-null object\n",
      "id_pmc            64274 non-null object\n",
      "id_pmid           63447 non-null object\n",
      "journal_ids       64274 non-null object\n",
      "journal_titles    64274 non-null object\n",
      "src               64274 non-null object\n",
      "text              20450 non-null object\n",
      "title             64272 non-null object\n",
      "dtypes: object(18)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Prefer entrez doc for duplicates (make docs unique)\n",
    "dff = df[(df['src'] == 'entrez') | ~df['id_pmc'].isin(dupe_ids)]\n",
    "assert dff['id_pmc'].is_unique\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src\n",
       "entrez    20450\n",
       "pmcoa     43824\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc count by source\n",
    "dff.groupby('src').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-text    53309\n",
      "Abstract     10965\n",
      "Name: body, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "src              \n",
       "entrez  Abstract     10582\n",
       "        Full-text     9868\n",
       "pmcoa   Full-text    43441\n",
       "        Abstract       383\n",
       "Name: body, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abstract only vs full text counts\n",
    "def count_article_type(g):\n",
    "    return (g['body'].fillna('').str.len() > 0).map({True: 'Full-text', False: 'Abstract'}).value_counts()\n",
    "print(count_article_type(dff))\n",
    "dff.groupby('src').apply(count_article_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 0]               10965\n",
       "(0, 1]                    0\n",
       "(1, 10]                   2\n",
       "(10, 100]                 2\n",
       "(100, 1000]              26\n",
       "(1000, 10000]          2528\n",
       "(10000, 50000]        44557\n",
       "(50000, 100000000]     6194\n",
       "Name: body, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(dff['body'].fillna('').str.len(), [-1, 0, 1, 10, 100, 1000, 10000, 50000, 100000000]).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import Document\n",
    "\n",
    "def get_doc_ids():\n",
    "    session = SnorkelSession()\n",
    "    ids = [r[0] for r in session.query(Document.name).all()]\n",
    "    session.close()\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9549 existing docs\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54725 entries, 1784 to 48693\n",
      "Data columns (total 18 columns):\n",
      "abstract          52919 non-null object\n",
      "arch_archive      41938 non-null object\n",
      "arch_id           41938 non-null object\n",
      "arch_name         41938 non-null object\n",
      "arch_path         41938 non-null object\n",
      "arch_venue        41938 non-null object\n",
      "body              47545 non-null object\n",
      "date_accepted     54725 non-null object\n",
      "date_pub          54725 non-null object\n",
      "date_received     54725 non-null object\n",
      "id_doi            52087 non-null object\n",
      "id_pmc            54725 non-null object\n",
      "id_pmid           53932 non-null object\n",
      "journal_ids       54725 non-null object\n",
      "journal_titles    54725 non-null object\n",
      "src               54725 non-null object\n",
      "text              12787 non-null object\n",
      "title             54723 non-null object\n",
      "dtypes: object(18)\n",
      "memory usage: 7.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Remove records for documents that are already imported \n",
    "if not clear_docs:\n",
    "    doc_ids = get_doc_ids()\n",
    "    print(f'Found {len(doc_ids)} existing docs')\n",
    "    n = len(dff)\n",
    "    dff = dff[~('PMC' + dff['id_pmc']).isin(doc_ids)]\n",
    "    assert n - len(dff) == len(doc_ids)\n",
    "    print(dff.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['entrez', 'pmcoa'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff['src'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:07<00:00, 19.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def stage_docs(df, batch_size=500, staging_dir='/tmp/corpus_staging'):\n",
    "    if osp.exists(staging_dir):\n",
    "        shutil.rmtree(staging_dir)\n",
    "    os.makedirs(staging_dir)\n",
    "\n",
    "    n_batches = len(df) // batch_size\n",
    "    batches = np.array_split(np.arange(len(df)), n_batches)\n",
    "    for i, batch in tqdm.tqdm(enumerate(batches), total=len(batches)):\n",
    "        path = osp.join(staging_dir, 'B{:05d}.feather'.format(i))\n",
    "        df.iloc[batch].reset_index(drop=True).to_feather(path)\n",
    "\n",
    "stage_docs(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Jobs To Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence, Candidate\n",
    "from dask.distributed import Client, progress\n",
    "from tcre import processing\n",
    "from tcre import supervision\n",
    "import dask\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_jobs(staging_dir='/tmp/corpus_staging'):\n",
    "    jobs = []\n",
    "    for f in os.listdir(staging_dir):\n",
    "        i = int(f.split('.')[0][1:])\n",
    "        jobs.append((i, osp.join(staging_dir, f)))\n",
    "    return sorted(jobs)\n",
    "jobs = get_jobs()\n",
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs = [jobs[i] for i in [  \n",
    "#     0,   1,   2,   3,   4,   5,   6,   8,   9,  11,  12,  13,  14,\n",
    "# ]]\n",
    "# len(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Existing Contexts (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_ct():\n",
    "    session = SnorkelSession()\n",
    "    ct = session.query(Document).count()\n",
    "    session.close()\n",
    "    return ct\n",
    "    \n",
    "def clear_documents():\n",
    "    session = SnorkelSession()\n",
    "    parser = CorpusParser(parser=lambda v: None)\n",
    "    parser.clear(session)\n",
    "    session.commit()\n",
    "    session.close()\n",
    "    \n",
    "if clear_docs:\n",
    "    n = get_doc_ct()\n",
    "    clear_documents()\n",
    "    print('Num docs before =', n, 'after =', get_doc_ct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:40059\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37545'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:45901'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34085'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37201'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46263'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34677'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:40001'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38443'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37883'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:44709'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46105'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46173'\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:40579\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:40579\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:37381\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:39443\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:42835\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:35871\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:35839\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:38181\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:39979\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:46351\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:36931\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:38737\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:35359\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:37381\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:39443\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:42835\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:35871\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:35839\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:38181\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:39979\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:46351\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:36931\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:38737\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:35359\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:45901'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37545'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37201'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46105'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37883'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46263'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46173'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34677'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:44709'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38443'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34085'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:40001'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37545'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:45901'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34085'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37201'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46263'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34677'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:40001'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38443'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37883'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:44709'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46105'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:46173'\n",
      "distributed.scheduler - INFO - Receive client connection: Client-fb44e780-d95c-11e9-8a7f-0242ac110002\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:40059\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>12</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>3.07 TB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:40059' processes=12 cores=12>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(\n",
    "    threads_per_worker=1, n_workers=12, processes=True, \n",
    "    memory_limit='256GB', direct_to_workers=True, \n",
    "    silence_logs=logging.INFO\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(job):\n",
    "    import logging\n",
    "    logging.basicConfig()\n",
    "    batch_id, batch_file = job\n",
    "    logging.info('Processing job %s (%s)', batch_id, batch_file)\n",
    "    loader = processing.DocLoader(batch_file)\n",
    "    loader.run(limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea3676a0d1d45e28587b12c0d34c03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time estimates:\n",
    "# docs=20450 -> time=2hr 22min\n",
    "futures = client.map(process, jobs)\n",
    "dask.distributed.progress(futures, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dask.distributed.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([f.status == 'finished' for f in futures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([f.status == 'finished' for f in futures]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere([f.status != 'finished' for f in futures]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.6/site-packages/distributed/client.py:1338: UserWarning: Shutdown is deprecated.  Please use close instead\n",
      "  warnings.warn(\"Shutdown is deprecated.  Please use close instead\")\n",
      "distributed.scheduler - INFO - Scheduler closing...\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:36931\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:36931\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:35359\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:35359\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:35839\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:35839\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:46351\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:46351\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:38737\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:38737\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:38181\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:38181\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:42835\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:42835\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:40579\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:40579\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:39979\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:39979\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:39443\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:39443\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:35871\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:35871\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:37381\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:37381\n",
      "distributed.scheduler - INFO - Lost all workers\n",
      "distributed.scheduler - INFO - Scheduler closing all comms\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:34677'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:46263'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:45901'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:37201'\n",
      "distributed.batched - INFO - Batched Comm Closed: in <closed TCP>: Stream is closed\n",
      "distributed.batched - INFO - Batched Comm Closed: in <closed TCP>: Stream is closed\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:37545'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:34085'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:40001'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:37883'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:46105'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:44709'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:38443'\n",
      "distributed.nanny - INFO - Closing Nanny at 'tcp://172.17.0.2:46173'\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64274"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = SnorkelSession()\n",
    "session.query(Document).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
