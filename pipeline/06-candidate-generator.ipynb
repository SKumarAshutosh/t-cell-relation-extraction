{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Generator\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Load all documents \n",
    "    - Note: rather than streaming them, they are loaded into memory due to pyarrow segmentation faults when used in conjunction with snorkel -- most likely due to pytorch imports\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Be careful not to import anything related to Snorkel here as it will crash pyarrow on staging step *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dotenv\n",
    "import shutil\n",
    "import glob\n",
    "import tqdm\n",
    "from tcre.env import *\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy.orm import sessionmaker\n",
    "# from snorkel import SnorkelSession\n",
    "# session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 69135 entries, 0 to 48693\n",
      "Data columns (total 17 columns):\n",
      "abstract          67211 non-null object\n",
      "arch_archive      48685 non-null object\n",
      "arch_id           48685 non-null object\n",
      "arch_name         48685 non-null object\n",
      "arch_path         48685 non-null object\n",
      "arch_venue        48685 non-null object\n",
      "body              58206 non-null object\n",
      "date_accepted     69135 non-null object\n",
      "date_pub          69135 non-null object\n",
      "date_received     69135 non-null object\n",
      "id_doi            65905 non-null object\n",
      "id_pmc            69135 non-null object\n",
      "id_pmid           68306 non-null object\n",
      "journal_ids       69135 non-null object\n",
      "journal_titles    69135 non-null object\n",
      "src               69135 non-null object\n",
      "title             69133 non-null object\n",
      "dtypes: object(17)\n",
      "memory usage: 9.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    pd.read_parquet(osp.join(IMPORT_DATA_DIR_02, 'corpus_01.parquet')).assign(src='entrez'),\n",
    "    pd.read_parquet(osp.join(IMPORT_DATA_DIR_03, 'corpus_03.parquet')).assign(src='pmcoa')\n",
    "], sort=True)\n",
    "assert df['id_pmc'].notnull().all()\n",
    "df = df.drop_duplicates(subset=['src', 'id_pmc'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entrez</th>\n",
       "      <th>pmcoa</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entrez  pmcoa  count\n",
       "0       0      1  43824\n",
       "1       1      0  15589\n",
       "2       1      1   4861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show document intersection\n",
    "cts = df.assign(ind=1).pivot(index='id_pmc', columns='src', values='ind').fillna(0).astype(int)\n",
    "cts.groupby(['entrez', 'pmcoa']).size().rename('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    59413\n",
       "True      4861\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cts > 0).all(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupe_ids = cts[(cts > 0).all(axis=1)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 64274 entries, 0 to 48693\n",
      "Data columns (total 17 columns):\n",
      "abstract          62360 non-null object\n",
      "arch_archive      43824 non-null object\n",
      "arch_id           43824 non-null object\n",
      "arch_name         43824 non-null object\n",
      "arch_path         43824 non-null object\n",
      "arch_venue        43824 non-null object\n",
      "body              53365 non-null object\n",
      "date_accepted     64274 non-null object\n",
      "date_pub          64274 non-null object\n",
      "date_received     64274 non-null object\n",
      "id_doi            61191 non-null object\n",
      "id_pmc            64274 non-null object\n",
      "id_pmid           63447 non-null object\n",
      "journal_ids       64274 non-null object\n",
      "journal_titles    64274 non-null object\n",
      "src               64274 non-null object\n",
      "title             64272 non-null object\n",
      "dtypes: object(17)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Prefer entrez doc for duplicates (make docs unique)\n",
    "dff = df[(df['src'] == 'entrez') | ~df['id_pmc'].isin(dupe_ids)]\n",
    "assert dff['id_pmc'].is_unique\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:08<00:00, 15.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def stage_docs(df, batch_size=500, staging_dir='/tmp/corpus_staging'):\n",
    "    if osp.exists(staging_dir):\n",
    "        shutil.rmtree(staging_dir)\n",
    "    os.makedirs(staging_dir)\n",
    "\n",
    "    n_batches = len(df) // batch_size\n",
    "    batches = np.array_split(np.arange(len(df)), n_batches)\n",
    "    for i, batch in tqdm.tqdm(enumerate(batches), total=len(batches)):\n",
    "        path = osp.join(staging_dir, 'B{:05d}.feather'.format(i))\n",
    "        df.iloc[batch].reset_index(drop=True).to_feather(path)\n",
    "stage_docs(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Jobs To Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence, Candidate\n",
    "from dask.distributed import Client, progress\n",
    "from tcre import processing\n",
    "from tcre import supervision\n",
    "import dask\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_jobs(staging_dir='/tmp/corpus_staging'):\n",
    "    jobs = []\n",
    "    for f in os.listdir(staging_dir):\n",
    "        i = int(f.split('.')[0][1:])\n",
    "        jobs.append((i, osp.join(staging_dir, f)))\n",
    "    return sorted(jobs)\n",
    "jobs = get_jobs()\n",
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs = [jobs[i] for i in [  \n",
    "#     0,   1,   2,   3,   4,   5,   6,   8,   9,  11,  12,  13,  14,\n",
    "# ]]\n",
    "# len(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Existing Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs before = 0 after = 0\n"
     ]
    }
   ],
   "source": [
    "def get_doc_ct():\n",
    "    session = SnorkelSession()\n",
    "    ct = session.query(Document).count()\n",
    "    session.close()\n",
    "    return ct\n",
    "    \n",
    "def clear_documents():\n",
    "    session = SnorkelSession()\n",
    "    parser = CorpusParser(parser=lambda v: None)\n",
    "    parser.clear(session)\n",
    "    session.commit()\n",
    "    session.close()\n",
    "    \n",
    "n = get_doc_ct()\n",
    "clear_documents()\n",
    "print('Num docs before =', n, 'after =', get_doc_ct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:42609\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34281'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:43737'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38343'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34605'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:44437'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:39241'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:41679'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34697'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37409'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:40481'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:32883'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38405'\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:42987\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:42987\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:36605\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:44255\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:46085\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:35065\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:36169\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:45405\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:44965\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:46289\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:34133\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:44979\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:38005\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:36605\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:44255\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:46085\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:35065\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:36169\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:45405\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:44965\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:46289\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:34133\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:44979\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:38005\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:44437'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:39241'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34605'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:40481'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:41679'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37409'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:32883'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38343'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:43737'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38405'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34697'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34281'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34281'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:43737'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38343'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34605'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:44437'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:39241'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:41679'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:34697'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:37409'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:40481'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:32883'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://172.17.0.2:38405'\n",
      "distributed.scheduler - INFO - Receive client connection: Client-64423108-b2b4-11e9-9b71-0242ac110002\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:42609\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>12</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>3.07 TB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:42609' processes=12 cores=12>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(\n",
    "    threads_per_worker=1, n_workers=12, processes=True, \n",
    "    memory_limit='256GB', direct_to_workers=True, \n",
    "    silence_logs=logging.INFO\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(job):\n",
    "    import logging\n",
    "    logging.basicConfig()\n",
    "    batch_id, batch_file = job\n",
    "    logging.info('Processing job %s (%s)', batch_id, batch_file)\n",
    "    loader = processing.DocLoader(batch_file)\n",
    "    loader.run(limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2750df9422a4baaa6146f8867bcaa27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "futures = client.map(process, jobs)\n",
    "dask.distributed.progress(futures, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:32809\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:32809\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:33585\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:33585\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:45321\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:45321\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:44893\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:44893\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:37581\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:37581\n",
      "distributed.batched - INFO - Batched Comm Closed: in <closed TCP>: Stream is closed\n",
      "distributed.batched - INFO - Batched Comm Closed: in <closed TCP>: Stream is closed\n",
      "distributed.batched - INFO - Batched Comm Closed: in <closed TCP>: Stream is closed\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:35005\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:35005\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:43697\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:43697\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:46197\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:46197\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:41739\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:41739\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:34353\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:34353\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:44077\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:44077\n",
      "distributed.scheduler - INFO - Remove worker tcp://172.17.0.2:38001\n",
      "distributed.core - INFO - Removing comms to tcp://172.17.0.2:38001\n",
      "distributed.scheduler - INFO - Lost all workers\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.batched - INFO - Batched Comm Closed: \n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:43053\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:35437\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:43291\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:45537\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:33365\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:41239\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:39885\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:45185\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:41057\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:39791\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:33679\n",
      "distributed.scheduler - INFO - Register tcp://172.17.0.2:45667\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:43053\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:35437\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:43291\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:45537\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:33365\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:41239\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:39885\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:45185\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:41057\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:39791\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:33679\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.0.2:45667\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-73b7ed63c4c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m   4056\u001b[0m     \"\"\"\n\u001b[1;32m   4057\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4058\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_when\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_when\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4059\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             return sync(\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             )\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dask.distributed.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([f.status == 'finished' for f in futures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([f.status == 'finished' for f in futures]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   8,   9,  11,  12,  13,  14,\n",
       "        15,  16,  17,  18,  20,  21,  23,  25,  26,  27,  29,  30,  32,\n",
       "        33,  35,  36,  37,  38,  39,  41,  42,  44,  45,  47,  48,  49,\n",
       "        51,  52,  54,  56,  57,  59,  60,  61,  63,  64,  68,  69,  72,\n",
       "        74,  75,  76,  77,  78,  81,  82,  83,  84,  85,  87,  88,  89,\n",
       "        90,  91,  92,  93,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118,\n",
       "       119, 120, 121, 122, 123, 124, 125, 126, 127])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere([f.status != 'finished' for f in futures]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Document).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = session.query(Sentence).all()\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = set(sents)\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import PretaggedCandidateExtractor\n",
    "classes = supervision.get_candidate_classes()\n",
    "candidate_extractors = [\n",
    "    PretaggedCandidateExtractor(c.subclass, c.entity_types)\n",
    "    for c in classes.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning candidate extraction for split 9, relation type InducingCytokine, num batches 1\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.88s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates generated for split 9, relation type InducingCytokine = 1356\n",
      "Beginning candidate extraction for split 9, relation type SecretedCytokine, num batches 1\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.28s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates generated for split 9, relation type SecretedCytokine = 1356\n",
      "Beginning candidate extraction for split 9, relation type InducingTranscriptionFactor, num batches 1\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates generated for split 9, relation type InducingTranscriptionFactor = 291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def apply_extraction(sents, split, batch_size=10000):\n",
    "    for extractor in candidate_extractors:\n",
    "        relation_class = extractor.udf_init_kwargs['candidate_class']\n",
    "        n_batch = int(np.ceil(len(sents) / batch_size))\n",
    "        print('Beginning candidate extraction for split {}, relation type {}, num batches {}'.format(\n",
    "            split, relation_class.__name__, n_batch\n",
    "        ))\n",
    "        for batch in tqdm.tqdm(np.array_split(list(sents), n_batch)):\n",
    "            extractor.apply(batch, split=split, clear=False, progress_bar=False)\n",
    "        print('Number of candidates generated for split {}, relation type {} = {}'.format(\n",
    "            split, relation_class.__name__,\n",
    "            session.query(relation_class).filter(relation_class.split == split).count()\n",
    "        ))\n",
    "\n",
    "\n",
    "apply_extraction(sents, supervision.SPLIT_INFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                           split\n",
       "inducing_cytokine              9        1356\n",
       "inducing_transcription_factor  9         291\n",
       "secreted_cytokine              9        1356\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands = session.query(Candidate.type, Candidate.split).all()\n",
    "pd.DataFrame(cands).groupby(['type', 'split']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gold Labels\n",
    "\n",
    "Only relevant for ```candidate_mode == 'training'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if candidate_mode != 'training':\n",
    "    raise ValueError('Loading manual annotations only relevant for training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e1_end_chr</th>\n",
       "      <th>e1_start_chr</th>\n",
       "      <th>e1_text</th>\n",
       "      <th>e1_typ</th>\n",
       "      <th>e2_end_chr</th>\n",
       "      <th>e2_start_chr</th>\n",
       "      <th>e2_text</th>\n",
       "      <th>e2_typ</th>\n",
       "      <th>id</th>\n",
       "      <th>rel_typ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>Gfi-1</td>\n",
       "      <td>TRANSCRIPTION_FACTOR</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>Th17</td>\n",
       "      <td>IMMUNE_CELL_TYPE</td>\n",
       "      <td>PMC2646571</td>\n",
       "      <td>Differentiation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>TGF-β</td>\n",
       "      <td>CYTOKINE</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>Th17</td>\n",
       "      <td>IMMUNE_CELL_TYPE</td>\n",
       "      <td>PMC2646571</td>\n",
       "      <td>Induction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>TGF-β</td>\n",
       "      <td>CYTOKINE</td>\n",
       "      <td>119</td>\n",
       "      <td>97</td>\n",
       "      <td>inducible regulatory T</td>\n",
       "      <td>IMMUNE_CELL_TYPE</td>\n",
       "      <td>PMC2646571</td>\n",
       "      <td>Induction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>Gfi-1</td>\n",
       "      <td>TRANSCRIPTION_FACTOR</td>\n",
       "      <td>119</td>\n",
       "      <td>97</td>\n",
       "      <td>inducible regulatory T</td>\n",
       "      <td>IMMUNE_CELL_TYPE</td>\n",
       "      <td>PMC2646571</td>\n",
       "      <td>Differentiation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>371</td>\n",
       "      <td>366</td>\n",
       "      <td>Gfi-1</td>\n",
       "      <td>TRANSCRIPTION_FACTOR</td>\n",
       "      <td>436</td>\n",
       "      <td>433</td>\n",
       "      <td>Th2</td>\n",
       "      <td>IMMUNE_CELL_TYPE</td>\n",
       "      <td>PMC2646571</td>\n",
       "      <td>Differentiation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e1_end_chr  e1_start_chr e1_text                e1_typ  e2_end_chr  \\\n",
       "0          24            19   Gfi-1  TRANSCRIPTION_FACTOR          85   \n",
       "1          44            39   TGF-β              CYTOKINE          85   \n",
       "2          44            39   TGF-β              CYTOKINE         119   \n",
       "3          24            19   Gfi-1  TRANSCRIPTION_FACTOR         119   \n",
       "4         371           366   Gfi-1  TRANSCRIPTION_FACTOR         436   \n",
       "\n",
       "   e2_start_chr                 e2_text            e2_typ          id  \\\n",
       "0            81                    Th17  IMMUNE_CELL_TYPE  PMC2646571   \n",
       "1            81                    Th17  IMMUNE_CELL_TYPE  PMC2646571   \n",
       "2            97  inducible regulatory T  IMMUNE_CELL_TYPE  PMC2646571   \n",
       "3            97  inducible regulatory T  IMMUNE_CELL_TYPE  PMC2646571   \n",
       "4           433                     Th2  IMMUNE_CELL_TYPE  PMC2646571   \n",
       "\n",
       "           rel_typ  \n",
       "0  Differentiation  \n",
       "1        Induction  \n",
       "2        Induction  \n",
       "3  Differentiation  \n",
       "4  Differentiation  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv export with annotated relations to load:\n",
    "relations = pd.read_csv(osp.join(corpus_dir, 'relations.csv'))\n",
    "relations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Induction          150\n",
       "Secretion          131\n",
       "Differentiation    119\n",
       "Name: rel_typ, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations['rel_typ'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e1_typ                e2_typ            rel_typ        \n",
       "CYTOKINE              IMMUNE_CELL_TYPE  Induction          150\n",
       "                                        Secretion          131\n",
       "TRANSCRIPTION_FACTOR  IMMUNE_CELL_TYPE  Differentiation    119\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations.groupby(['e1_typ', 'e2_typ', 'rel_typ']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset annotation tables (if loading them below fails)\n",
    "# session.execute('DELETE FROM stable_label;')\n",
    "# session.execute('DELETE FROM gold_label;')\n",
    "# session.execute('DELETE FROM gold_label_key;')\n",
    "# from snorkel.models import GoldLabel, GoldLabelKey, StableLabel\n",
    "# session.commit()\n",
    "# session.query(StableLabel).count(), session.query(GoldLabel).count(), session.query(GoldLabelKey).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 relations for type Induction\n",
      "inducing_cytokine\n",
      "AnnotatorLabels created: 133, missed: 17\n",
      "Found 131 relations for type Secretion\n",
      "secreted_cytokine\n",
      "AnnotatorLabels created: 81, missed: 50\n",
      "Found 119 relations for type Differentiation\n",
      "inducing_transcription_factor\n",
      "AnnotatorLabels created: 86, missed: 33\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import StableLabel\n",
    "from snorkel.db_helpers import reload_annotator_labels\n",
    "\n",
    "def get_stable_id(r):\n",
    "    return '{}::span:{}:{}'.format(r['id'], r['start_chr'], r['end_chr']-1)\n",
    "\n",
    "def reload_labels(\n",
    "    session, candidate_class, annotator_name, split, \n",
    "    filter_label_split=True, create_missing_cands=False):\n",
    "    \"\"\"Reloads stable annotator labels into the AnnotatorLabel table\"\"\"\n",
    "    from snorkel.models import GoldLabel, GoldLabelKey, StableLabel, Context\n",
    "    from future.utils import iteritems\n",
    "    # Sets up the AnnotatorLabelKey to use\n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    if ak is None:\n",
    "        ak = GoldLabelKey(name=annotator_name)\n",
    "        session.add(ak)\n",
    "        session.commit()\n",
    "\n",
    "    labels = []\n",
    "    missed = []\n",
    "    sl_query = session.query(StableLabel).filter(StableLabel.annotator_name == annotator_name)\n",
    "    sl_query = sl_query.filter(StableLabel.split == split) if filter_label_split else sl_query\n",
    "    for sl in sl_query.all():\n",
    "        context_stable_ids = sl.context_stable_ids.split('~~')\n",
    "\n",
    "        # Check for labeled Contexts\n",
    "        # TODO: Does not create the Contexts if they do not yet exist!\n",
    "        contexts = []\n",
    "        for stable_id in context_stable_ids:\n",
    "            context = session.query(Context).filter(Context.stable_id == stable_id).first()\n",
    "            if context:\n",
    "                contexts.append(context)\n",
    "        if len(contexts) < len(context_stable_ids):\n",
    "            missed.append(sl)\n",
    "            continue\n",
    "\n",
    "        # Check for Candidate\n",
    "        # Assemble candidate arguments\n",
    "        candidate_args  = {'split' : split}\n",
    "        for i, arg_name in enumerate(candidate_class.__argnames__):\n",
    "            candidate_args[arg_name] = contexts[i]\n",
    "\n",
    "        # Assemble query and check\n",
    "        candidate_query = session.query(candidate_class)\n",
    "        for k, v in iteritems(candidate_args):\n",
    "            candidate_query = candidate_query.filter(getattr(candidate_class, k) == v)\n",
    "        candidate = candidate_query.first()\n",
    "\n",
    "        # Optionally construct missing candidates\n",
    "        if candidate is None and create_missing_cands:\n",
    "            candidate = candidate_class(**candidate_args)\n",
    "\n",
    "        # If candidate is none, mark as missed and continue\n",
    "        if candidate is None:\n",
    "            missed.append(sl)\n",
    "            continue\n",
    "\n",
    "        # Check for AnnotatorLabel, otherwise create\n",
    "        label = session.query(GoldLabel).filter(GoldLabel.key == ak).filter(GoldLabel.candidate == candidate).first()\n",
    "        if label is None:\n",
    "            label = GoldLabel(candidate=candidate, key=ak, value=sl.value)\n",
    "            session.add(label)\n",
    "            labels.append(label)\n",
    "\n",
    "    session.commit()\n",
    "    print(\"AnnotatorLabels created: %s, missed: %s\" % (len(labels), len(missed)))\n",
    "    return missed, labels\n",
    "    \n",
    "def load_external_labels(session, relations, candidate_class, annotator_name='gold'):\n",
    "    print(annotator_name)\n",
    "    for i, r in relations.iterrows():    \n",
    "\n",
    "        # We check if the label already exists, in case this cell was already executed\n",
    "        e1_id = get_stable_id(r.filter(regex='^e1_|^id$').rename(lambda v: v.replace('e1_', '')))\n",
    "        e2_id = get_stable_id(r.filter(regex='^e2_|^id$').rename(lambda v: v.replace('e2_', '')))\n",
    "        \n",
    "        context_stable_ids = \"~~\".join([e1_id, e2_id])\n",
    "        query = session.query(StableLabel)\\\n",
    "            .filter(StableLabel.context_stable_ids == context_stable_ids)\\\n",
    "            .filter(StableLabel.annotator_name == annotator_name)\n",
    "        if query.count() == 0:\n",
    "            session.add(StableLabel(\n",
    "                context_stable_ids=context_stable_ids,\n",
    "                annotator_name=annotator_name,\n",
    "                value=1\n",
    "            ))\n",
    "            \n",
    "    session.commit()\n",
    "    # This function will create GoldLabel records for each StableLabel above after\n",
    "    # selecting them based on annotator_name.  The annotator name should be different\n",
    "    # for each candidate class if they might have identical context_stable_ids since\n",
    "    # otherwise as written above only the first value for the same annotator + context_stable_ids\n",
    "    # will be saved.\n",
    "    # Other notes: split will be used to find candidates necessary to create GoldLabels though\n",
    "    # it is not necessary for StableLabel filtering in this case (thus filter_label_split=False)\n",
    "    # because the labels were not created with a split above\n",
    "    #reload_annotator_labels(\n",
    "    return reload_labels(\n",
    "        session, candidate_class, annotator_name, split=1, \n",
    "        filter_label_split=False, create_missing_cands=False)\n",
    "    \n",
    "cand_summary = {}\n",
    "for extractor in candidate_extractors:\n",
    "    relation_class = extractor.udf_init_kwargs['candidate_class']\n",
    "    label = classes[relation_class.__name__].label\n",
    "    field = classes[relation_class.__name__].field\n",
    "    df = relations[relations['rel_typ'] == label]\n",
    "    assert len(df) > 0, 'Found no records for relation type {}'.format(label)\n",
    "    print('Found {} relations for type {}'.format(len(df), label))\n",
    "    cand_summary[relation_class.__name__] = load_external_labels(session, df, relation_class, annotator_name=field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relation                     missing                              \n",
       "InducingCytokine             cytokine                                  7\n",
       "                             immune_cell_type                         10\n",
       "InducingTranscriptionFactor                                            1\n",
       "                             immune_cell_type                          7\n",
       "                             transcription_factor                     18\n",
       "                             transcription_factor,immune_cell_type     7\n",
       "SecretedCytokine             cytokine                                 11\n",
       "                             cytokine,immune_cell_type                 3\n",
       "                             immune_cell_type                         36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.models import Context \n",
    "\n",
    "# Show which entity types and relations were unable to be matched with spans\n",
    "# extracted and inserted into snorkel db\n",
    "def summarize_missing_candidates(cand_summary):\n",
    "    df = []\n",
    "    for c in cand_summary:\n",
    "        class_name = classes[c].name\n",
    "        missed = cand_summary[c][0]\n",
    "        for mc in missed:\n",
    "            doc_id = mc.context_stable_ids.split('::')[0]\n",
    "            ids = mc.context_stable_ids.split('~~')\n",
    "            typs = []\n",
    "            for i, sid in enumerate(ids):\n",
    "                ctx = session.query(Context).filter(Context.stable_id == sid).all()\n",
    "                if len(ctx) == 0:\n",
    "                    typs.append(classes[c].entity_types[i])\n",
    "            df.append((class_name, doc_id, ','.join(typs)))\n",
    "    return pd.DataFrame(df, columns=['relation', 'doc_id', 'missing'])\n",
    "df_miss = summarize_missing_candidates(cand_summary)\n",
    "df_miss.groupby(['relation', 'missing']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relation                     doc_id    \n",
       "SecretedCytokine             PMC3046151    13\n",
       "                             PMC2196041    11\n",
       "InducingTranscriptionFactor  PMC2587175     8\n",
       "                             PMC2783637     6\n",
       "                             PMC2646571     5\n",
       "SecretedCytokine             PMC2193209     5\n",
       "InducingCytokine             PMC3173465     4\n",
       "SecretedCytokine             PMC4385920     4\n",
       "InducingTranscriptionFactor  PMC3173465     4\n",
       "SecretedCytokine             PMC3650071     4\n",
       "InducingTranscriptionFactor  PMC3304099     3\n",
       "                             PMC5591438     2\n",
       "InducingCytokine             PMC4023883     2\n",
       "InducingTranscriptionFactor  PMC5206501     2\n",
       "InducingCytokine             PMC2196041     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find documents with most annotations unable to be matched and either improve tagging or change annotations\n",
    "df_miss.groupby(['relation', 'doc_id']).size().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels, load_matrix\n",
    "from snorkel.models import Candidate\n",
    "\n",
    "L_gold = {}\n",
    "for c in classes:\n",
    "    cids_query = get_cids_query(session, classes[c], split=1)\n",
    "    L_gold[c] = load_gold_labels(session, annotator_name=classes[c].field, split=1, cids_query=cids_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InducingCytokine (673, 1)\n",
      "SecretedCytokine (673, 1)\n",
      "InducingTranscriptionFactor (410, 1)\n"
     ]
    }
   ],
   "source": [
    "for c in L_gold:\n",
    "    print(c, L_gold[c].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate counts: InducingCytokine (split 0) -> 11735\n",
      "Candidate counts: InducingCytokine (split 1) -> 673\n",
      "Candidate counts: SecretedCytokine (split 0) -> 11735\n",
      "Candidate counts: SecretedCytokine (split 1) -> 673\n",
      "Candidate counts: InducingTranscriptionFactor (split 0) -> 6696\n",
      "Candidate counts: InducingTranscriptionFactor (split 1) -> 410\n"
     ]
    }
   ],
   "source": [
    "for c in classes.values():\n",
    "    for split in [0, 1]:\n",
    "        n = session.query(c.subclass).filter(c.subclass.split == split).count()\n",
    "        print('Candidate counts: {} (split {}) -> {}'.format(c.name, split, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  1\n",
       "0  0    459\n",
       "   1     81\n",
       "1  0    133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that inducing/secreted cytokine labels are mutally exclusive\n",
    "L_df = pd.DataFrame(np.hstack((\n",
    "    L_gold[classes.inducing_cytokine.name].toarray(), \n",
    "    L_gold[classes.secreted_cytokine.name].toarray()\n",
    ")))\n",
    "L_df.groupby([0, 1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InducingCytokine    673\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([\n",
    "    type(L_gold[classes.inducing_cytokine.name].get_candidate(session, i)).__name__ \n",
    "    for i in range(L_gold[classes.inducing_cytokine.name].shape[0])\n",
    "]).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
