{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Fine-Tuning Example\n",
    "\n",
    "This notebook will export training and evaluation data for fine-tuning and scoring of a BERT model\n",
    "\n",
    "See: \n",
    "- [Running Pytorch-Transformers on Custom Datasets](https://medium.com/dsnet/running-pytorch-transformers-on-custom-datasets-717fd9e10fe2)\n",
    "- [pytorch-transformers-extensions](https://github.com/nikhilno1/nlp_projects/tree/f5e4ae159970b6fd613d2c2181265db336acc934/pytorch-transformers-extensions) (slight abstractions for running pytorch-transformers on custom datasets)\n",
    "- [pytorch-transformers](https://github.com/huggingface/pytorch-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from snorkel import SnorkelSession\n",
    "from tcre.env import *\n",
    "from tcre import supervision\n",
    "from tcre.supervision import SPLIT_DEV, SPLIT_VAL, SPLIT_TEST, REL_FIELD_INDUCING_CYTOKINE, ENT_TYP_CT_L\n",
    "from tcre.modeling import features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "session = SnorkelSession()\n",
    "classes = supervision.get_candidate_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CandidateClass({'index': 0, 'name': 'InducingCytokine', 'field': 'inducing_cytokine', 'label': 'Induction', 'abbr': 'indck', 'entity_types': ['cytokine', 'immune_cell_type'], 'subclass': <class 'snorkel.models.candidate.InducingCytokine'>})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_class = {classes[c].field: classes[c] for c in classes}[REL_FIELD_INDUCING_CYTOKINE]\n",
    "candidate_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.models import Candidate\n",
    "splits = [SPLIT_DEV, SPLIT_VAL, SPLIT_TEST]\n",
    "cands = session.query(Candidate).filter(Candidate.split.in_(frozenset(splits))).filter(Candidate.type == candidate_class.field).all()\n",
    "len(cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcand = pd.DataFrame([dict(split=c.split, cand=c) for c in cands]).groupby('split')['cand'].unique().to_dict()\n",
    "mcand.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcre.modeling import sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cand, df_dist = sampling.get_modeling_splits(session, target_split_map={'dev': 'train', 'val': 'val'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 998 entries, 831 to 2967\n",
      "Data columns (total 4 columns):\n",
      "id       998 non-null int64\n",
      "label    998 non-null int64\n",
      "split    998 non-null object\n",
      "task     998 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 39.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cand.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    766\n",
       "val      232\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cand['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "DEFAULT_CONFIG = dict(max_seq_length=128, learning_rate=2e-5, num_train_epochs=3.0)\n",
    "DEFAULT_BERT_PATH = osp.join('/lab/data/scibert', 'scibert_scivocab_uncased')\n",
    "\n",
    "def run_transformer_model(session, config=DEFAULT_CONFIG, data_dir=None):\n",
    "    df_cand, df_dist = sampling.get_modeling_splits(session, target_split_map={'dev': 'train', 'val': 'val', 'test': 'test'})\n",
    "    \n",
    "    def get_cands(split):\n",
    "        cands = df_cand[df_cand['split'] == split]['id'].unique()\n",
    "        cands = session.query(Candidate).filter(Candidate.id.in_(frozenset(cands)))\n",
    "        return cands\n",
    "    \n",
    "    train_cands, eval_cands, test_cands = [get_cands(s) for s in ['train', 'val', 'test']]\n",
    "    \n",
    "    \n",
    "CMD_TRAIN = \"\"\"\n",
    "python run_dataset.py --task_name tcre --do_train \\\n",
    "--do_lower_case --data_dir {data_dir} \\\n",
    "--model_type bert --model_name_or_path {model_name_or_path} \\\n",
    "--max_seq_length {max_seq_length} --learning_rate {learning_rate} --num_train_epochs {learning_rate} \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir {output_dir}\n",
    "\"\"\"\n",
    "\n",
    "CMD_EVAL = \"\"\"\n",
    "python run_dataset.py --task_name tcre --do_eval \\\n",
    "--do_lower_case --data_dir {data_dir} \\\n",
    "--model_type bert --model_name_or_path {model_name_or_path} \\\n",
    "--max_seq_length {max_seq_length} --learning_rate {learning_rate} --num_train_epochs {learning_rate} \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir {output_dir}\n",
    "\"\"\"\n",
    "    \n",
    "def run_training(train_cands, eval_cands, config=DEFAULT_CONFIG, data_dir=None, bert_path=DEFAULT_BERT_PATH):\n",
    "    \n",
    "    cands = []\n",
    "    cands += list(zip(['train'] * len(train_cands), train_cands)) \n",
    "    cands += list(zip(['dev'] * len(eval_cands), eval_cands)) \n",
    "    df = pd.DataFrame([\n",
    "        dict(guid=c.id, text=features.get_scibert_text(c), split=s, label=str(features.get_label(c)))\n",
    "        for (s, c) in cands\n",
    "    ])\n",
    "    assert df['label'].isin(['0', '1']).all()\n",
    "    \n",
    "    if data_dir is None:\n",
    "        data_dir = tempfile.mkdtemp()\n",
    "    input_dir = osp.join(data_dir, 'input')\n",
    "    output_dir = osp.join(data_dir, 'output')\n",
    "    log_file = osp.join(data_dir, 'log.txt')\n",
    "    if osp.exists(input_dir):\n",
    "        shutil.rmtree(input_dir)\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    if osp.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if osp.exists(log_file):\n",
    "        os.remove(log_file)\n",
    "    df.to_csv(osp.join(input_dir, 'data.csv'), index=False)\n",
    "    \n",
    "    \n",
    "    cmd = CMD_TRAIN.format(data_dir=input_dir, output_dir=output_dir, model_name_or_path=bert_path, **config)\n",
    "    cmd += \" >> {}\".format(log_file)\n",
    "    rc = os.system(cmd)\n",
    "    if rc != 0:\n",
    "        raise ValueError('Command failed with return code {}:\\n{}'.format(rc, cmd))\n",
    "        \n",
    "    \n",
    "    pd.read_json('/tmp/scibert/inducing_cytokine/output/scores.json', lines=True).iloc[0]\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "train_cands = mcand[SPLIT_DEV]\n",
    "eval_cands = mcand[SPLIT_VAL]\n",
    "data_dir = osp.join('/tmp', 'scibert', candidate_class.field)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "cmd = run_transformer({}, train_cands, eval_cands, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    python run_dataset.py --task_name tcre --do_train --do_eval     --do_lower_case --data_dir /tmp/scibert/inducing_cytokine/input     --model_type bert --model_name_or_path /lab/data/scibert/scibert_scivocab_uncased     --max_seq_length 128 --learning_rate 2e-5 --num_train_epochs 3.0     --overwrite_output_dir     --output_dir /tmp/scibert/inducing_cytokine/output\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_transformers import file_utils\n",
    "# file_utils.PYTORCH_PRETRAINED_BERT_CACHE\n",
    "# from pytorch_transformers import BertForSequenceClassification\n",
    "# model = BertForSequenceClassification.from_pretrained('/tmp/scibert/inducing_cytokine/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/10/2019 00:42:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /lab/data/scibert/scibert_scivocab_uncased/config.json\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"tcre\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   Model name '/lab/data/scibert/scibert_scivocab_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/lab/data/scibert/scibert_scivocab_uncased' is a path or url to a directory containing tokenizer files.\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /lab/data/scibert/scibert_scivocab_uncased/added_tokens.json. We won't load it.\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /lab/data/scibert/scibert_scivocab_uncased/special_tokens_map.json. We won't load it.\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /lab/data/scibert/scibert_scivocab_uncased/tokenizer_config.json. We won't load it.\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   loading file /lab/data/scibert/scibert_scivocab_uncased/vocab.txt\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "09/10/2019 00:42:07 - INFO - pytorch_transformers.modeling_utils -   loading weights file /lab/data/scibert/scibert_scivocab_uncased/pytorch_model.bin\n",
      "09/10/2019 00:42:10 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "09/10/2019 00:42:10 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "09/10/2019 00:42:13 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/tmp/scibert/inducing_cytokine/input', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='/lab/data/scibert/scibert_scivocab_uncased', model_type='bert', n_gpu=2, no_cuda=False, num_train_epochs=8.0, output_dir='/tmp/scibert/inducing_cytokine/output', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50, seed=42, server_ip='', server_port='', task_name='tcre', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "09/10/2019 00:42:13 - INFO - __main__ -   Loading features from cached file /tmp/scibert/inducing_cytokine/input/cached_train_scibert_scivocab_uncased_128_tcre\n",
      "09/10/2019 00:42:14 - INFO - __main__ -   ***** Running training *****\n",
      "09/10/2019 00:42:14 - INFO - __main__ -     Num examples = 1025\n",
      "09/10/2019 00:42:14 - INFO - __main__ -     Num Epochs = 8\n",
      "09/10/2019 00:42:14 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
      "09/10/2019 00:42:14 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "09/10/2019 00:42:14 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "09/10/2019 00:42:14 - INFO - __main__ -     Total optimization steps = 520\n",
      "Epoch:   0%|                                              | 0/8 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                         | 0/65 [00:00<?, ?it/s]\u001b[A/opt/conda/envs/nlp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "Iteration:   2%|▌                                | 1/65 [00:03<03:43,  3.49s/it]\u001b[A\n",
      "Iteration:   3%|█                                | 2/65 [00:03<02:38,  2.52s/it]\u001b[A\n",
      "Iteration:   5%|█▌                               | 3/65 [00:03<01:53,  1.84s/it]\u001b[A\n",
      "Iteration:   6%|██                               | 4/65 [00:04<01:23,  1.36s/it]\u001b[A\n",
      "Iteration:   8%|██▌                              | 5/65 [00:04<01:01,  1.03s/it]\u001b[A\n",
      "Iteration:   9%|███                              | 6/65 [00:04<00:47,  1.25it/s]\u001b[A\n",
      "Iteration:  11%|███▌                             | 7/65 [00:05<00:36,  1.58it/s]\u001b[A\n",
      "Iteration:  12%|████                             | 8/65 [00:05<00:29,  1.92it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 9/65 [00:05<00:24,  2.27it/s]\u001b[A\n",
      "Iteration:  15%|████▉                           | 10/65 [00:05<00:21,  2.61it/s]\u001b[A\n",
      "Iteration:  17%|█████▍                          | 11/65 [00:06<00:18,  2.90it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                          | 12/65 [00:06<00:16,  3.12it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 13/65 [00:06<00:15,  3.33it/s]\u001b[A\n",
      "Iteration:  22%|██████▉                         | 14/65 [00:06<00:14,  3.50it/s]\u001b[A\n",
      "Iteration:  23%|███████▍                        | 15/65 [00:07<00:13,  3.63it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                        | 16/65 [00:07<00:13,  3.73it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 17/65 [00:07<00:12,  3.80it/s]\u001b[A\n",
      "Iteration:  28%|████████▊                       | 18/65 [00:07<00:12,  3.85it/s]\u001b[A\n",
      "Iteration:  29%|█████████▎                      | 19/65 [00:08<00:11,  3.88it/s]\u001b[A\n",
      "Iteration:  31%|█████████▊                      | 20/65 [00:08<00:11,  3.90it/s]\u001b[A\n",
      "Iteration:  32%|██████████▎                     | 21/65 [00:08<00:11,  3.92it/s]\u001b[A\n",
      "Iteration:  34%|██████████▊                     | 22/65 [00:08<00:10,  3.94it/s]\u001b[A\n",
      "Iteration:  35%|███████████▎                    | 23/65 [00:09<00:10,  3.95it/s]\u001b[A\n",
      "Iteration:  37%|███████████▊                    | 24/65 [00:09<00:10,  3.96it/s]\u001b[A\n",
      "Iteration:  38%|████████████▎                   | 25/65 [00:09<00:10,  3.96it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 26/65 [00:09<00:09,  3.97it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▎                  | 27/65 [00:10<00:09,  3.97it/s]\u001b[A\n",
      "Iteration:  43%|█████████████▊                  | 28/65 [00:10<00:09,  3.97it/s]\u001b[A\n",
      "Iteration:  45%|██████████████▎                 | 29/65 [00:10<00:09,  3.97it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▊                 | 30/65 [00:10<00:08,  3.97it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 31/65 [00:11<00:08,  3.97it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▊                | 32/65 [00:11<00:08,  3.96it/s]\u001b[A\n",
      "Iteration:  51%|████████████████▏               | 33/65 [00:11<00:08,  3.96it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 34/65 [00:11<00:07,  3.95it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▏              | 35/65 [00:12<00:07,  3.95it/s]\u001b[A\n",
      "Iteration:  55%|█████████████████▋              | 36/65 [00:12<00:07,  3.95it/s]\u001b[A\n",
      "Iteration:  57%|██████████████████▏             | 37/65 [00:12<00:07,  3.95it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▋             | 38/65 [00:12<00:06,  3.94it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 39/65 [00:13<00:06,  3.94it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▋            | 40/65 [00:13<00:06,  3.95it/s]\u001b[A\n",
      "Iteration:  63%|████████████████████▏           | 41/65 [00:13<00:06,  3.96it/s]\u001b[A\n",
      "Iteration:  65%|████████████████████▋           | 42/65 [00:13<00:05,  3.96it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████▏          | 43/65 [00:14<00:05,  3.96it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▋          | 44/65 [00:14<00:05,  3.96it/s]\u001b[A\n",
      "Iteration:  69%|██████████████████████▏         | 45/65 [00:14<00:05,  3.96it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████▋         | 46/65 [00:14<00:04,  3.95it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████▏        | 47/65 [00:15<00:04,  3.95it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 48/65 [00:15<00:04,  3.95it/s]\u001b[A\n",
      "Iteration:  75%|████████████████████████        | 49/65 [00:15<00:04,  3.95it/s]\u001b[A09/10/2019 00:42:29 - INFO - __main__ -   Loading features from cached file /tmp/scibert/inducing_cytokine/input/cached_dev_scibert_scivocab_uncased_128_tcre\n",
      "09/10/2019 00:42:29 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "09/10/2019 00:42:29 - INFO - __main__ -     Num examples = 278\n",
      "09/10/2019 00:42:29 - INFO - __main__ -     Batch size = 16\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  11%|███▌                            | 2/18 [00:00<00:01, 11.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  22%|███████                         | 4/18 [00:00<00:01, 11.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▋                     | 6/18 [00:00<00:01, 11.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|██████████████▏                 | 8/18 [00:00<00:00, 11.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████████████████▏             | 10/18 [00:00<00:00, 11.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 12/18 [00:01<00:00, 11.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  78%|████████████████████████       | 14/18 [00:01<00:00, 11.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  89%|███████████████████████████▌   | 16/18 [00:01<00:00, 11.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 18/18 [00:01<00:00, 12.31it/s]\u001b[A\u001b[A/opt/conda/envs/nlp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "09/10/2019 00:42:31 - INFO - __main__ -   ***** Eval results  *****\n",
      "09/10/2019 00:42:31 - INFO - __main__ -     acc = 0.9172661870503597\n",
      "09/10/2019 00:42:31 - INFO - __main__ -     acc_and_f1 = 0.45863309352517984\n",
      "09/10/2019 00:42:31 - INFO - __main__ -     f1 = 0.0\n",
      "09/10/2019 00:42:33 - INFO - __main__ -   Saving model checkpoint to /tmp/scibert/inducing_cytokine/output/checkpoint-50\n",
      "\n",
      "Iteration:  77%|████████████████████████▌       | 50/65 [00:19<00:20,  1.35s/it]\u001b[A\n",
      "Iteration:  78%|█████████████████████████       | 51/65 [00:19<00:14,  1.03s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 52/65 [00:20<00:10,  1.25it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████      | 53/65 [00:20<00:07,  1.56it/s]\u001b[A\n",
      "Iteration:  83%|██████████████████████████▌     | 54/65 [00:20<00:05,  1.89it/s]\u001b[A\n",
      "Iteration:  85%|███████████████████████████     | 55/65 [00:20<00:04,  2.22it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 56/65 [00:21<00:03,  2.52it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████    | 57/65 [00:21<00:02,  2.79it/s]\u001b[A\n",
      "Iteration:  89%|████████████████████████████▌   | 58/65 [00:21<00:02,  3.02it/s]\u001b[A\n",
      "Iteration:  91%|█████████████████████████████   | 59/65 [00:21<00:01,  3.20it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▌  | 60/65 [00:22<00:01,  3.34it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 61/65 [00:22<00:01,  3.45it/s]\u001b[A\n",
      "Iteration:  95%|██████████████████████████████▌ | 62/65 [00:22<00:00,  3.53it/s]\u001b[A\n",
      "Iteration:  97%|███████████████████████████████ | 63/65 [00:23<00:00,  3.58it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▌| 64/65 [00:23<00:00,  3.62it/s]\u001b[A\n",
      "Epoch:  12%|████▊                                 | 1/8 [00:23<02:44, 23.50s/it]\u001b[A\n",
      "Iteration:   0%|                                         | 0/65 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                                | 1/65 [00:00<00:17,  3.72it/s]\u001b[A\n",
      "Iteration:   3%|█                                | 2/65 [00:00<00:16,  3.72it/s]\u001b[A\n",
      "Iteration:   5%|█▌                               | 3/65 [00:00<00:16,  3.72it/s]\u001b[A\n",
      "Iteration:   6%|██                               | 4/65 [00:01<00:16,  3.72it/s]\u001b[A\n",
      "Iteration:   8%|██▌                              | 5/65 [00:01<00:17,  3.48it/s]\u001b[A\n",
      "Iteration:   9%|███                              | 6/65 [00:01<00:16,  3.60it/s]\u001b[A\n",
      "Iteration:  11%|███▌                             | 7/65 [00:01<00:15,  3.69it/s]\u001b[A\n",
      "Iteration:  12%|████                             | 8/65 [00:02<00:15,  3.75it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 9/65 [00:02<00:14,  3.80it/s]\u001b[A\n",
      "Iteration:  15%|████▉                           | 10/65 [00:02<00:14,  3.83it/s]\u001b[A\n",
      "Iteration:  17%|█████▍                          | 11/65 [00:02<00:13,  3.86it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                          | 12/65 [00:03<00:13,  3.89it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 13/65 [00:03<00:13,  3.90it/s]\u001b[A\n",
      "Iteration:  22%|██████▉                         | 14/65 [00:03<00:13,  3.91it/s]\u001b[A\n",
      "Iteration:  23%|███████▍                        | 15/65 [00:03<00:12,  3.92it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                        | 16/65 [00:04<00:12,  3.91it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 17/65 [00:04<00:12,  3.92it/s]\u001b[A\n",
      "Iteration:  28%|████████▊                       | 18/65 [00:04<00:11,  3.92it/s]\u001b[A\n",
      "Iteration:  29%|█████████▎                      | 19/65 [00:04<00:11,  3.92it/s]\u001b[A\n",
      "Iteration:  31%|█████████▊                      | 20/65 [00:05<00:11,  3.92it/s]\u001b[A\n",
      "Iteration:  32%|██████████▎                     | 21/65 [00:05<00:11,  3.92it/s]\u001b[A\n",
      "Iteration:  34%|██████████▊                     | 22/65 [00:05<00:10,  3.92it/s]\u001b[A\n",
      "Iteration:  35%|███████████▎                    | 23/65 [00:05<00:10,  3.92it/s]\u001b[A\n",
      "Iteration:  37%|███████████▊                    | 24/65 [00:06<00:10,  3.91it/s]\u001b[A\n",
      "Iteration:  38%|████████████▎                   | 25/65 [00:06<00:10,  3.92it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 26/65 [00:06<00:09,  3.92it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▎                  | 27/65 [00:07<00:09,  3.92it/s]\u001b[A\n",
      "Iteration:  43%|█████████████▊                  | 28/65 [00:07<00:09,  3.92it/s]\u001b[A\n",
      "Iteration:  45%|██████████████▎                 | 29/65 [00:07<00:09,  3.92it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▊                 | 30/65 [00:07<00:08,  3.92it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 31/65 [00:08<00:08,  3.92it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▊                | 32/65 [00:08<00:08,  3.91it/s]\u001b[A\n",
      "Iteration:  51%|████████████████▏               | 33/65 [00:08<00:08,  3.91it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 34/65 [00:08<00:07,  3.91it/s]\u001b[A09/10/2019 00:42:46 - INFO - __main__ -   Loading features from cached file /tmp/scibert/inducing_cytokine/input/cached_dev_scibert_scivocab_uncased_128_tcre\n",
      "09/10/2019 00:42:46 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "09/10/2019 00:42:46 - INFO - __main__ -     Num examples = 278\n",
      "09/10/2019 00:42:46 - INFO - __main__ -     Batch size = 16\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  11%|███▌                            | 2/18 [00:00<00:01, 11.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  22%|███████                         | 4/18 [00:00<00:01, 11.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▋                     | 6/18 [00:00<00:01, 11.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|██████████████▏                 | 8/18 [00:00<00:00, 11.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████████████████▏             | 10/18 [00:00<00:00, 11.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 12/18 [00:01<00:00, 11.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  78%|████████████████████████       | 14/18 [00:01<00:00, 11.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  89%|███████████████████████████▌   | 16/18 [00:01<00:00, 11.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 18/18 [00:01<00:00, 12.33it/s]\u001b[A\u001b[A09/10/2019 00:42:48 - INFO - __main__ -   ***** Eval results  *****\n",
      "09/10/2019 00:42:48 - INFO - __main__ -     acc = 0.9244604316546763\n",
      "09/10/2019 00:42:48 - INFO - __main__ -     acc_and_f1 = 0.6001612503100968\n",
      "09/10/2019 00:42:48 - INFO - __main__ -     f1 = 0.27586206896551724\n",
      "09/10/2019 00:42:50 - INFO - __main__ -   Saving model checkpoint to /tmp/scibert/inducing_cytokine/output/checkpoint-100\n",
      "\n",
      "Iteration:  54%|█████████████████▏              | 35/65 [00:13<00:43,  1.45s/it]\u001b[A\n",
      "Iteration:  55%|█████████████████▋              | 36/65 [00:13<00:31,  1.10s/it]\u001b[A\n",
      "Iteration:  57%|██████████████████▏             | 37/65 [00:13<00:23,  1.18it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▋             | 38/65 [00:13<00:18,  1.48it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 39/65 [00:14<00:14,  1.80it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▋            | 40/65 [00:14<00:11,  2.13it/s]\u001b[A\n",
      "Iteration:  63%|████████████████████▏           | 41/65 [00:14<00:09,  2.45it/s]\u001b[A\n",
      "Iteration:  65%|████████████████████▋           | 42/65 [00:14<00:08,  2.73it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████▏          | 43/65 [00:15<00:07,  2.96it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Training + Evaluation\n",
    "# Note: /lab/data/scibert/scibert_scivocab_uncased/bert_config.json must be renamed to config.json\n",
    "!python run_dataset.py --task_name tcre --do_train --do_eval \\\n",
    "--do_lower_case --data_dir /tmp/scibert/inducing_cytokine/input \\\n",
    "--model_type bert --model_name_or_path /lab/data/scibert/scibert_scivocab_uncased \\\n",
    "--max_seq_length 128 --learning_rate 2e-5 --num_train_epochs 8.0 \\\n",
    "--evaluate_during_training \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir /tmp/scibert/inducing_cytokine/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/10/2019 14:25:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /tmp/scibert/inducing_cytokine/output/config.json\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"tcre\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.tokenization_utils -   Model name '/tmp/scibert/inducing_cytokine/output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/tmp/scibert/inducing_cytokine/output' is a path or url to a directory containing tokenizer files.\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.tokenization_utils -   loading file /tmp/scibert/inducing_cytokine/output/vocab.txt\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.tokenization_utils -   loading file /tmp/scibert/inducing_cytokine/output/added_tokens.json\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.tokenization_utils -   loading file /tmp/scibert/inducing_cytokine/output/special_tokens_map.json\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.tokenization_utils -   loading file /tmp/scibert/inducing_cytokine/output/tokenizer_config.json\n",
      "09/10/2019 14:25:38 - INFO - pytorch_transformers.modeling_utils -   loading weights file /tmp/scibert/inducing_cytokine/output/pytorch_model.bin\n",
      "09/10/2019 14:25:45 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/tmp/scibert/inducing_cytokine/input', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='/tmp/scibert/inducing_cytokine/output', model_type='bert', n_gpu=2, no_cuda=False, num_train_epochs=8.0, output_dir='/tmp/scibert/inducing_cytokine/output', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=50, seed=42, server_ip='', server_port='', task_name='tcre', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "09/10/2019 14:25:45 - INFO - __main__ -   Evaluate the following checkpoints: ['/tmp/scibert/inducing_cytokine/output']\n",
      "09/10/2019 14:25:45 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /tmp/scibert/inducing_cytokine/output/config.json\n",
      "09/10/2019 14:25:45 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"tcre\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "09/10/2019 14:25:45 - INFO - pytorch_transformers.modeling_utils -   loading weights file /tmp/scibert/inducing_cytokine/output/pytorch_model.bin\n",
      "09/10/2019 14:25:48 - INFO - __main__ -   Loading features from cached file /tmp/scibert/inducing_cytokine/input/cached_dev_output_128_tcre\n",
      "09/10/2019 14:25:48 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "09/10/2019 14:25:48 - INFO - __main__ -     Num examples = 278\n",
      "09/10/2019 14:25:48 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100%|███████████████████████████████| 18/18 [00:01<00:00, 16.25it/s]\n",
      "09/10/2019 14:25:49 - INFO - __main__ -   ***** Eval results  *****\n",
      "09/10/2019 14:25:49 - INFO - __main__ -     acc = 0.9172661870503597\n",
      "09/10/2019 14:25:49 - INFO - __main__ -     acc_and_f1 = 0.7239392159741594\n",
      "09/10/2019 14:25:49 - INFO - __main__ -     f1 = 0.5306122448979592\n",
      "09/10/2019 14:25:49 - INFO - __main__ -     n = 278\n",
      "09/10/2019 14:25:49 - INFO - __main__ -     precision = 0.5\n",
      "09/10/2019 14:25:49 - INFO - __main__ -     rate = 0.08273381294964029\n",
      "09/10/2019 14:25:49 - INFO - __main__ -     recall = 0.5652173913043478\n",
      "{'acc': 0.9172661870503597, 'f1': 0.5306122448979592, 'acc_and_f1': 0.7239392159741594, 'precision': 0.5, 'recall': 0.5652173913043478, 'n': 278, 'rate': 0.08273381294964029}\n",
      "09/10/2019 14:25:49 - INFO - __main__ -   Saved evaluation results to path /tmp/scibert/inducing_cytokine/output/scores.json\n"
     ]
    }
   ],
   "source": [
    "# Evaluation only using model trained above:\n",
    "!python run_dataset.py --task_name tcre --do_eval \\\n",
    "--do_lower_case --data_dir /tmp/scibert/inducing_cytokine/input \\\n",
    "--model_type bert --model_name_or_path /tmp/scibert/inducing_cytokine/output \\\n",
    "--max_seq_length 128 --learning_rate 2e-5 --num_train_epochs 8.0 \\\n",
    "--evaluate_during_training \\\n",
    "--output_dir /tmp/scibert/inducing_cytokine/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acc\":0.9172661871,\"f1\":0.5306122449,\"acc_and_f1\":0.723939216,\"precision\":0.5,\"recall\":0.5652173913,\"n\":278.0,\"rate\":0.0827338129}"
     ]
    }
   ],
   "source": [
    "!cat /tmp/scibert/inducing_cytokine/output/scores.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc</td>\n",
       "      <td>0.917266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acc_and_f1</td>\n",
       "      <td>0.723939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n</td>\n",
       "      <td>278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rate</td>\n",
       "      <td>0.082734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric       value\n",
       "0         acc    0.917266\n",
       "1  acc_and_f1    0.723939\n",
       "2          f1    0.530612\n",
       "3           n  278.000000\n",
       "4   precision    0.500000\n",
       "5        rate    0.082734\n",
       "6      recall    0.565217"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('/tmp/scibert/inducing_cytokine/output/scores.json', lines=True).iloc[0].rename('value').rename_axis('metric').reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
