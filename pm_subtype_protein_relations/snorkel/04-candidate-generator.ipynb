{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import snorkel\n",
    "import dotenv\n",
    "from snorkel.parser import TextDocPreprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "dotenv.load_dotenv('../env.sh')\n",
    "corpus_dir = osp.join(os.environ['DATA_DIR'], 'articles', 'corpus', 'corpus_00')\n",
    "corpus_docs_dir = osp.join(corpus_dir, 'docs')\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_preprocessor = TextDocPreprocessor(corpus_docs_dir, max_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>ent_typ_id</th>\n",
       "      <th>ent_typ_lbl</th>\n",
       "      <th>start_chr</th>\n",
       "      <th>end_chr</th>\n",
       "      <th>start_wrd</th>\n",
       "      <th>end_wrd</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC5743442</td>\n",
       "      <td>CELL_TYPE</td>\n",
       "      <td>4</td>\n",
       "      <td>Th17</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>Th17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC5743442</td>\n",
       "      <td>CELL_TYPE</td>\n",
       "      <td>4</td>\n",
       "      <td>Th17</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Th17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC5743442</td>\n",
       "      <td>CYTOKINE</td>\n",
       "      <td>81</td>\n",
       "      <td>TGF-β1</td>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>TGF-β</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC5743442</td>\n",
       "      <td>CELL_TYPE</td>\n",
       "      <td>4</td>\n",
       "      <td>Th17</td>\n",
       "      <td>199</td>\n",
       "      <td>203</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>Th17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC5743442</td>\n",
       "      <td>CYTOKINE</td>\n",
       "      <td>81</td>\n",
       "      <td>TGF-β1</td>\n",
       "      <td>275</td>\n",
       "      <td>280</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>TGF-β</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       type  ent_typ_id ent_typ_lbl  start_chr  end_chr  \\\n",
       "0  PMC5743442  CELL_TYPE           4        Th17         65       69   \n",
       "1  PMC5743442  CELL_TYPE           4        Th17         87       91   \n",
       "2  PMC5743442   CYTOKINE          81      TGF-β1        174      179   \n",
       "3  PMC5743442  CELL_TYPE           4        Th17        199      203   \n",
       "4  PMC5743442   CYTOKINE          81      TGF-β1        275      280   \n",
       "\n",
       "   start_wrd  end_wrd   text  \n",
       "0          8        9   Th17  \n",
       "1         11       12   Th17  \n",
       "2         25       26  TGF-β  \n",
       "3         29       30   Th17  \n",
       "4         42       43  TGF-β  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pd.read_csv(osp.join(corpus_dir, 'tags.csv'))\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def offsets_to_token(left, right, offset_array, lemmas, punc=set(punctuation)):\n",
    "    \"\"\"Find the token range that spans character offsets \n",
    "    \n",
    "    This will find the indexes of tokens, as a range, that span a target character \n",
    "    range where the first token index has a character offset <= `left` and the \n",
    "    last token index has a character offset > `right`\n",
    "    \n",
    "    Example: offsets_to_token(15, 25, [0, 10, 20, 30]) --> range(1, 3)\n",
    "    \n",
    "    This is useful for identifying all tokens in a document that span a range\n",
    "    of characters determined by another process that may have tokenized the same\n",
    "    document differently.\n",
    "    \"\"\"\n",
    "    token_start, token_end = None, None\n",
    "    for i, c in enumerate(offset_array):\n",
    "        if left >= c:\n",
    "            token_start = i\n",
    "        if c > right and token_end is None:\n",
    "            token_end = i\n",
    "            break\n",
    "    token_end = len(offset_array) - 1 if token_end is None else token_end\n",
    "    token_end = token_end - 1 if lemmas[token_end - 1] in punc else token_end\n",
    "    return range(token_start, token_end)\n",
    "\n",
    "\n",
    "class EntityTagger(object):\n",
    "\n",
    "    def __init__(self, tags):   \n",
    "        self.tags = tags.set_index('id')\n",
    "        self.reset_stats()\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.stats = {'docs': set(), 'found': set()}\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return dict(\n",
    "            n_tags=len(self.tags), \n",
    "            n_docs=len(self.stats['docs']),\n",
    "            n_docs_found=len(self.stats['found']),\n",
    "            pct_docs_found=100*len(self.stats['found'])/len(self.stats['docs'])\n",
    "        )\n",
    "    \n",
    "    def tag(self, parts):\n",
    "        \"\"\"Tag tokens in a single sentence\"\"\"\n",
    "        # Extract doc id (e.g. PMC123932) and character offsets of sentence\n",
    "        docid, _, _, sent_start, sent_end = parts['stable_id'].split(':')\n",
    "        self.stats['docs'].add(docid)\n",
    "        if docid not in self.tags.index:\n",
    "            return parts\n",
    "        self.stats['found'].add(docid)\n",
    "        tags = self.tags.loc[[docid]]\n",
    "        sent_start, sent_end = int(sent_start), int(sent_end)\n",
    "        for r in tags.itertuples():\n",
    "            tag_start, tag_end = r.start_chr, r.end_chr\n",
    "            # Determine whether or not the tag is in this sentence\n",
    "            if not (sent_start <= tag_start <= sent_end):\n",
    "                continue\n",
    "            offsets = [offset + sent_start for offset in parts['char_offsets']]\n",
    "            tkn_idx_rng = offsets_to_token(tag_start, tag_end, offsets, parts['lemmas'])\n",
    "            for tkn_idx in tkn_idx_rng:\n",
    "                parts['entity_types'][tkn_idx] = r.type.lower()\n",
    "                parts['entity_cids'][tkn_idx] = r.ent_typ_lbl # TODO: use the id of the entity instead\n",
    "        return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser, Spacy\n",
    "\n",
    "tagger = EntityTagger(tags)\n",
    "corpus_parser = CorpusParser(fn=tagger.tag)\n",
    "corpus_parser.apply(list(doc_preprocessor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_tags': 23310, 'n_docs': 10, 'n_docs_found': 8, 'pct_docs_found': 80.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10\n",
      "Sentences: 1675\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = session.query(Document).all()\n",
    "doc_ids = [doc.name for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_ids, test_ids = train_test_split(doc_ids, test_size=.1)\n",
    "len(train_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents = set(), set()\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)\n",
    "        elif doc.name in test_ids:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            raise Exception('ID <{0}> not found in any id set'.format(doc.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in session.query(Sentence).all():\n",
    "#     if 'cytokine' in sent.entity_types and 'cell_type' in sent.entity_types:\n",
    "#         print(set(sent.entity_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent.entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(train_sents)[0].entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Candidate, candidate_subclass\n",
    "from snorkel.candidates import PretaggedCandidateExtractor\n",
    "\n",
    "InducingCytokine = candidate_subclass('InducingCytokine', ['cytokine', 'cell_type'])\n",
    "# The entity_types passed here appear to need to be exact matches on the strings\n",
    "# provided above if \"_\" is included (must be some kind of camel case conversion oversight)\n",
    "candidate_extractor = PretaggedCandidateExtractor(InducingCytokine, ['cytokine', 'cell_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 580/1375 [00:00<00:00, 5726.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1375/1375 [00:00<00:00, 5142.18it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 2794.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 58\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "Number of candidates: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, sents in enumerate([train_sents, test_sents]):\n",
    "    candidate_extractor.apply(sents, split=k)\n",
    "    print(\"Number of candidates:\", session.query(InducingCytokine).filter(InducingCytokine.split == k).count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
