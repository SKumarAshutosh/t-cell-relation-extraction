{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_sign = ['+', '-', 'low', 'high', 'pos']\n",
    "vocab_sign_pos = [\n",
    "    '+', \n",
    "    'pos', 'positive',\n",
    "    'high', 'hi', \n",
    "    'bright'\n",
    "]\n",
    "vocab_sign_neg = [\n",
    "    '-', '–', '—', '−', # hyphen, en dash, em dash, minus\n",
    "    'neg', 'negative', \n",
    "    'lo', 'low',\n",
    "    'dim'\n",
    "]\n",
    "vocab_pr = [\n",
    "    'CD4', 'CD45RA', 'CD45', 'CD45RO', 'CD62L', 'CCR7', 'CD127',\n",
    "    'CD27', 'CD28', 'CD122', 'CD8a', 'CD8', 'CD3', 'Thy1', '4-1BB', \n",
    "    'CCR7', 'RORgt', 'CD95', 'CD122'\n",
    "]\n",
    "vocab = vocab_sign_pos + vocab_sign_neg + vocab_pr\n",
    "CASES = [\n",
    "    ('CD4+CD45RA+CD45RO-4-1BB-CD62L+++CCR7loCD127posCD27positiveCD28hiCD95+CD122+', [\n",
    "        'CD4+', 'CD45RA+', 'CD45RO-', '4-1BB-', 'CD62L+', 'CCR7', 'CD127+', 'CD28+.', 'CD95+.', 'CD122+.'\n",
    "    ], ['CD95', 'CD28', 'CD122']),\n",
    "    #('CD4+CD45RO/RBbright', ['CD4+', 'CD45RO/RB+.']),\n",
    "#     ('CD3CD4PBMC', ['CD3', 'CD4', 'PBMC'])\n",
    "#     'CD95CD4negativeCD45RApositiveCD45RO-CD62Lpos',\n",
    "#     'CD3CD4CD8alow',\n",
    "#     'CD3CD4PBMC',\n",
    "#     'CD3CD4CD99+IL-12',\n",
    "#     'Thy1.1+OT-1+CD8+',\n",
    "#     'CD8-4-1BB-CCR7highCD45RO+',\n",
    "#     'CD4-CD8+CD3+RORγt+T',\n",
    "#     'CD4+Foxp3gfp+',\n",
    "#     'CD4+CD45RO/RBbright',\n",
    "#     'CD4+CD25+CD45RO+IL-7Ralphalow',\n",
    "#     'CD4+CD8-Vbeta3hi'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 24, 24, [('CD4', True), ('+', True), ('CD45RA', True), ('+', True), ('CD45RO', True), ('-', True), ('4-1BB', True), ('-', True), ('CD62L', True), ('+', True), ('+', True), ('+', True), ('CCR7', True), ('lo', True), ('CD127', True), ('pos', True), ('CD27', True), ('positive', True), ('CD28', True), ('hi', True), ('CD95', True), ('+', True), ('CD122', True), ('+', True)])\n",
      "(2, 1, 24, 25, [('CD4', True), ('+', True), ('CD45RA', True), ('+', True), ('CD45', True), ('RO', False), ('-', True), ('4-1BB', True), ('-', True), ('CD62L', True), ('+', True), ('+', True), ('+', True), ('CCR7', True), ('lo', True), ('CD127', True), ('pos', True), ('CD27', True), ('positive', True), ('CD28', True), ('hi', True), ('CD95', True), ('+', True), ('CD122', True), ('+', True)])\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def match(string, words, max_unmatched_len):\n",
    "    \"\"\"Recursively break string input using known vocabulary (with unknown substrings of maximum size)\n",
    "    \n",
    "    Generates tuples with values so far for (in this order):\n",
    "        - count of unmatched characters\n",
    "        - count of unmatched words \n",
    "        - count of matched words\n",
    "        - list of (substring, is_known) tuples where substring is a word to be broken out and is_known\n",
    "            is a boolean indicator of whether or not that word was present in the provided word list\n",
    "    \"\"\"\n",
    "    if len(string) == 0:\n",
    "        yield 0, 0, 0, []\n",
    "        return\n",
    "    \n",
    "    # Loop through oov offsets up to at most `max_unmatched_len`\n",
    "    n = min(max_unmatched_len + 1, len(string))\n",
    "    matched = False\n",
    "    for u in range(n):\n",
    "        # Ignore u characters at beginning of current candidate and run\n",
    "        # prefix search for all substrings, taking the largest one possible\n",
    "        substr = string[u:]\n",
    "        for i in range(1, len(substr)+1):\n",
    "            prefix = substr[:i]\n",
    "            tail = None if i >= len(substr) else substr[:(i+1)]\n",
    "            if prefix not in words or (tail is not None and tail in words):\n",
    "                continue\n",
    "            # Recursion for substring starting at index u + i\n",
    "            for ctcu, ctwu, ctwm, tokens in match(substr[i:], words, max_unmatched_len):\n",
    "                matched = True\n",
    "                tknu, tknm = [] if u == 0 else [(string[:u], False)], [(prefix, True)]\n",
    "                yield ctcu + u, ctwu + len(tknu), ctwm + len(tknm), tknu + tknm + tokens \n",
    "        # Break when any oov offset results in a prefix hit \n",
    "        # (this minimizes the length of oov substrings)\n",
    "        if matched:\n",
    "            break\n",
    "    # Return trailing unmatched substring, if necessary\n",
    "    if not matched:\n",
    "        yield len(string), 1, 0, [(string, False)]\n",
    "            \n",
    "\n",
    "def split(string, words, max_unmatched_len=0, n_results=1, mode='most_words'):\n",
    "    \"\"\"Partition a string into words using a controlled vocabulary\n",
    "    \n",
    "    This is a solution to the Word Break Problem that includes:\n",
    "        - Unknown words of at most length `max_unmatched_len`\n",
    "        - Greedy maximum length known word partitioning\n",
    "        - Greedy minimum length unknown word partitioning\n",
    "        \n",
    "    Example:\n",
    "    \n",
    "        words = [\"this\", \"is\", \"the\", \"famous\", \"Word\", \"break\", \"b\", \"r\", \"e\", \"a\", \"k\", \"br\", \"problem\"]\n",
    "        string = \"WordbreakINGproblem\"\n",
    "        results = split(string, words, max_unmatched_len=3, mode='most_words', n_results=1)[0]\n",
    "        print([w if known else '|' + w + '|' for w, known in results[-1]])\n",
    "        >>> ['Word', 'br', 'e', 'a', 'k', '|ING|', 'problem']\n",
    "        \n",
    "        results = split(string, words, max_unmatched_len=3, mode='least_words', n_results=1)[0]\n",
    "        print([w if known else '|' + w + '|' for w, known in results[-1]])\n",
    "        >>> ['Word', 'break', '|ING|', 'problem']\n",
    "        \n",
    "    Returns:\n",
    "        A list of tuples with values (in this order):\n",
    "            - count of unmatched characters\n",
    "            - count of unmatched words \n",
    "            - count of matched words\n",
    "            - list of (substring, is_known) tuples where substring is a word to be broken out and is_known\n",
    "                is a boolean indicator of whether or not that word was present in the provided word list\n",
    "    \"\"\"\n",
    "    if mode not in ['most_words', 'least_words']:\n",
    "        raise ValueError(f'Mode must be one of \"most_words\" or \"least_words\" not \"{mode}\"')\n",
    "    # Limit max unmatched length to length of input string (default to 0 if None)\n",
    "    max_unmatched_len = min(max(max_unmatched_len or 0, 0), len(string))\n",
    "    mode = (-1, -1) if mode == 'least_words' else (-1, 1)\n",
    "    res = []\n",
    "\n",
    "    # Get unmatched char count, unmatched word count, matched word count, and token list (in that order)\n",
    "    for ctcu, ctwu, ctwm, tokens in match(string, words, max_unmatched_len):\n",
    "        ctwt = ctwu + ctwm\n",
    "        # Minimize number of unmatched characters, maximize or minimize \n",
    "        # number of total tokens based on mode\n",
    "        key = (ctcu * mode[0], ctwt * mode[1])\n",
    "        item = (key, ctcu, ctwu, ctwm, ctwt, tokens)\n",
    "        # Push onto bounded heap\n",
    "        (heapq.heappush if len(res) < n_results else heapq.heappushpop)(res, item)\n",
    "\n",
    "    # Return all requested results except for those used in sorting heap entry\n",
    "    return [r[1:] for r in heapq.nlargest(n_results, res)]\n",
    "\n",
    "for r in split(CASES[0][0], vocab, 8, n_results=2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = split(CASES[0][0], vocab, 8, n_results=1)[0][-1]\n",
    "# groups = [(k, list(g)) for k, g in itertools.groupby(results, lambda v: v[0] in vocab_sign_pos + vocab_sign_neg)]\n",
    "# for (k, g), (kn, gn) in list(zip(groups, groups[1:] + [(None, None)])):\n",
    "#     # If first token group is for signs or on the last token group, yield\n",
    "#     if k or kn is None:\n",
    "#         print(g)\n",
    "#     print(k, kn, gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class ProteinToken(object):\n",
    "    \n",
    "    def __init__(self, token_text, sign_text, sign_value):\n",
    "        self.token_text = token_text\n",
    "        self.sign_text = sign_text\n",
    "        self.sign_value = sign_value\n",
    "        self.text = (token_text or '') + (sign_text or '')\n",
    "        \n",
    "    @property\n",
    "    def sign_value_text(self):\n",
    "        sign = ''\n",
    "        if self.sign_value == 1:\n",
    "            sign = '⁺'\n",
    "        elif self.sign_value == -1:\n",
    "            sign = '⁻'\n",
    "        return sign\n",
    "    \n",
    "    def to_string(self, show_sign_text=True):\n",
    "        return ''.join([\n",
    "            self.token_text or '', \n",
    "            self.sign_value_text or '', \n",
    "            '(' + self.sign_text + ')' if self.sign_text and show_sign_text else ''\n",
    "        ])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.to_string(show_sign_text=True)\n",
    "\n",
    "        \n",
    "class ProteinTokenizer(object):\n",
    "    \n",
    "    def __init__(self, vocab_pr, vocab_sign_pos, vocab_sign_neg):\n",
    "        self.vocab = vocab_pr + vocab_sign_pos + vocab_sign_neg\n",
    "        self.vocab_sign = {**{v:1 for v in vocab_sign_pos}, **{v:-1 for v in vocab_sign_neg}}\n",
    "        \n",
    "    def tokenize(self, string, max_unmatched_len=8):\n",
    "        splits = split(string, self.vocab, max_unmatched_len=max_unmatched_len, n_results=1)\n",
    "        if not splits:\n",
    "            yield ProteinToken(string, None, 0)\n",
    "            return\n",
    "        \n",
    "        # Get last item in first result (i.e. list of tokens)\n",
    "        tokens = splits[0][-1]\n",
    "\n",
    "        # Group tokens into consecutive sign or non-sign lists\n",
    "        # (group by here will create a group every time that changes, not in aggregate by True/False)\n",
    "        groups = [(k, list(g)) for k, g in itertools.groupby(tokens, lambda v: v[0] in self.vocab_sign)]\n",
    "\n",
    "        # Yield collapsed tokens\n",
    "        none = [(None, None)]\n",
    "        for (kp, gp), (k, g), (kn, gn) in zip(none + groups[:-1], groups, groups[1:] + none):\n",
    "            sn_txt, pr_txt, sn_val = None, None, 0\n",
    "\n",
    "            # If on a non-sign group, pull in next group (for a sign) \n",
    "            if not k:\n",
    "                pr_txt = ''.join([v[0] for v in g])\n",
    "                sn_txt = None if gn is None else [v[0] for v in gn]\n",
    "            # Otherwise if the first group is a sign group\n",
    "            elif kp is None:\n",
    "                sn_txt = [v[0] for v in g]\n",
    "\n",
    "            # Resolve sign string to integer value\n",
    "            if sn_txt:\n",
    "                signs = set([self.vocab_sign.get(v, 0) for v in sn_txt]) - set([0])\n",
    "                # Only set sign if no conflicts exist\n",
    "                if len(signs) == 1:\n",
    "                    sn_val = list(signs)[0]\n",
    "                sn_txt = ''.join(sn_txt)\n",
    "                \n",
    "            # Yield if on a collapsed token group\n",
    "            if sn_txt or pr_txt:\n",
    "                yield ProteinToken(pr_txt, sn_txt, sn_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CD4⁺(+), CD45RA⁺(+), CD45RO⁻(-), 4-1BB⁻(-), CD62L⁺(+++), CCR7⁻(lo), CD127⁺(pos), CD27⁺(positive), CD28⁺(hi), CD95⁺(+), CD122⁺(+)]\n"
     ]
    }
   ],
   "source": [
    "def test_cases():\n",
    "    for c in CASES:\n",
    "        v = list(set(vocab) - set(c[2] if len(c) > 2 else []))\n",
    "        tokenizer = ProteinTokenizer(v, vocab_sign_pos, vocab_sign_neg)\n",
    "        #tokens = split(c[0], v, max_unmatched_len=8, n_results=1)\n",
    "        tokens = tokenizer.tokenize(c[0])\n",
    "        print(list(tokens))\n",
    "        \n",
    "test_cases()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
