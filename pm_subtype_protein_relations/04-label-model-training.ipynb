{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "%matplotlib inline\n",
    "%run env.py\n",
    "%run src/supervision.py\n",
    "session = SnorkelSession()\n",
    "classes = get_candidate_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.models import Candidate, GoldLabel\n",
    "candidate_class = classes.inducing_cytokine\n",
    "cands = session.query(candidate_class.subclass)\\\n",
    "    .filter(candidate_class.subclass.split == SPLIT_DEV).all()\n",
    "len(cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([c.gold_labels[0].value if c.gold_labels else np.nan for c in cands]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cytokine    673\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check on ordering (verify that type of first entity is always the same)\n",
    "pd.Series([c.get_parent().entity_types[c.get_contexts()[0].get_word_start()] for c in cands]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    342\n",
       "True     331\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([ c.get_contexts()[0].get_word_start() < c.get_contexts()[1].get_word_start()  for c in cands]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IL-12 induces not only Ifng expression1 but also T-bet,  which promotes the survival and proliferation of differentiating TH1 cells.   ',\n",
       " Span(\"b'IL-12'\", sentence=13515, chars=[0,4], words=[0,0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = cands[0]\n",
    "c.get_parent().text, c.get_contexts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = c.get_contexts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_entities(tokens, positions, markers=[], style=\"insert\"):\n",
    "    \"\"\"Adds special markers around tokens at specific positions (e.g., entities)\n",
    "    Args:\n",
    "        tokens: A list of tokens (the sentence)\n",
    "        positions:\n",
    "            1) A list of inclusive ranges (tuples) corresponding to the\n",
    "            token ranges of the entities in order. (Assumes each entity\n",
    "            has only one corresponding mention.)\n",
    "            OR\n",
    "            2) A dict of lists with keys corresponding to mention indices and\n",
    "            values corresponding to one or more inclusive ranges corresponding\n",
    "            to that mention. (Allows entities to potentially have multiple\n",
    "            mentions)\n",
    "        markers: A list of strings (length of 2 * the number of entities) to\n",
    "            use as markers of the entities.\n",
    "        style: Where to apply the markers:\n",
    "            'insert': Insert the markers as new tokens before/after each entity\n",
    "            'concatenate': Prepend/append the markers to the first/last token\n",
    "                of each entity\n",
    "            If the tokens are going to be input to an LSTM, then it is usually\n",
    "            best to use the 'insert' option; 'concatenate' may be better for\n",
    "            viewing.\n",
    "    Returns:\n",
    "        toks: An extended list of tokens with markers around the mentions\n",
    "    WARNING: if the marked token set will be used with pretrained embeddings,\n",
    "        provide markers that will not result in UNK embeddings!\n",
    "    Example:\n",
    "        Input:  (['The', 'cat', 'sat'], [(1,1)])\n",
    "        Output: ['The', '[[BEGIN0]]', 'cat', '[[END0]]', 'sat']\n",
    "    \"\"\"\n",
    "    if markers and len(markers) != 2 * len(positions):\n",
    "        msg = (\n",
    "            f\"Expected len(markers) == 2 * len(positions), \"\n",
    "            f\"but {len(markers)} != {2 * len(positions)}.\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    toks = list(tokens)\n",
    "\n",
    "    # markings will be of the form:\n",
    "    # [(position, entity_idx), (position, entity_idx), ...]\n",
    "    if isinstance(positions, list):\n",
    "        markings = [(position, idx) for idx, position in enumerate(positions)]\n",
    "    elif isinstance(positions, dict):\n",
    "        markings = []\n",
    "        for idx, v in positions.items():\n",
    "            for position in v:\n",
    "                markings.append((position, idx))\n",
    "    else:\n",
    "        msg = (\n",
    "            f\"Argument _positions_ must be a list or dict. \"\n",
    "            f\"Instead, got {type(positions)}\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    markings = sorted(markings)\n",
    "    for i, ((si, ei), idx) in enumerate(markings):\n",
    "        if markers:\n",
    "            start_marker = markers[2 * idx]\n",
    "            end_marker = markers[2 * idx + 1]\n",
    "        else:\n",
    "            start_marker = f\"[[BEGIN{idx}]]\"\n",
    "            end_marker = f\"[[END{idx}]]\"\n",
    "        if style == \"insert\":\n",
    "            toks.insert(si + 2 * i, start_marker)\n",
    "            toks.insert(ei + 2 * (i + 1), end_marker)\n",
    "        elif style == \"concatenate\":\n",
    "            toks[si] = start_marker + toks[si]\n",
    "            toks[ei] = toks[ei] + end_marker\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30167</td>\n",
       "      <td>1</td>\n",
       "      <td>[[ IL-12 ]] induces not only Ifng expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating &lt;&lt; TH1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30168</td>\n",
       "      <td>0</td>\n",
       "      <td>IL-12 induces not only [[ Ifng ]] expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating &lt;&lt; TH1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30169</td>\n",
       "      <td>1</td>\n",
       "      <td>In mice , [[ TGFβ ]] together with IL6 can activate antigen - responsive naïve CD4 + T cells to develop into &lt;&lt; Th17 &gt;&gt; cells [ 39 ] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30170</td>\n",
       "      <td>1</td>\n",
       "      <td>In mice , TGFβ together with [[ IL6 ]] can activate antigen - responsive naïve CD4 + T cells to develop into &lt;&lt; Th17 &gt;&gt; cells [ 39 ] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30171</td>\n",
       "      <td>0</td>\n",
       "      <td>Several findings suggest that during the initiation of a &lt;&lt; Th1 &gt;&gt; response , [[ IL-12 ]] is produced particularly by macrophages in response to certain microbial antigens , while NK cells are the main source of IFN-γ in response to IL-12 ( 7 , 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    guid  label  \\\n",
       "0  30167      1   \n",
       "1  30168      0   \n",
       "2  30169      1   \n",
       "3  30170      1   \n",
       "4  30171      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                        text  \n",
       "0                                                                                                   [[ IL-12 ]] induces not only Ifng expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating << TH1 >> cells .     \n",
       "1                                                                                                   IL-12 induces not only [[ Ifng ]] expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating << TH1 >> cells .     \n",
       "2                                                                                                                     In mice , [[ TGFβ ]] together with IL6 can activate antigen - responsive naïve CD4 + T cells to develop into << Th17 >> cells [ 39 ] .  \n",
       "3                                                                                                                     In mice , TGFβ together with [[ IL6 ]] can activate antigen - responsive naïve CD4 + T cells to develop into << Th17 >> cells [ 39 ] .  \n",
       "4  Several findings suggest that during the initiation of a << Th1 >> response , [[ IL-12 ]] is produced particularly by macrophages in response to certain microbial antigens , while NK cells are the main source of IFN-γ in response to IL-12 ( 7 , 1...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text(cand):\n",
    "    words = list(cand.get_parent().words)\n",
    "    spans = cand.get_contexts()\n",
    "    positions = [spans[0].get_word_range(), spans[1].get_word_range()]\n",
    "#     return ' '.join(mark_entities(words, positions, style='concatenate', markers=[\n",
    "#         '[[', ']]', '<<', '>>'\n",
    "#     ]))\n",
    "    return ' '.join(mark_entities(words, positions, style='insert', markers=[\n",
    "        '[[', ']]', '<<', '>>'\n",
    "    ]))\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    dict(\n",
    "        guid=c.id,\n",
    "        text=get_text(c),\n",
    "        label=c.gold_labels[0].value if c.gold_labels else 0\n",
    "    ) \n",
    "    for c in cands\n",
    "])\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    540\n",
       "1    133\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.802377\n",
       "1    0.197623\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1234c6278>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Approximate token list length distribution\n",
    "df['text'].str.split().str.len().plot('hist', bins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1234dbcc0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# String length distribution\n",
    "%matplotlib inline\n",
    "df['text'].str.len().plot('hist', bins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1300fc7f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = '/Users/eczech/tmp/scibert/scibert_scivocab_uncased'\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "df['text'].apply(lambda v: len(tokenizer.tokenize(v))).hist(bins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/bert_test/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/bert_test/input/train.csv'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/tmp/bert_test/input/train.csv'\n",
    "df.to_csv(path, encoding='utf-8', index=False)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/bert_test/input/dev.csv'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/tmp/bert_test/input/dev.csv'\n",
    "df.to_csv(path, encoding='utf-8', index=False)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Command\n",
    "\n",
    "```bash\n",
    "cd /Users/eczech/repos/hammer/t-cell-relation-extraction/pm_subtype_protein_relations/bert\n",
    "python training.py \\\n",
    "--data_dir=/tmp/bert_test/input \\\n",
    "--bert_model=/Users/eczech/tmp/scibert/scibert_scivocab_uncased \\\n",
    "--task_name=tcre \\\n",
    "--output_dir=/tmp/bert_test/output \\\n",
    "--max_seq_length=128 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_lower_case \\\n",
    "--train_batch_size=32 \\\n",
    "--num_train_epochs=3 \\\n",
    "--no_cuda \n",
    "```\n",
    "\n",
    "Eval only (note the bert_model change):\n",
    "\n",
    "```bash\n",
    "cd /Users/eczech/repos/hammer/t-cell-relation-extraction/pm_subtype_protein_relations/bert\n",
    "python training.py \\\n",
    "--data_dir=/tmp/bert_test/input \\\n",
    "--bert_model=/tmp/bert_test/output \\\n",
    "--task_name=tcre \\\n",
    "--output_dir=/tmp/bert_test/output \\\n",
    "--max_seq_length=128 \\\n",
    "--do_eval \\\n",
    "--do_lower_case \\\n",
    "--train_batch_size=16 \\\n",
    "--num_train_epochs=3 \\\n",
    "--no_cuda \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/Users/eczech/tmp/scibert/scibert_scivocab_uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import word2vec\n",
    "nlp = spacy.load('en_core_sci_md')\n",
    "model = word2vec.load('/Users/eczech/Downloads/PubMed-and-PMC-w2v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.words = set(model.vocab)\n",
    "        \n",
    "    def indices(self, sentence):\n",
    "        tokens = [w.text for w in nlp(sentence)]\n",
    "        tokens = [t if t in self.words else 'UNK' for t in tokens]\n",
    "        indices = [self.model.ix(t) for t in tokens]\n",
    "        return np.array(indices), np.array(tokens)\n",
    "    \n",
    "    def embeddings(self, sentence):\n",
    "        indices, tokens = self.indices(sentence)\n",
    "        return np.stack([model.vectors[i] for i in indices]), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(model)\n",
    "idx, tkns = vectorizer.indices(\"il2 does stuff blahlbk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([864129,    484,  48451, 265456]),\n",
       " array(['il2', 'does', 'stuff', 'UNK'], dtype='<U5'))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  61,   61, 5691,   60,   60, 1588]),\n",
       " array(['[', '[', 'IL-12', ']', ']', 'induces'], dtype='<U7'))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.indices('[[ IL-12 ]] induces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   86,    86, 28399,   198,   198,    30]),\n",
       " array(['<', '<', 'TH1', '>', '>', 'cells'], dtype='<U5'))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.indices('<< TH1 >> cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30167</td>\n",
       "      <td>1</td>\n",
       "      <td>[[ IL-12 ]] induces not only Ifng expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating &lt;&lt; TH1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30168</td>\n",
       "      <td>0</td>\n",
       "      <td>IL-12 induces not only [[ Ifng ]] expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating &lt;&lt; TH1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30169</td>\n",
       "      <td>1</td>\n",
       "      <td>In mice , [[ TGFβ ]] together with IL6 can activate antigen - responsive naïve CD4 + T cells to develop into &lt;&lt; Th17 &gt;&gt; cells [ 39 ] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30170</td>\n",
       "      <td>1</td>\n",
       "      <td>In mice , TGFβ together with [[ IL6 ]] can activate antigen - responsive naïve CD4 + T cells to develop into &lt;&lt; Th17 &gt;&gt; cells [ 39 ] .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30171</td>\n",
       "      <td>0</td>\n",
       "      <td>Several findings suggest that during the initiation of a &lt;&lt; Th1 &gt;&gt; response , [[ IL-12 ]] is produced particularly by macrophages in response to certain microbial antigens , while NK cells are the main source of IFN-γ in response to IL-12 ( 7 , 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    guid  label  \\\n",
       "0  30167      1   \n",
       "1  30168      0   \n",
       "2  30169      1   \n",
       "3  30170      1   \n",
       "4  30171      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                        text  \n",
       "0                                                                                                   [[ IL-12 ]] induces not only Ifng expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating << TH1 >> cells .     \n",
       "1                                                                                                   IL-12 induces not only [[ Ifng ]] expression1 but also T - bet ,   which promotes the survival and proliferation of differentiating << TH1 >> cells .     \n",
       "2                                                                                                                     In mice , [[ TGFβ ]] together with IL6 can activate antigen - responsive naïve CD4 + T cells to develop into << Th17 >> cells [ 39 ] .  \n",
       "3                                                                                                                     In mice , TGFβ together with [[ IL6 ]] can activate antigen - responsive naïve CD4 + T cells to develop into << Th17 >> cells [ 39 ] .  \n",
       "4  Several findings suggest that during the initiation of a << Th1 >> response , [[ IL-12 ]] is produced particularly by macrophages in response to certain microbial antigens , while NK cells are the main source of IFN-γ in response to IL-12 ( 7 , 1...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(df, vectorizer):\n",
    "    X_train, Y_train = [], []\n",
    "    for i, r in df.iterrows():\n",
    "        indices, tokens = vectorizer.indices(r['text'])\n",
    "        X_train.append(indices)\n",
    "        Y_train.append(r['label'])\n",
    "        assert r['label'] in [0, 1]\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, Y = featurize(df, vectorizer)\n",
    "idx_train, idx_test = train_test_split(np.arange(len(X)), test_size=.3, stratify=Y)\n",
    "X_train, Y_train = X[idx_train], Y[idx_train]\n",
    "X_test, Y_test = X[idx_test], Y[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((673,), dtype('int64'))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, Y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = None\n",
    "        \n",
    "    def build_model(self, embedding_model, hidden_dim=50, num_layers=1, \n",
    "                     dropout=0, bidirectional=False, max_seq_len=128,\n",
    "                     **kwargs):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cardinality = 2\n",
    "        \n",
    "        # Initialize and freeze embedding layer\n",
    "        self.embedding_dim = self.embedding_model.vectors.shape[1]\n",
    "        self.embedding_len = self.embedding_model.vectors.shape[0]\n",
    "        self.embedding = nn.Embedding(self.embedding_len, self.embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding.weight.data = torch.FloatTensor(self.embedding_model.vectors)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim,\n",
    "                            num_layers=num_layers, bidirectional=bidirectional,\n",
    "                            dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dim * self.num_directions, \n",
    "                                      self.cardinality if self.cardinality > 2 else 1)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def check_model(self, lr):\n",
    "        if not hasattr(self, 'loss'):\n",
    "            # Define loss and marginals ops\n",
    "            if self.cardinality > 2:\n",
    "                self.loss = cross_entropy_loss\n",
    "            else:\n",
    "                self.loss = nn.BCEWithLogitsLoss()\n",
    "        if not hasattr(self, 'optimizer'):\n",
    "            params = [param for param in self.parameters() if param.requires_grad == True]\n",
    "            self.optimizer = optim.Adam(params, lr)\n",
    "        \n",
    "        \n",
    "    def forward(self, X, hidden_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Tensor with token indices (batch_size, max_batch_seq_length)\n",
    "        \"\"\"\n",
    "        seq_lengths = torch.zeros((X.size(0)), dtype=torch.long)\n",
    "        for i in range(X.size(0)):\n",
    "            for j in range(X.size(1)):\n",
    "                if X[i, j] == 0:\n",
    "                    seq_lengths[i] = j\n",
    "                    break\n",
    "                seq_lengths[i] = X.size(1)\n",
    "\n",
    "        seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "        X = X[perm_idx, :]\n",
    "        inv_perm_idx = torch.tensor([i for i, _ in sorted(enumerate(perm_idx), key=lambda idx: idx[1])], dtype=torch.long)\n",
    "\n",
    "        encoded_X = self.embedding(X)\n",
    "        encoded_X = pack_padded_sequence(encoded_X, seq_lengths, batch_first=True)\n",
    "        _, (ht, _) = self.lstm(encoded_X, hidden_state)\n",
    "        output = ht[-1] if self.num_directions == 1 else torch.cat((ht[0], ht[1]), dim=1)\n",
    "\n",
    "        return self.output_layer(self.dropout_layer(output[inv_perm_idx, :]))\n",
    "    \n",
    "    def _pytorch_outputs(self, X, batch_size):\n",
    "        n = len(X)\n",
    "        hidden_state = self.initialize_hidden_state(n)\n",
    "        max_batch_length = min(max(map(len, X)), self.max_seq_len)\n",
    "\n",
    "        padded_X = torch.zeros((n, max_batch_length), dtype=torch.long)\n",
    "        for idx, seq in enumerate(X):\n",
    "            # TODO: Don't instantiate tensor for each row\n",
    "            nseq = min(len(seq), max_batch_length)\n",
    "            padded_X[idx, :nseq] = torch.LongTensor(seq[:nseq])\n",
    "\n",
    "        output = self.forward(padded_X, hidden_state)\n",
    "        if self.cardinality == 2:\n",
    "            return output.view(-1)\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    \n",
    "    def run_training(self, X_train, Y_train, n_epochs=25, lr=0.01, batch_size=32, seed=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train: Array of numpy arrays containing each token index (each row can vary in length)\n",
    "            Y_train: 1D numpy array of labels (0 or 1)\n",
    "        \"\"\"\n",
    "        random_state = np.random.RandomState(seed=seed)\n",
    "        n = len(X_train)\n",
    "        train_idxs = np.arange(n)\n",
    "        \n",
    "#         self.build_model(**kwargs)\n",
    "#         self.check_model(lr)\n",
    "        \n",
    "        # Run mini-batch SGD\n",
    "        st = time()\n",
    "        for epoch in range(n_epochs):\n",
    "    \n",
    "            # Shuffle training data\n",
    "            train_idxs = random_state.permutation(list(range(n)))\n",
    "            Y_train = Y_train[train_idxs]\n",
    "            X_train = X_train[train_idxs]\n",
    "            batch_size = min(batch_size, n) \n",
    "            epoch_losses = []\n",
    "\n",
    "            nn.Module.train(self)\n",
    "            for batch in range(0, n, batch_size):\n",
    "                \n",
    "                # zero gradients for each batch\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if batch_size > len(X_train[batch:batch+batch_size]):\n",
    "                    batch_size = len(X_train[batch:batch+batch_size])\n",
    "\n",
    "                output = self._pytorch_outputs(X_train[batch:batch + batch_size], None)\n",
    "                \n",
    "                #Calculate loss\n",
    "                calculated_loss = self.loss(output, torch.Tensor(Y_train[batch:batch+batch_size]))\n",
    "                \n",
    "                #Compute gradient\n",
    "                calculated_loss.backward()\n",
    "                \n",
    "                #Step on the optimizer\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_losses.append(calculated_loss)\n",
    "                \n",
    "                msg = \"Epoch {} ({:.2f}s)\\tAverage loss={:.6f}, Current loss={:.6f}\".format(\n",
    "                    epoch+1, time() - st, torch.stack(epoch_losses).mean(), calculated_loss)\n",
    "                print(msg)\n",
    "        print('Training complete')\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_dim),\n",
    "            torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_dim)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lstm.build_model(model, hidden_dim=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([4087446, 200]))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_lstm.embedding.weight.dtype, emb_lstm.embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), (4087446, 200), 4087446)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.dtype, model.vectors.shape, len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lstm.check_model(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (0.18s)\tAverage loss=0.639588, Current loss=0.639588\n",
      "Epoch 1 (0.35s)\tAverage loss=0.630245, Current loss=0.620902\n",
      "Epoch 1 (0.52s)\tAverage loss=0.606187, Current loss=0.558069\n",
      "Epoch 1 (0.69s)\tAverage loss=0.587409, Current loss=0.531074\n",
      "Epoch 1 (0.86s)\tAverage loss=0.541987, Current loss=0.360301\n",
      "Epoch 1 (1.15s)\tAverage loss=0.595443, Current loss=0.862725\n",
      "Epoch 1 (1.30s)\tAverage loss=0.595876, Current loss=0.598468\n",
      "Epoch 1 (1.46s)\tAverage loss=0.582870, Current loss=0.491829\n",
      "Epoch 1 (1.60s)\tAverage loss=0.571870, Current loss=0.483874\n",
      "Epoch 1 (1.76s)\tAverage loss=0.572264, Current loss=0.575812\n",
      "Epoch 1 (1.93s)\tAverage loss=0.557555, Current loss=0.410461\n",
      "Epoch 1 (2.08s)\tAverage loss=0.545254, Current loss=0.409946\n",
      "Epoch 1 (2.22s)\tAverage loss=0.537473, Current loss=0.444100\n",
      "Epoch 1 (2.38s)\tAverage loss=0.522430, Current loss=0.326875\n",
      "Epoch 1 (2.52s)\tAverage loss=0.511631, Current loss=0.360446\n",
      "Epoch 1 (2.68s)\tAverage loss=0.506869, Current loss=0.435440\n",
      "Epoch 1 (2.83s)\tAverage loss=0.496906, Current loss=0.337494\n",
      "Epoch 1 (3.00s)\tAverage loss=0.504290, Current loss=0.629812\n",
      "Epoch 1 (3.31s)\tAverage loss=0.508766, Current loss=0.589340\n",
      "Epoch 1 (3.45s)\tAverage loss=0.517734, Current loss=0.688115\n",
      "Epoch 1 (3.61s)\tAverage loss=0.511434, Current loss=0.385453\n",
      "Epoch 1 (3.64s)\tAverage loss=0.571832, Current loss=1.840173\n",
      "Epoch 2 (3.72s)\tAverage loss=0.215994, Current loss=0.215994\n",
      "Epoch 2 (3.77s)\tAverage loss=0.217810, Current loss=0.219626\n",
      "Epoch 2 (3.79s)\tAverage loss=0.655729, Current loss=1.531568\n",
      "Epoch 2 (3.82s)\tAverage loss=0.560572, Current loss=0.275102\n",
      "Epoch 2 (3.86s)\tAverage loss=0.499174, Current loss=0.253581\n",
      "Epoch 2 (3.94s)\tAverage loss=0.679673, Current loss=1.582167\n",
      "Epoch 2 (3.97s)\tAverage loss=0.628413, Current loss=0.320853\n",
      "Epoch 2 (3.99s)\tAverage loss=0.590146, Current loss=0.322274\n",
      "Epoch 2 (4.04s)\tAverage loss=0.557509, Current loss=0.296412\n",
      "Epoch 2 (4.06s)\tAverage loss=0.619443, Current loss=1.176858\n",
      "Epoch 2 (4.08s)\tAverage loss=0.596594, Current loss=0.368098\n",
      "Epoch 2 (4.10s)\tAverage loss=0.577937, Current loss=0.372712\n",
      "Epoch 2 (4.12s)\tAverage loss=0.620651, Current loss=1.133215\n",
      "Epoch 2 (4.14s)\tAverage loss=0.605599, Current loss=0.409933\n",
      "Epoch 2 (4.16s)\tAverage loss=0.590854, Current loss=0.384410\n",
      "Epoch 2 (4.18s)\tAverage loss=0.578549, Current loss=0.393984\n",
      "Epoch 2 (4.21s)\tAverage loss=0.569087, Current loss=0.417692\n",
      "Epoch 2 (4.24s)\tAverage loss=0.554608, Current loss=0.308457\n",
      "Epoch 2 (4.27s)\tAverage loss=0.587933, Current loss=1.187801\n",
      "Epoch 2 (4.35s)\tAverage loss=0.576300, Current loss=0.355268\n",
      "Epoch 2 (4.37s)\tAverage loss=0.604611, Current loss=1.170833\n",
      "Epoch 2 (4.42s)\tAverage loss=0.590370, Current loss=0.291302\n",
      "Epoch 2 (4.47s)\tAverage loss=0.577463, Current loss=0.293506\n",
      "Epoch 2 (4.50s)\tAverage loss=0.565196, Current loss=0.283059\n",
      "Epoch 2 (4.54s)\tAverage loss=0.557595, Current loss=0.375170\n",
      "Epoch 2 (4.55s)\tAverage loss=0.552832, Current loss=0.433748\n",
      "Epoch 2 (4.57s)\tAverage loss=0.546044, Current loss=0.369554\n",
      "Epoch 2 (4.60s)\tAverage loss=0.539937, Current loss=0.375055\n",
      "Epoch 2 (4.63s)\tAverage loss=0.577794, Current loss=1.637787\n",
      "Epoch 2 (4.65s)\tAverage loss=0.612332, Current loss=1.613938\n",
      "Epoch 2 (4.68s)\tAverage loss=0.599864, Current loss=0.225837\n",
      "Epoch 2 (4.71s)\tAverage loss=0.628652, Current loss=1.521070\n",
      "Epoch 2 (4.73s)\tAverage loss=0.617533, Current loss=0.261740\n",
      "Epoch 2 (4.75s)\tAverage loss=0.610179, Current loss=0.367477\n",
      "Epoch 2 (4.82s)\tAverage loss=0.601628, Current loss=0.310919\n",
      "Epoch 2 (4.87s)\tAverage loss=0.595246, Current loss=0.371843\n",
      "Epoch 2 (4.90s)\tAverage loss=0.589035, Current loss=0.365448\n",
      "Epoch 2 (4.94s)\tAverage loss=0.581365, Current loss=0.297579\n",
      "Epoch 2 (4.98s)\tAverage loss=0.597240, Current loss=1.200509\n",
      "Epoch 2 (5.05s)\tAverage loss=0.589834, Current loss=0.300969\n",
      "Epoch 2 (5.10s)\tAverage loss=0.583997, Current loss=0.350537\n",
      "Epoch 2 (5.16s)\tAverage loss=0.578322, Current loss=0.345631\n",
      "Epoch 2 (5.18s)\tAverage loss=0.572914, Current loss=0.345772\n",
      "Epoch 2 (5.21s)\tAverage loss=0.587978, Current loss=1.235725\n",
      "Epoch 2 (5.46s)\tAverage loss=0.581565, Current loss=0.299425\n",
      "Epoch 2 (5.48s)\tAverage loss=0.576075, Current loss=0.329007\n",
      "Epoch 2 (5.50s)\tAverage loss=0.570575, Current loss=0.317578\n",
      "Epoch 2 (5.52s)\tAverage loss=0.585308, Current loss=1.277785\n",
      "Epoch 2 (5.55s)\tAverage loss=0.579597, Current loss=0.305449\n",
      "Epoch 2 (5.58s)\tAverage loss=0.574082, Current loss=0.303830\n",
      "Epoch 2 (5.61s)\tAverage loss=0.568662, Current loss=0.297684\n",
      "Epoch 2 (5.65s)\tAverage loss=0.563426, Current loss=0.296364\n",
      "Epoch 2 (5.72s)\tAverage loss=0.557183, Current loss=0.232580\n",
      "Epoch 2 (5.73s)\tAverage loss=0.573120, Current loss=1.417790\n",
      "Epoch 2 (5.75s)\tAverage loss=0.588176, Current loss=1.401165\n",
      "Epoch 2 (5.79s)\tAverage loss=0.583319, Current loss=0.316168\n",
      "Epoch 2 (5.83s)\tAverage loss=0.577560, Current loss=0.255077\n",
      "Epoch 2 (5.86s)\tAverage loss=0.572377, Current loss=0.276930\n",
      "Epoch 2 (5.90s)\tAverage loss=0.566931, Current loss=0.251083\n",
      "Epoch 2 (5.92s)\tAverage loss=0.580969, Current loss=1.409206\n",
      "Epoch 2 (5.96s)\tAverage loss=0.576052, Current loss=0.281016\n",
      "Epoch 2 (5.99s)\tAverage loss=0.571451, Current loss=0.290837\n",
      "Epoch 2 (6.08s)\tAverage loss=0.588015, Current loss=1.614952\n",
      "Epoch 2 (6.14s)\tAverage loss=0.583655, Current loss=0.308969\n",
      "Epoch 2 (6.17s)\tAverage loss=0.596526, Current loss=1.420293\n",
      "Epoch 2 (6.21s)\tAverage loss=0.592071, Current loss=0.302496\n",
      "Epoch 2 (6.25s)\tAverage loss=0.588057, Current loss=0.323146\n",
      "Epoch 2 (6.28s)\tAverage loss=0.600211, Current loss=1.414488\n",
      "Epoch 2 (6.31s)\tAverage loss=0.610072, Current loss=1.280654\n",
      "Epoch 2 (6.36s)\tAverage loss=0.606270, Current loss=0.343925\n",
      "Epoch 2 (6.43s)\tAverage loss=0.602577, Current loss=0.344047\n",
      "Epoch 2 (6.46s)\tAverage loss=0.599077, Current loss=0.350581\n",
      "Epoch 2 (6.48s)\tAverage loss=0.596430, Current loss=0.405829\n",
      "Epoch 2 (6.52s)\tAverage loss=0.593301, Current loss=0.364908\n",
      "Epoch 2 (6.56s)\tAverage loss=0.590276, Current loss=0.366393\n",
      "Epoch 2 (6.61s)\tAverage loss=0.587165, Current loss=0.353865\n",
      "Epoch 2 (6.64s)\tAverage loss=0.594860, Current loss=1.179694\n",
      "Epoch 2 (6.67s)\tAverage loss=0.591911, Current loss=0.364826\n",
      "Epoch 2 (6.70s)\tAverage loss=0.599647, Current loss=1.203052\n",
      "Epoch 2 (6.72s)\tAverage loss=0.597049, Current loss=0.391817\n",
      "Epoch 2 (6.77s)\tAverage loss=0.594344, Current loss=0.377930\n",
      "Epoch 2 (6.78s)\tAverage loss=0.591773, Current loss=0.383498\n",
      "Epoch 2 (6.83s)\tAverage loss=0.588955, Current loss=0.357940\n",
      "Epoch 2 (6.85s)\tAverage loss=0.586194, Current loss=0.357001\n",
      "Epoch 2 (6.88s)\tAverage loss=0.592980, Current loss=1.163038\n",
      "Epoch 2 (6.96s)\tAverage loss=0.589833, Current loss=0.322317\n",
      "Epoch 2 (6.98s)\tAverage loss=0.586973, Current loss=0.341019\n",
      "Epoch 2 (7.06s)\tAverage loss=0.583850, Current loss=0.312153\n",
      "Epoch 2 (7.14s)\tAverage loss=0.580766, Current loss=0.309346\n",
      "Epoch 2 (7.16s)\tAverage loss=0.588344, Current loss=1.262759\n",
      "Epoch 2 (7.20s)\tAverage loss=0.585416, Current loss=0.321895\n",
      "Epoch 2 (7.22s)\tAverage loss=0.582550, Current loss=0.321739\n",
      "Epoch 2 (7.29s)\tAverage loss=0.579265, Current loss=0.277070\n",
      "Epoch 2 (7.32s)\tAverage loss=0.576394, Current loss=0.309436\n",
      "Epoch 2 (7.35s)\tAverage loss=0.573458, Current loss=0.297406\n",
      "Epoch 2 (7.56s)\tAverage loss=0.569952, Current loss=0.236913\n",
      "Epoch 2 (7.61s)\tAverage loss=0.567004, Current loss=0.284017\n",
      "Epoch 2 (7.64s)\tAverage loss=0.563919, Current loss=0.264650\n",
      "Epoch 2 (7.68s)\tAverage loss=0.573372, Current loss=1.499789\n",
      "Epoch 2 (7.71s)\tAverage loss=0.582374, Current loss=1.473613\n",
      "Epoch 2 (7.76s)\tAverage loss=0.579058, Current loss=0.247380\n",
      "Epoch 2 (7.78s)\tAverage loss=0.576042, Current loss=0.271508\n",
      "Epoch 2 (7.82s)\tAverage loss=0.572978, Current loss=0.260416\n",
      "Epoch 2 (7.89s)\tAverage loss=0.569426, Current loss=0.203536\n",
      "Epoch 2 (7.90s)\tAverage loss=0.578397, Current loss=1.511435\n",
      "Epoch 2 (7.92s)\tAverage loss=0.575426, Current loss=0.263471\n",
      "Epoch 2 (7.97s)\tAverage loss=0.572557, Current loss=0.268440\n",
      "Epoch 2 (8.01s)\tAverage loss=0.569616, Current loss=0.254894\n",
      "Epoch 2 (8.03s)\tAverage loss=0.566767, Current loss=0.259062\n",
      "Epoch 2 (8.07s)\tAverage loss=0.575734, Current loss=1.553153\n",
      "Epoch 2 (8.09s)\tAverage loss=0.572695, Current loss=0.238369\n",
      "Epoch 2 (8.12s)\tAverage loss=0.581030, Current loss=1.506274\n",
      "Epoch 2 (8.15s)\tAverage loss=0.588537, Current loss=1.429285\n",
      "Epoch 2 (8.17s)\tAverage loss=0.585753, Current loss=0.271169\n",
      "Epoch 2 (8.19s)\tAverage loss=0.583189, Current loss=0.290845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (8.24s)\tAverage loss=0.580334, Current loss=0.252041\n",
      "Epoch 2 (8.29s)\tAverage loss=0.577540, Current loss=0.253504\n",
      "Epoch 2 (8.31s)\tAverage loss=0.583927, Current loss=1.331187\n",
      "Epoch 2 (8.35s)\tAverage loss=0.581640, Current loss=0.311767\n",
      "Epoch 2 (8.38s)\tAverage loss=0.589211, Current loss=1.490176\n",
      "Epoch 2 (8.41s)\tAverage loss=0.587174, Current loss=0.342643\n",
      "Epoch 2 (8.42s)\tAverage loss=0.585121, Current loss=0.336778\n",
      "Epoch 2 (8.46s)\tAverage loss=0.583107, Current loss=0.337425\n",
      "Epoch 2 (8.49s)\tAverage loss=0.581273, Current loss=0.355645\n",
      "Epoch 2 (8.53s)\tAverage loss=0.579331, Current loss=0.338534\n",
      "Epoch 2 (8.55s)\tAverage loss=0.577451, Current loss=0.342408\n",
      "Epoch 2 (8.58s)\tAverage loss=0.575582, Current loss=0.340154\n",
      "Epoch 2 (8.65s)\tAverage loss=0.572949, Current loss=0.238481\n",
      "Epoch 2 (8.67s)\tAverage loss=0.571028, Current loss=0.325209\n",
      "Epoch 2 (8.70s)\tAverage loss=0.569029, Current loss=0.311092\n",
      "Epoch 2 (8.73s)\tAverage loss=0.567020, Current loss=0.305919\n",
      "Epoch 2 (8.76s)\tAverage loss=0.565004, Current loss=0.300944\n",
      "Epoch 2 (8.78s)\tAverage loss=0.562924, Current loss=0.288325\n",
      "Epoch 2 (8.81s)\tAverage loss=0.560687, Current loss=0.263179\n",
      "Epoch 2 (8.85s)\tAverage loss=0.558421, Current loss=0.254744\n",
      "Epoch 2 (8.88s)\tAverage loss=0.556058, Current loss=0.237008\n",
      "Epoch 2 (8.93s)\tAverage loss=0.553713, Current loss=0.234799\n",
      "Epoch 2 (8.95s)\tAverage loss=0.561602, Current loss=1.642371\n",
      "Epoch 2 (8.99s)\tAverage loss=0.559069, Current loss=0.209577\n",
      "Epoch 2 (9.01s)\tAverage loss=0.556518, Current loss=0.201946\n",
      "Epoch 2 (9.04s)\tAverage loss=0.553975, Current loss=0.197938\n",
      "Epoch 2 (9.08s)\tAverage loss=0.551370, Current loss=0.184088\n",
      "Epoch 2 (9.10s)\tAverage loss=0.548758, Current loss=0.177838\n",
      "Epoch 2 (9.17s)\tAverage loss=0.546204, Current loss=0.180924\n",
      "Epoch 2 (9.23s)\tAverage loss=0.543536, Current loss=0.159396\n",
      "Epoch 2 (9.25s)\tAverage loss=0.540879, Current loss=0.155683\n",
      "Epoch 2 (9.29s)\tAverage loss=0.538191, Current loss=0.145706\n",
      "Epoch 2 (9.36s)\tAverage loss=0.535261, Current loss=0.104508\n",
      "Epoch 2 (9.41s)\tAverage loss=0.532525, Current loss=0.127645\n",
      "Epoch 2 (9.45s)\tAverage loss=0.529840, Current loss=0.129785\n",
      "Epoch 2 (9.47s)\tAverage loss=0.541000, Current loss=2.214923\n",
      "Epoch 2 (9.51s)\tAverage loss=0.538241, Current loss=0.121720\n",
      "Epoch 2 (9.56s)\tAverage loss=0.535441, Current loss=0.109791\n",
      "Epoch 2 (9.76s)\tAverage loss=0.546554, Current loss=2.246857\n",
      "Epoch 2 (9.80s)\tAverage loss=0.543810, Current loss=0.121280\n",
      "Epoch 2 (9.82s)\tAverage loss=0.541123, Current loss=0.124632\n",
      "Epoch 2 (9.86s)\tAverage loss=0.538481, Current loss=0.126268\n",
      "Epoch 2 (9.89s)\tAverage loss=0.535872, Current loss=0.126292\n",
      "Epoch 2 (9.92s)\tAverage loss=0.533314, Current loss=0.129179\n",
      "Epoch 2 (9.96s)\tAverage loss=0.530704, Current loss=0.115688\n",
      "Epoch 2 (9.99s)\tAverage loss=0.540523, Current loss=2.111562\n",
      "Epoch 2 (10.01s)\tAverage loss=0.538006, Current loss=0.132715\n",
      "Epoch 2 (10.04s)\tAverage loss=0.535557, Current loss=0.138859\n",
      "Epoch 2 (10.06s)\tAverage loss=0.533139, Current loss=0.138973\n",
      "Epoch 2 (10.08s)\tAverage loss=0.530773, Current loss=0.142739\n",
      "Epoch 2 (10.10s)\tAverage loss=0.528426, Current loss=0.141205\n",
      "Epoch 2 (10.14s)\tAverage loss=0.526116, Current loss=0.142715\n",
      "Epoch 2 (10.16s)\tAverage loss=0.523817, Current loss=0.139764\n",
      "Epoch 2 (10.19s)\tAverage loss=0.521548, Current loss=0.140331\n",
      "Epoch 2 (10.27s)\tAverage loss=0.519059, Current loss=0.098452\n",
      "Epoch 2 (10.30s)\tAverage loss=0.528081, Current loss=2.061841\n",
      "Epoch 2 (10.33s)\tAverage loss=0.525790, Current loss=0.133985\n",
      "Epoch 2 (10.36s)\tAverage loss=0.534505, Current loss=2.033473\n",
      "Epoch 2 (10.41s)\tAverage loss=0.532129, Current loss=0.121143\n",
      "Epoch 2 (10.44s)\tAverage loss=0.529947, Current loss=0.150326\n",
      "Epoch 2 (10.46s)\tAverage loss=0.527668, Current loss=0.128713\n",
      "Epoch 2 (10.49s)\tAverage loss=0.525599, Current loss=0.161463\n",
      "Epoch 2 (10.52s)\tAverage loss=0.533178, Current loss=1.874682\n",
      "Epoch 2 (10.59s)\tAverage loss=0.530873, Current loss=0.120572\n",
      "Epoch 2 (10.62s)\tAverage loss=0.528899, Current loss=0.175549\n",
      "Epoch 2 (10.65s)\tAverage loss=0.526996, Current loss=0.184503\n",
      "Epoch 2 (10.68s)\tAverage loss=0.525132, Current loss=0.187812\n",
      "Epoch 2 (10.71s)\tAverage loss=0.531942, Current loss=1.771358\n",
      "Epoch 2 (10.76s)\tAverage loss=0.529850, Current loss=0.146886\n",
      "Epoch 2 (10.78s)\tAverage loss=0.528070, Current loss=0.200688\n",
      "Epoch 2 (10.81s)\tAverage loss=0.526322, Current loss=0.202957\n",
      "Epoch 2 (10.85s)\tAverage loss=0.524624, Current loss=0.208681\n",
      "Epoch 2 (10.88s)\tAverage loss=0.522965, Current loss=0.212730\n",
      "Epoch 2 (10.91s)\tAverage loss=0.521304, Current loss=0.209143\n",
      "Epoch 2 (10.95s)\tAverage loss=0.519652, Current loss=0.207303\n",
      "Epoch 2 (11.02s)\tAverage loss=0.517649, Current loss=0.137115\n",
      "Epoch 2 (11.04s)\tAverage loss=0.516013, Current loss=0.203521\n",
      "Epoch 2 (11.06s)\tAverage loss=0.522209, Current loss=1.711857\n",
      "Epoch 2 (11.10s)\tAverage loss=0.520542, Current loss=0.198819\n",
      "Epoch 2 (11.14s)\tAverage loss=0.526763, Current loss=1.733587\n",
      "Epoch 2 (11.22s)\tAverage loss=0.524762, Current loss=0.134685\n",
      "Epoch 2 (11.27s)\tAverage loss=0.522842, Current loss=0.146387\n",
      "Epoch 2 (11.30s)\tAverage loss=0.521284, Current loss=0.214518\n",
      "Epoch 2 (11.37s)\tAverage loss=0.519359, Current loss=0.138189\n",
      "Epoch 2 (11.40s)\tAverage loss=0.517864, Current loss=0.220231\n",
      "Epoch 2 (11.44s)\tAverage loss=0.516373, Current loss=0.218161\n",
      "Epoch 2 (11.47s)\tAverage loss=0.514863, Current loss=0.211491\n",
      "Epoch 2 (11.53s)\tAverage loss=0.513402, Current loss=0.218136\n",
      "Epoch 2 (11.55s)\tAverage loss=0.519042, Current loss=1.664032\n",
      "Epoch 2 (11.62s)\tAverage loss=0.526975, Current loss=2.145424\n",
      "Epoch 2 (11.64s)\tAverage loss=0.532372, Current loss=1.638586\n",
      "Epoch 2 (11.67s)\tAverage loss=0.530957, Current loss=0.239495\n",
      "Epoch 2 (11.70s)\tAverage loss=0.529576, Current loss=0.243803\n",
      "Epoch 2 (11.92s)\tAverage loss=0.528229, Current loss=0.248066\n",
      "Epoch 2 (11.94s)\tAverage loss=0.532675, Current loss=1.461817\n",
      "Epoch 2 (11.98s)\tAverage loss=0.531391, Current loss=0.261739\n",
      "Epoch 2 (12.01s)\tAverage loss=0.530187, Current loss=0.276062\n",
      "Epoch 2 (12.04s)\tAverage loss=0.528977, Current loss=0.272447\n",
      "Epoch 2 (12.08s)\tAverage loss=0.527835, Current loss=0.284737\n",
      "Epoch 2 (12.13s)\tAverage loss=0.526714, Current loss=0.286761\n",
      "Epoch 2 (12.17s)\tAverage loss=0.525565, Current loss=0.278467\n",
      "Epoch 2 (12.18s)\tAverage loss=0.529621, Current loss=1.405792\n",
      "Epoch 2 (12.21s)\tAverage loss=0.528486, Current loss=0.282309\n",
      "Epoch 2 (12.25s)\tAverage loss=0.527339, Current loss=0.277245\n",
      "Epoch 2 (12.28s)\tAverage loss=0.525855, Current loss=0.200803\n",
      "Epoch 2 (12.32s)\tAverage loss=0.529912, Current loss=1.422403\n",
      "Epoch 2 (12.36s)\tAverage loss=0.528805, Current loss=0.284124\n",
      "Epoch 2 (12.40s)\tAverage loss=0.527305, Current loss=0.194331\n",
      "Epoch 2 (12.45s)\tAverage loss=0.525813, Current loss=0.193163\n",
      "Epoch 2 (12.48s)\tAverage loss=0.524332, Current loss=0.192506\n",
      "Epoch 2 (12.51s)\tAverage loss=0.523244, Current loss=0.278563\n",
      "Epoch 2 (12.56s)\tAverage loss=0.528847, Current loss=1.795120\n",
      "Epoch 2 (12.59s)\tAverage loss=0.527365, Current loss=0.190942\n",
      "Epoch 2 (12.62s)\tAverage loss=0.526290, Current loss=0.281212\n",
      "Epoch 2 (12.66s)\tAverage loss=0.525232, Current loss=0.282995\n",
      "Epoch 2 (12.71s)\tAverage loss=0.523774, Current loss=0.188325\n",
      "Epoch 2 (12.75s)\tAverage loss=0.522698, Current loss=0.274221\n",
      "Epoch 2 (12.79s)\tAverage loss=0.526624, Current loss=1.437387\n",
      "Epoch 2 (12.82s)\tAverage loss=0.530418, Current loss=1.414393\n",
      "Epoch 2 (12.84s)\tAverage loss=0.529332, Current loss=0.275230\n",
      "Epoch 2 (12.86s)\tAverage loss=0.528290, Current loss=0.283450\n",
      "Epoch 2 (12.90s)\tAverage loss=0.527245, Current loss=0.280584\n",
      "Epoch 2 (12.97s)\tAverage loss=0.525906, Current loss=0.208609\n",
      "Epoch 2 (12.99s)\tAverage loss=0.524901, Current loss=0.285798\n",
      "Epoch 2 (13.01s)\tAverage loss=0.523909, Current loss=0.286694\n",
      "Epoch 2 (13.06s)\tAverage loss=0.522906, Current loss=0.282238\n",
      "Epoch 2 (13.09s)\tAverage loss=0.521923, Current loss=0.284955\n",
      "Epoch 2 (13.12s)\tAverage loss=0.525601, Current loss=1.415796\n",
      "Epoch 2 (13.15s)\tAverage loss=0.524589, Current loss=0.278652\n",
      "Epoch 2 (13.18s)\tAverage loss=0.523182, Current loss=0.179909\n",
      "Epoch 2 (13.22s)\tAverage loss=0.522112, Current loss=0.259767\n",
      "Epoch 2 (13.26s)\tAverage loss=0.525964, Current loss=1.473730\n",
      "Epoch 2 (13.29s)\tAverage loss=0.524872, Current loss=0.255183\n",
      "Epoch 2 (13.31s)\tAverage loss=0.523787, Current loss=0.254613\n",
      "Epoch 2 (13.33s)\tAverage loss=0.522746, Current loss=0.263596\n",
      "Epoch 2 (13.36s)\tAverage loss=0.521696, Current loss=0.259086\n",
      "Epoch 2 (13.40s)\tAverage loss=0.520610, Current loss=0.248086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (13.43s)\tAverage loss=0.524564, Current loss=1.520973\n",
      "Epoch 2 (13.46s)\tAverage loss=0.523457, Current loss=0.243255\n",
      "Epoch 2 (13.49s)\tAverage loss=0.527344, Current loss=1.514842\n",
      "Epoch 2 (13.51s)\tAverage loss=0.526281, Current loss=0.255170\n",
      "Epoch 2 (13.53s)\tAverage loss=0.530051, Current loss=1.494989\n",
      "Epoch 2 (13.56s)\tAverage loss=0.529035, Current loss=0.268095\n",
      "Epoch 2 (13.59s)\tAverage loss=0.528003, Current loss=0.261604\n",
      "Epoch 2 (13.62s)\tAverage loss=0.526988, Current loss=0.264126\n",
      "Epoch 2 (13.65s)\tAverage loss=0.526029, Current loss=0.276813\n",
      "Epoch 2 (13.69s)\tAverage loss=0.525020, Current loss=0.261543\n",
      "Epoch 2 (13.73s)\tAverage loss=0.524011, Current loss=0.259645\n",
      "Epoch 2 (13.78s)\tAverage loss=0.522653, Current loss=0.165609\n",
      "Epoch 2 (13.82s)\tAverage loss=0.521634, Current loss=0.252687\n",
      "Epoch 2 (14.07s)\tAverage loss=0.520338, Current loss=0.176837\n",
      "Epoch 2 (14.14s)\tAverage loss=0.519045, Current loss=0.175234\n",
      "Epoch 2 (14.23s)\tAverage loss=0.517744, Current loss=0.170116\n",
      "Epoch 2 (14.25s)\tAverage loss=0.516696, Current loss=0.236082\n",
      "Epoch 2 (14.30s)\tAverage loss=0.515624, Current loss=0.227085\n",
      "Epoch 2 (14.38s)\tAverage loss=0.514360, Current loss=0.173204\n",
      "Epoch 2 (14.41s)\tAverage loss=0.513268, Current loss=0.217294\n",
      "Epoch 2 (14.44s)\tAverage loss=0.512144, Current loss=0.206305\n",
      "Epoch 2 (14.47s)\tAverage loss=0.510988, Current loss=0.195337\n",
      "Epoch 2 (14.52s)\tAverage loss=0.515612, Current loss=1.782830\n",
      "Epoch 2 (14.55s)\tAverage loss=0.514440, Current loss=0.191898\n",
      "Epoch 2 (14.59s)\tAverage loss=0.513241, Current loss=0.182579\n",
      "Epoch 2 (14.63s)\tAverage loss=0.512039, Current loss=0.179021\n",
      "Epoch 2 (14.71s)\tAverage loss=0.510623, Current loss=0.116977\n",
      "Epoch 2 (14.73s)\tAverage loss=0.509429, Current loss=0.176250\n",
      "Epoch 2 (14.77s)\tAverage loss=0.508213, Current loss=0.167835\n",
      "Epoch 2 (14.80s)\tAverage loss=0.507001, Current loss=0.166372\n",
      "Epoch 2 (14.84s)\tAverage loss=0.505546, Current loss=0.095097\n",
      "Epoch 2 (14.92s)\tAverage loss=0.504111, Current loss=0.098159\n",
      "Epoch 2 (14.94s)\tAverage loss=0.502856, Current loss=0.146253\n",
      "Epoch 2 (14.99s)\tAverage loss=0.501386, Current loss=0.082687\n",
      "Epoch 2 (15.01s)\tAverage loss=0.500112, Current loss=0.135654\n",
      "Epoch 2 (15.04s)\tAverage loss=0.498820, Current loss=0.127851\n",
      "Epoch 2 (15.07s)\tAverage loss=0.497516, Current loss=0.122058\n",
      "Epoch 2 (15.14s)\tAverage loss=0.496056, Current loss=0.074238\n",
      "Epoch 2 (15.17s)\tAverage loss=0.502055, Current loss=2.241519\n",
      "Epoch 2 (15.21s)\tAverage loss=0.500556, Current loss=0.064377\n",
      "Epoch 2 (15.24s)\tAverage loss=0.499231, Current loss=0.112586\n",
      "Epoch 2 (15.27s)\tAverage loss=0.505061, Current loss=2.213154\n",
      "Epoch 2 (15.29s)\tAverage loss=0.503707, Current loss=0.105748\n",
      "Epoch 2 (15.37s)\tAverage loss=0.502252, Current loss=0.072806\n",
      "Epoch 2 (15.41s)\tAverage loss=0.500975, Current loss=0.123000\n",
      "Epoch 2 (15.46s)\tAverage loss=0.506433, Current loss=2.127676\n",
      "Epoch 2 (15.48s)\tAverage loss=0.505198, Current loss=0.137092\n",
      "Epoch 2 (15.55s)\tAverage loss=0.503786, Current loss=0.081516\n",
      "Epoch 2 (15.59s)\tAverage loss=0.502588, Current loss=0.143221\n",
      "Epoch 2 (15.62s)\tAverage loss=0.507461, Current loss=1.974161\n",
      "Epoch 2 (15.67s)\tAverage loss=0.506047, Current loss=0.078953\n",
      "Epoch 2 (15.71s)\tAverage loss=0.504948, Current loss=0.172178\n",
      "Epoch 2 (15.73s)\tAverage loss=0.503849, Current loss=0.169558\n",
      "Epoch 2 (15.76s)\tAverage loss=0.502780, Current loss=0.176870\n",
      "Epoch 2 (15.79s)\tAverage loss=0.501722, Current loss=0.177850\n",
      "Epoch 2 (15.81s)\tAverage loss=0.500688, Current loss=0.183201\n",
      "Epoch 2 (15.84s)\tAverage loss=0.499647, Current loss=0.179149\n",
      "Epoch 2 (15.92s)\tAverage loss=0.498349, Current loss=0.097215\n",
      "Epoch 2 (15.94s)\tAverage loss=0.497328, Current loss=0.180898\n",
      "Epoch 2 (16.02s)\tAverage loss=0.496037, Current loss=0.094477\n",
      "Epoch 2 (16.04s)\tAverage loss=0.495031, Current loss=0.181062\n",
      "Epoch 2 (16.28s)\tAverage loss=0.494015, Current loss=0.176170\n",
      "Epoch 2 (16.30s)\tAverage loss=0.500706, Current loss=2.601513\n",
      "Epoch 2 (16.35s)\tAverage loss=0.499653, Current loss=0.168052\n",
      "Epoch 2 (16.38s)\tAverage loss=0.498620, Current loss=0.172173\n",
      "Epoch 2 (16.42s)\tAverage loss=0.497587, Current loss=0.170310\n",
      "Epoch 2 (16.45s)\tAverage loss=0.501809, Current loss=1.844315\n",
      "Epoch 2 (16.50s)\tAverage loss=0.507571, Current loss=2.345485\n",
      "Epoch 2 (16.54s)\tAverage loss=0.506545, Current loss=0.178396\n",
      "Epoch 2 (16.56s)\tAverage loss=0.505556, Current loss=0.188098\n",
      "Epoch 2 (16.61s)\tAverage loss=0.504575, Current loss=0.188773\n",
      "Epoch 2 (16.64s)\tAverage loss=0.503630, Current loss=0.198254\n",
      "Epoch 2 (16.67s)\tAverage loss=0.507460, Current loss=1.748341\n",
      "Epoch 2 (16.72s)\tAverage loss=0.506516, Current loss=0.199565\n",
      "Epoch 2 (16.75s)\tAverage loss=0.505597, Current loss=0.206303\n",
      "Epoch 2 (16.77s)\tAverage loss=0.504704, Current loss=0.212452\n",
      "Epoch 2 (16.80s)\tAverage loss=0.503809, Current loss=0.210449\n",
      "Epoch 2 (16.85s)\tAverage loss=0.507334, Current loss=1.667051\n",
      "Epoch 2 (16.88s)\tAverage loss=0.506459, Current loss=0.217475\n",
      "Epoch 2 (16.92s)\tAverage loss=0.505590, Current loss=0.217977\n",
      "Epoch 2 (16.95s)\tAverage loss=0.508932, Current loss=1.618639\n",
      "Epoch 2 (16.98s)\tAverage loss=0.508082, Current loss=0.225161\n",
      "Epoch 2 (17.02s)\tAverage loss=0.507268, Current loss=0.235410\n",
      "Epoch 2 (17.04s)\tAverage loss=0.510400, Current loss=1.559440\n",
      "Epoch 2 (17.08s)\tAverage loss=0.513349, Current loss=1.504108\n",
      "Epoch 2 (17.10s)\tAverage loss=0.512576, Current loss=0.252283\n",
      "Epoch 2 (17.13s)\tAverage loss=0.511836, Current loss=0.261711\n",
      "Epoch 2 (17.16s)\tAverage loss=0.514562, Current loss=1.438625\n",
      "Epoch 2 (17.25s)\tAverage loss=0.513724, Current loss=0.228697\n",
      "Epoch 2 (17.29s)\tAverage loss=0.513072, Current loss=0.290757\n",
      "Epoch 2 (17.32s)\tAverage loss=0.512424, Current loss=0.290815\n",
      "Epoch 2 (17.36s)\tAverage loss=0.511788, Current loss=0.293783\n",
      "Epoch 2 (17.39s)\tAverage loss=0.514277, Current loss=1.370502\n",
      "Epoch 2 (17.42s)\tAverage loss=0.517001, Current loss=1.456753\n",
      "Epoch 2 (17.47s)\tAverage loss=0.516398, Current loss=0.307737\n",
      "Epoch 2 (17.50s)\tAverage loss=0.515836, Current loss=0.320685\n",
      "Epoch 2 (17.52s)\tAverage loss=0.515282, Current loss=0.322597\n",
      "Epoch 2 (17.56s)\tAverage loss=0.514760, Current loss=0.332544\n",
      "Epoch 2 (17.57s)\tAverage loss=0.514248, Current loss=0.335233\n",
      "Epoch 2 (17.60s)\tAverage loss=0.513704, Current loss=0.322535\n",
      "Epoch 2 (17.64s)\tAverage loss=0.513165, Current loss=0.323484\n",
      "Epoch 2 (17.66s)\tAverage loss=0.512636, Current loss=0.326009\n",
      "Epoch 2 (17.68s)\tAverage loss=0.512097, Current loss=0.321025\n",
      "Epoch 2 (17.72s)\tAverage loss=0.511530, Current loss=0.310367\n",
      "Epoch 2 (17.77s)\tAverage loss=0.510908, Current loss=0.289651\n",
      "Epoch 2 (17.79s)\tAverage loss=0.510317, Current loss=0.299267\n",
      "Epoch 2 (17.82s)\tAverage loss=0.509734, Current loss=0.300797\n",
      "Epoch 2 (17.84s)\tAverage loss=0.512195, Current loss=1.395766\n",
      "Epoch 2 (17.89s)\tAverage loss=0.511551, Current loss=0.279701\n",
      "Epoch 2 (17.97s)\tAverage loss=0.510725, Current loss=0.212631\n",
      "Epoch 2 (18.01s)\tAverage loss=0.510072, Current loss=0.273685\n",
      "Epoch 2 (18.03s)\tAverage loss=0.509402, Current loss=0.266226\n",
      "Epoch 2 (18.07s)\tAverage loss=0.508732, Current loss=0.264645\n",
      "Epoch 2 (18.10s)\tAverage loss=0.508043, Current loss=0.256701\n",
      "Epoch 2 (18.12s)\tAverage loss=0.510726, Current loss=1.492712\n",
      "Epoch 2 (18.16s)\tAverage loss=0.510024, Current loss=0.252464\n",
      "Epoch 2 (18.19s)\tAverage loss=0.509315, Current loss=0.248228\n",
      "Epoch 2 (18.41s)\tAverage loss=0.512170, Current loss=1.565936\n",
      "Epoch 2 (18.44s)\tAverage loss=0.514928, Current loss=1.535066\n",
      "Epoch 2 (18.47s)\tAverage loss=0.514212, Current loss=0.248615\n",
      "Epoch 2 (18.49s)\tAverage loss=0.513504, Current loss=0.250365\n",
      "Epoch 2 (18.53s)\tAverage loss=0.512801, Current loss=0.250310\n",
      "Epoch 2 (18.58s)\tAverage loss=0.512101, Current loss=0.250600\n",
      "Epoch 2 (18.61s)\tAverage loss=0.511405, Current loss=0.250104\n",
      "Epoch 2 (18.63s)\tAverage loss=0.514085, Current loss=1.522018\n",
      "Epoch 2 (18.66s)\tAverage loss=0.513379, Current loss=0.247144\n",
      "Epoch 2 (18.69s)\tAverage loss=0.512686, Current loss=0.250781\n",
      "Epoch 2 (18.72s)\tAverage loss=0.511991, Current loss=0.248404\n",
      "Epoch 2 (18.75s)\tAverage loss=0.511300, Current loss=0.248989\n",
      "Epoch 2 (18.82s)\tAverage loss=0.510451, Current loss=0.186691\n",
      "Epoch 2 (18.85s)\tAverage loss=0.509752, Current loss=0.242835\n",
      "Epoch 2 (18.93s)\tAverage loss=0.508949, Current loss=0.201541\n",
      "Epoch 2 (18.96s)\tAverage loss=0.508240, Current loss=0.235745\n",
      "Epoch 2 (18.98s)\tAverage loss=0.507524, Current loss=0.232127\n",
      "Epoch 2 (19.03s)\tAverage loss=0.506791, Current loss=0.223786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (19.06s)\tAverage loss=0.506062, Current loss=0.223693\n",
      "Epoch 2 (19.09s)\tAverage loss=0.505312, Current loss=0.214649\n",
      "Epoch 2 (19.13s)\tAverage loss=0.504559, Current loss=0.211570\n",
      "Epoch 2 (19.17s)\tAverage loss=0.507610, Current loss=1.697460\n",
      "Epoch 2 (19.21s)\tAverage loss=0.506827, Current loss=0.200709\n",
      "Epoch 2 (19.25s)\tAverage loss=0.506040, Current loss=0.197380\n",
      "Epoch 2 (19.30s)\tAverage loss=0.505256, Current loss=0.197108\n",
      "Epoch 2 (19.31s)\tAverage loss=0.504467, Current loss=0.193755\n",
      "Epoch 2 (19.35s)\tAverage loss=0.503675, Current loss=0.190642\n",
      "Epoch 2 (19.42s)\tAverage loss=0.502743, Current loss=0.133886\n",
      "Epoch 2 (19.47s)\tAverage loss=0.501939, Current loss=0.182578\n",
      "Epoch 2 (19.51s)\tAverage loss=0.501120, Current loss=0.175351\n",
      "Epoch 2 (19.56s)\tAverage loss=0.500295, Current loss=0.170963\n",
      "Epoch 2 (19.59s)\tAverage loss=0.499462, Current loss=0.166463\n",
      "Epoch 2 (19.63s)\tAverage loss=0.498628, Current loss=0.164126\n",
      "Epoch 2 (19.66s)\tAverage loss=0.497783, Current loss=0.157830\n",
      "Epoch 2 (19.73s)\tAverage loss=0.496934, Current loss=0.154887\n",
      "Epoch 2 (19.76s)\tAverage loss=0.500593, Current loss=1.978816\n",
      "Epoch 2 (19.78s)\tAverage loss=0.499724, Current loss=0.147818\n",
      "Epoch 2 (19.83s)\tAverage loss=0.498853, Current loss=0.145254\n",
      "Epoch 2 (19.90s)\tAverage loss=0.497878, Current loss=0.100911\n",
      "Epoch 2 (19.94s)\tAverage loss=0.497004, Current loss=0.140412\n",
      "Epoch 2 (19.98s)\tAverage loss=0.496128, Current loss=0.138089\n",
      "Epoch 2 (20.00s)\tAverage loss=0.495256, Current loss=0.137817\n",
      "Epoch 2 (20.03s)\tAverage loss=0.499032, Current loss=2.050667\n",
      "Epoch 2 (20.11s)\tAverage loss=0.498049, Current loss=0.093189\n",
      "Epoch 2 (20.13s)\tAverage loss=0.497178, Current loss=0.137643\n",
      "Epoch 2 (20.15s)\tAverage loss=0.496310, Current loss=0.136641\n",
      "Epoch 2 (20.19s)\tAverage loss=0.495444, Current loss=0.136255\n",
      "Epoch 2 (20.22s)\tAverage loss=0.494583, Current loss=0.136455\n",
      "Epoch 2 (20.24s)\tAverage loss=0.498344, Current loss=2.066529\n",
      "Epoch 2 (20.27s)\tAverage loss=0.502034, Current loss=2.044317\n",
      "Epoch 2 (20.30s)\tAverage loss=0.505592, Current loss=1.996620\n",
      "Epoch 2 (20.33s)\tAverage loss=0.504748, Current loss=0.150319\n",
      "Epoch 2 (20.37s)\tAverage loss=0.503924, Current loss=0.157012\n",
      "Epoch 2 (20.59s)\tAverage loss=0.507207, Current loss=1.892375\n",
      "Epoch 2 (20.64s)\tAverage loss=0.510368, Current loss=1.847483\n",
      "Epoch 2 (20.67s)\tAverage loss=0.509604, Current loss=0.185760\n",
      "Epoch 2 (20.70s)\tAverage loss=0.512329, Current loss=1.670588\n",
      "Epoch 2 (20.73s)\tAverage loss=0.511616, Current loss=0.207985\n",
      "Epoch 2 (20.76s)\tAverage loss=0.514078, Current loss=1.565138\n",
      "Epoch 2 (20.78s)\tAverage loss=0.513429, Current loss=0.235680\n",
      "Epoch 2 (20.81s)\tAverage loss=0.512735, Current loss=0.214979\n",
      "Epoch 2 (20.83s)\tAverage loss=0.512197, Current loss=0.281036\n",
      "Epoch 2 (20.87s)\tAverage loss=0.514365, Current loss=1.448596\n",
      "Epoch 2 (20.90s)\tAverage loss=0.516208, Current loss=1.312279\n",
      "Epoch 2 (20.93s)\tAverage loss=0.515681, Current loss=0.287756\n",
      "Epoch 2 (20.95s)\tAverage loss=0.517464, Current loss=1.290983\n",
      "Epoch 2 (20.99s)\tAverage loss=0.517025, Current loss=0.326424\n",
      "Epoch 2 (21.04s)\tAverage loss=0.516660, Current loss=0.357495\n",
      "Epoch 2 (21.08s)\tAverage loss=0.516159, Current loss=0.297093\n",
      "Epoch 2 (21.13s)\tAverage loss=0.515674, Current loss=0.303227\n",
      "Epoch 2 (21.16s)\tAverage loss=0.515347, Current loss=0.371850\n",
      "Epoch 2 (21.19s)\tAverage loss=0.515058, Current loss=0.387787\n",
      "Epoch 2 (21.20s)\tAverage loss=0.514794, Current loss=0.398571\n",
      "Epoch 2 (21.25s)\tAverage loss=0.514424, Current loss=0.350604\n",
      "Epoch 2 (21.33s)\tAverage loss=0.513833, Current loss=0.252174\n",
      "Epoch 2 (21.38s)\tAverage loss=0.513334, Current loss=0.291840\n",
      "Epoch 2 (21.40s)\tAverage loss=0.514926, Current loss=1.223474\n",
      "Epoch 2 (21.43s)\tAverage loss=0.514556, Current loss=0.349169\n",
      "Epoch 2 (21.50s)\tAverage loss=0.514144, Current loss=0.329936\n",
      "Epoch 2 (21.55s)\tAverage loss=0.513755, Current loss=0.339751\n",
      "Epoch 2 (21.58s)\tAverage loss=0.513373, Current loss=0.341634\n",
      "Epoch 2 (21.62s)\tAverage loss=0.512972, Current loss=0.332497\n",
      "Epoch 2 (21.69s)\tAverage loss=0.512306, Current loss=0.212215\n",
      "Epoch 2 (21.71s)\tAverage loss=0.513959, Current loss=1.260874\n",
      "Epoch 2 (21.76s)\tAverage loss=0.513473, Current loss=0.293445\n",
      "Epoch 2 (21.82s)\tAverage loss=0.512975, Current loss=0.286684\n",
      "Epoch 2 (21.86s)\tAverage loss=0.512596, Current loss=0.340213\n",
      "Epoch 2 (21.89s)\tAverage loss=0.514228, Current loss=1.258474\n",
      "Epoch 2 (21.92s)\tAverage loss=0.513789, Current loss=0.313356\n",
      "Epoch 2 (21.94s)\tAverage loss=0.515454, Current loss=1.277872\n",
      "Epoch 2 (21.96s)\tAverage loss=0.514840, Current loss=0.233149\n",
      "Epoch 2 (22.03s)\tAverage loss=0.514117, Current loss=0.181515\n",
      "Epoch 2 (22.08s)\tAverage loss=0.513490, Current loss=0.224225\n",
      "Epoch 2 (22.15s)\tAverage loss=0.512859, Current loss=0.221406\n",
      "Epoch 2 (22.17s)\tAverage loss=0.512229, Current loss=0.220720\n",
      "Epoch 2 (22.19s)\tAverage loss=0.513882, Current loss=1.280828\n",
      "Epoch 2 (22.22s)\tAverage loss=0.513449, Current loss=0.312108\n",
      "Epoch 2 (22.25s)\tAverage loss=0.513008, Current loss=0.307581\n",
      "Epoch 2 (22.29s)\tAverage loss=0.512565, Current loss=0.305586\n",
      "Epoch 2 (22.33s)\tAverage loss=0.514307, Current loss=1.329733\n",
      "Epoch 2 (22.36s)\tAverage loss=0.513905, Current loss=0.325333\n",
      "Epoch 2 (22.38s)\tAverage loss=0.515550, Current loss=1.288702\n",
      "Epoch 2 (22.43s)\tAverage loss=0.518219, Current loss=1.775363\n",
      "Epoch 2 (22.50s)\tAverage loss=0.517437, Current loss=0.148326\n",
      "Epoch 2 (22.52s)\tAverage loss=0.517018, Current loss=0.318759\n",
      "Epoch 2 (22.55s)\tAverage loss=0.516638, Current loss=0.336460\n",
      "Epoch 2 (22.76s)\tAverage loss=0.516254, Current loss=0.333999\n",
      "Epoch 2 (22.78s)\tAverage loss=0.517720, Current loss=1.215183\n",
      "Epoch 2 (22.81s)\tAverage loss=0.519224, Current loss=1.236870\n",
      "Epoch 2 (22.86s)\tAverage loss=0.518600, Current loss=0.220226\n",
      "Epoch 2 (22.94s)\tAverage loss=0.517852, Current loss=0.159360\n",
      "Epoch 2 (22.98s)\tAverage loss=0.517245, Current loss=0.225920\n",
      "Epoch 2 (23.01s)\tAverage loss=0.516977, Current loss=0.388134\n",
      "Epoch 2 (23.03s)\tAverage loss=0.516625, Current loss=0.346953\n",
      "Epoch 2 (23.05s)\tAverage loss=0.517900, Current loss=1.134092\n",
      "Epoch 2 (23.13s)\tAverage loss=0.517132, Current loss=0.145042\n",
      "Epoch 2 (23.16s)\tAverage loss=0.516553, Current loss=0.235691\n",
      "Epoch 2 (23.24s)\tAverage loss=0.515791, Current loss=0.145578\n",
      "Epoch 2 (23.26s)\tAverage loss=0.515536, Current loss=0.391483\n",
      "Epoch 2 (23.29s)\tAverage loss=0.516742, Current loss=1.105159\n",
      "Epoch 2 (23.32s)\tAverage loss=0.516521, Current loss=0.408543\n",
      "Epoch 2 (23.35s)\tAverage loss=0.517908, Current loss=1.197654\n",
      "Epoch 2 (23.42s)\tAverage loss=0.517107, Current loss=0.123869\n",
      "Epoch 2 (23.45s)\tAverage loss=0.518356, Current loss=1.132803\n",
      "Epoch 2 (23.48s)\tAverage loss=0.518075, Current loss=0.379597\n",
      "Epoch 2 (23.52s)\tAverage loss=0.517889, Current loss=0.425678\n",
      "Epoch 2 (23.56s)\tAverage loss=0.517258, Current loss=0.204819\n",
      "Epoch 2 (23.58s)\tAverage loss=0.518502, Current loss=1.135660\n",
      "Epoch 2 (23.59s)\tAverage loss=0.518274, Current loss=0.405022\n",
      "Epoch 2 (23.64s)\tAverage loss=0.518041, Current loss=0.402075\n",
      "Epoch 2 (23.66s)\tAverage loss=0.517847, Current loss=0.420752\n",
      "Epoch 2 (23.69s)\tAverage loss=0.517624, Current loss=0.406514\n",
      "Epoch 2 (23.74s)\tAverage loss=0.517352, Current loss=0.381106\n",
      "Epoch 2 (23.76s)\tAverage loss=0.516681, Current loss=0.179476\n",
      "Epoch 2 (23.79s)\tAverage loss=0.516337, Current loss=0.343346\n",
      "Epoch 2 (23.82s)\tAverage loss=0.518027, Current loss=1.369994\n",
      "Epoch 2 (23.84s)\tAverage loss=0.517584, Current loss=0.293608\n",
      "Epoch 2 (23.87s)\tAverage loss=0.517146, Current loss=0.295651\n",
      "Epoch 2 (23.89s)\tAverage loss=0.518907, Current loss=1.411668\n",
      "Epoch 2 (23.93s)\tAverage loss=0.518458, Current loss=0.290617\n",
      "Epoch 2 (23.96s)\tAverage loss=0.518012, Current loss=0.290983\n",
      "Epoch 2 (23.98s)\tAverage loss=0.517566, Current loss=0.289932\n",
      "Epoch 2 (24.03s)\tAverage loss=0.516804, Current loss=0.127637\n",
      "Epoch 2 (24.10s)\tAverage loss=0.516019, Current loss=0.113927\n",
      "Epoch 2 (24.14s)\tAverage loss=0.515529, Current loss=0.263943\n",
      "Epoch 2 (24.18s)\tAverage loss=0.515025, Current loss=0.255965\n",
      "Epoch 2 (24.21s)\tAverage loss=0.514480, Current loss=0.233823\n",
      "Epoch 2 (24.24s)\tAverage loss=0.516600, Current loss=1.610605\n",
      "Epoch 2 (24.28s)\tAverage loss=0.515780, Current loss=0.091741\n",
      "Epoch 2 (24.35s)\tAverage loss=0.514875, Current loss=0.046564\n",
      "Epoch 2 (24.38s)\tAverage loss=0.518652, Current loss=2.478845\n",
      "Epoch 2 (24.41s)\tAverage loss=0.518098, Current loss=0.229902\n",
      "Epoch 2 (24.46s)\tAverage loss=0.517557, Current loss=0.235767\n",
      "Epoch 2 (24.48s)\tAverage loss=0.517030, Current loss=0.242031\n",
      "Epoch 2 (24.50s)\tAverage loss=0.516275, Current loss=0.121150\n",
      "Epoch 2 (24.53s)\tAverage loss=0.515763, Current loss=0.247428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (24.56s)\tAverage loss=0.517724, Current loss=1.547391\n",
      "Epoch 2 (24.64s)\tAverage loss=0.516923, Current loss=0.095603\n",
      "Epoch 2 (24.68s)\tAverage loss=0.516402, Current loss=0.241545\n",
      "Epoch 2 (24.70s)\tAverage loss=0.515903, Current loss=0.252479\n",
      "Epoch 2 (24.91s)\tAverage loss=0.517796, Current loss=1.519314\n",
      "Epoch 2 (24.93s)\tAverage loss=0.517098, Current loss=0.146959\n",
      "Epoch 2 (24.96s)\tAverage loss=0.518924, Current loss=1.488995\n",
      "Epoch 2 (24.98s)\tAverage loss=0.518460, Current loss=0.271381\n",
      "Epoch 2 (25.01s)\tAverage loss=0.520108, Current loss=1.398314\n",
      "Epoch 2 (25.08s)\tAverage loss=0.519349, Current loss=0.114369\n",
      "Epoch 2 (25.11s)\tAverage loss=0.518946, Current loss=0.303034\n",
      "Epoch 2 (25.14s)\tAverage loss=0.518560, Current loss=0.311928\n",
      "Epoch 2 (25.19s)\tAverage loss=0.518168, Current loss=0.307782\n",
      "Epoch 2 (25.22s)\tAverage loss=0.517798, Current loss=0.318658\n",
      "Epoch 2 (25.24s)\tAverage loss=0.517428, Current loss=0.318081\n",
      "Epoch 2 (25.27s)\tAverage loss=0.517038, Current loss=0.305958\n",
      "Epoch 2 (25.36s)\tAverage loss=0.516294, Current loss=0.114150\n",
      "Epoch 2 (25.38s)\tAverage loss=0.515903, Current loss=0.303869\n",
      "Epoch 2 (25.40s)\tAverage loss=0.517441, Current loss=1.352857\n",
      "Epoch 2 (25.45s)\tAverage loss=0.517038, Current loss=0.297661\n",
      "Epoch 2 (25.52s)\tAverage loss=0.516231, Current loss=0.076368\n",
      "Epoch 2 (25.60s)\tAverage loss=0.515424, Current loss=0.074794\n",
      "Epoch 2 (25.64s)\tAverage loss=0.515025, Current loss=0.296885\n",
      "Epoch 2 (25.69s)\tAverage loss=0.516587, Current loss=1.372391\n",
      "Epoch 2 (25.74s)\tAverage loss=0.519136, Current loss=1.918413\n",
      "Epoch 2 (25.77s)\tAverage loss=0.518737, Current loss=0.299198\n",
      "Epoch 2 (25.81s)\tAverage loss=0.518122, Current loss=0.179367\n",
      "Epoch 2 (25.84s)\tAverage loss=0.517741, Current loss=0.307639\n",
      "Epoch 2 (25.88s)\tAverage loss=0.517355, Current loss=0.304033\n",
      "Epoch 2 (25.95s)\tAverage loss=0.516557, Current loss=0.074439\n",
      "Epoch 2 (25.98s)\tAverage loss=0.516176, Current loss=0.304658\n",
      "Epoch 2 (26.01s)\tAverage loss=0.515791, Current loss=0.301774\n",
      "Epoch 2 (26.05s)\tAverage loss=0.517321, Current loss=1.369289\n",
      "Epoch 2 (26.10s)\tAverage loss=0.516903, Current loss=0.283633\n",
      "Epoch 2 (26.17s)\tAverage loss=0.516107, Current loss=0.071120\n",
      "Epoch 2 (26.19s)\tAverage loss=0.515696, Current loss=0.285611\n",
      "Epoch 2 (26.21s)\tAverage loss=0.515262, Current loss=0.271590\n",
      "Epoch 2 (26.29s)\tAverage loss=0.514666, Current loss=0.180169\n",
      "Epoch 2 (26.36s)\tAverage loss=0.513856, Current loss=0.057564\n",
      "Epoch 2 (26.44s)\tAverage loss=0.513049, Current loss=0.057973\n",
      "Epoch 2 (26.49s)\tAverage loss=0.512474, Current loss=0.187415\n",
      "Epoch 2 (26.52s)\tAverage loss=0.511955, Current loss=0.218232\n",
      "Epoch 2 (26.56s)\tAverage loss=0.511411, Current loss=0.203163\n",
      "Epoch 2 (26.60s)\tAverage loss=0.510846, Current loss=0.189571\n",
      "Epoch 2 (26.62s)\tAverage loss=0.510253, Current loss=0.172964\n",
      "Epoch 2 (26.65s)\tAverage loss=0.509629, Current loss=0.154226\n",
      "Epoch 2 (26.67s)\tAverage loss=0.508917, Current loss=0.102319\n",
      "Epoch 2 (26.71s)\tAverage loss=0.508180, Current loss=0.086312\n",
      "Epoch 2 (26.73s)\tAverage loss=0.507477, Current loss=0.104578\n",
      "Epoch 2 (26.74s)\tAverage loss=0.510869, Current loss=2.458224\n",
      "Epoch 2 (26.78s)\tAverage loss=0.510139, Current loss=0.090350\n",
      "Epoch 2 (26.82s)\tAverage loss=0.513445, Current loss=2.417564\n",
      "Epoch 2 (26.85s)\tAverage loss=0.512747, Current loss=0.110277\n",
      "Epoch 2 (26.87s)\tAverage loss=0.512060, Current loss=0.114703\n",
      "Epoch 2 (27.11s)\tAverage loss=0.511238, Current loss=0.035307\n",
      "Epoch 2 (27.18s)\tAverage loss=0.510404, Current loss=0.026560\n",
      "Epoch 2 (27.23s)\tAverage loss=0.509664, Current loss=0.080059\n",
      "Epoch 2 (27.26s)\tAverage loss=0.509035, Current loss=0.142905\n",
      "Epoch 2 (27.29s)\tAverage loss=0.508403, Current loss=0.139721\n",
      "Epoch 2 (27.32s)\tAverage loss=0.507782, Current loss=0.145104\n",
      "Epoch 2 (27.37s)\tAverage loss=0.511306, Current loss=2.573034\n",
      "Epoch 2 (27.41s)\tAverage loss=0.510691, Current loss=0.150131\n",
      "Epoch 2 (27.43s)\tAverage loss=0.513108, Current loss=1.932095\n",
      "Epoch 2 (27.49s)\tAverage loss=0.512359, Current loss=0.071724\n",
      "Epoch 2 (27.53s)\tAverage loss=0.511788, Current loss=0.175512\n",
      "Epoch 2 (27.56s)\tAverage loss=0.511232, Current loss=0.183124\n",
      "Epoch 2 (27.59s)\tAverage loss=0.510682, Current loss=0.185534\n",
      "Epoch 2 (27.64s)\tAverage loss=0.510128, Current loss=0.182434\n",
      "Epoch 2 (27.66s)\tAverage loss=0.509503, Current loss=0.138829\n",
      "Epoch 2 (27.69s)\tAverage loss=0.508883, Current loss=0.140479\n",
      "Epoch 2 (27.71s)\tAverage loss=0.510954, Current loss=1.743401\n",
      "Epoch 2 (27.75s)\tAverage loss=0.510424, Current loss=0.194415\n",
      "Epoch 2 (27.79s)\tAverage loss=0.512424, Current loss=1.706490\n",
      "Epoch 2 (27.84s)\tAverage loss=0.511832, Current loss=0.157640\n",
      "Epoch 2 (27.87s)\tAverage loss=0.511326, Current loss=0.208532\n",
      "Epoch 2 (27.91s)\tAverage loss=0.510833, Current loss=0.214849\n",
      "Epoch 2 (27.99s)\tAverage loss=0.510080, Current loss=0.057930\n",
      "Epoch 2 (28.01s)\tAverage loss=0.509607, Current loss=0.224398\n",
      "Epoch 2 (28.06s)\tAverage loss=0.509039, Current loss=0.166654\n",
      "Epoch 2 (28.13s)\tAverage loss=0.508291, Current loss=0.056482\n",
      "Epoch 2 (28.15s)\tAverage loss=0.507834, Current loss=0.231470\n",
      "Epoch 2 (28.19s)\tAverage loss=0.510148, Current loss=1.912350\n",
      "Epoch 2 (28.23s)\tAverage loss=0.509692, Current loss=0.233067\n",
      "Epoch 2 (28.31s)\tAverage loss=0.508949, Current loss=0.056965\n",
      "Epoch 2 (28.34s)\tAverage loss=0.508500, Current loss=0.235427\n",
      "Epoch 2 (28.41s)\tAverage loss=0.507762, Current loss=0.057338\n",
      "Epoch 2 (28.44s)\tAverage loss=0.507310, Current loss=0.230936\n",
      "Epoch 2 (28.49s)\tAverage loss=0.506850, Current loss=0.225431\n",
      "Epoch 2 (28.51s)\tAverage loss=0.506400, Current loss=0.230433\n",
      "Epoch 2 (28.55s)\tAverage loss=0.508269, Current loss=1.656251\n",
      "Epoch 2 (28.58s)\tAverage loss=0.510023, Current loss=1.588466\n",
      "Epoch 2 (28.65s)\tAverage loss=0.509287, Current loss=0.055660\n",
      "Epoch 2 (28.71s)\tAverage loss=0.508791, Current loss=0.202836\n",
      "Epoch 2 (28.73s)\tAverage loss=0.508363, Current loss=0.244202\n",
      "Epoch 2 (28.78s)\tAverage loss=0.507879, Current loss=0.207871\n",
      "Epoch 2 (28.81s)\tAverage loss=0.507462, Current loss=0.248993\n",
      "Epoch 2 (28.85s)\tAverage loss=0.509159, Current loss=1.563297\n",
      "Epoch 2 (28.93s)\tAverage loss=0.508439, Current loss=0.060281\n",
      "Epoch 2 (29.01s)\tAverage loss=0.507721, Current loss=0.060749\n",
      "Epoch 2 (29.03s)\tAverage loss=0.509260, Current loss=1.469559\n",
      "Epoch 2 (29.25s)\tAverage loss=0.508869, Current loss=0.264306\n",
      "Epoch 2 (29.28s)\tAverage loss=0.508477, Current loss=0.263376\n",
      "Epoch 2 (29.33s)\tAverage loss=0.508092, Current loss=0.266367\n",
      "Epoch 2 (29.36s)\tAverage loss=0.507727, Current loss=0.278774\n",
      "Epoch 2 (29.40s)\tAverage loss=0.507359, Current loss=0.275678\n",
      "Epoch 2 (29.44s)\tAverage loss=0.506985, Current loss=0.271179\n",
      "Epoch 2 (29.46s)\tAverage loss=0.508450, Current loss=1.433322\n",
      "Epoch 2 (29.49s)\tAverage loss=0.509907, Current loss=1.430631\n",
      "Epoch 2 (29.53s)\tAverage loss=0.509446, Current loss=0.217829\n",
      "Epoch 2 (29.58s)\tAverage loss=0.511193, Current loss=1.618702\n",
      "Epoch 2 (29.60s)\tAverage loss=0.510852, Current loss=0.294214\n",
      "Epoch 2 (29.68s)\tAverage loss=0.510155, Current loss=0.067063\n",
      "Epoch 2 (29.71s)\tAverage loss=0.509846, Current loss=0.312800\n",
      "Epoch 2 (29.76s)\tAverage loss=0.511380, Current loss=1.489840\n",
      "Epoch 2 (29.84s)\tAverage loss=0.510689, Current loss=0.069169\n",
      "Epoch 2 (29.87s)\tAverage loss=0.510403, Current loss=0.327732\n",
      "Epoch 2 (29.91s)\tAverage loss=0.510110, Current loss=0.321854\n",
      "Epoch 2 (29.96s)\tAverage loss=0.509796, Current loss=0.308493\n",
      "Epoch 2 (29.98s)\tAverage loss=0.509526, Current loss=0.336181\n",
      "Epoch 2 (30.02s)\tAverage loss=0.509253, Current loss=0.332937\n",
      "Epoch 2 (30.05s)\tAverage loss=0.508973, Current loss=0.328329\n",
      "Epoch 2 (30.09s)\tAverage loss=0.508686, Current loss=0.323418\n",
      "Epoch 2 (30.12s)\tAverage loss=0.508394, Current loss=0.319788\n",
      "Epoch 2 (30.20s)\tAverage loss=0.507715, Current loss=0.067746\n",
      "Epoch 2 (30.25s)\tAverage loss=0.507383, Current loss=0.291525\n",
      "Epoch 2 (30.27s)\tAverage loss=0.507046, Current loss=0.287815\n",
      "Epoch 2 (30.30s)\tAverage loss=0.506700, Current loss=0.281716\n",
      "Epoch 2 (30.36s)\tAverage loss=0.506343, Current loss=0.273880\n",
      "Epoch 2 (30.37s)\tAverage loss=0.505971, Current loss=0.263054\n",
      "Epoch 2 (30.41s)\tAverage loss=0.505575, Current loss=0.246462\n",
      "Epoch 2 (30.49s)\tAverage loss=0.504873, Current loss=0.045060\n",
      "Epoch 2 (30.53s)\tAverage loss=0.504454, Current loss=0.229631\n",
      "Epoch 2 (30.59s)\tAverage loss=0.504081, Current loss=0.259079\n",
      "Epoch 2 (30.62s)\tAverage loss=0.505846, Current loss=1.666793\n",
      "Epoch 2 (30.67s)\tAverage loss=0.505389, Current loss=0.204498\n",
      "Epoch 2 (30.73s)\tAverage loss=0.504986, Current loss=0.239178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 (30.81s)\tAverage loss=0.504276, Current loss=0.034923\n",
      "Epoch 2 (30.90s)\tAverage loss=0.503565, Current loss=0.032730\n",
      "Epoch 2 (30.93s)\tAverage loss=0.503134, Current loss=0.217008\n",
      "Epoch 2 (30.98s)\tAverage loss=0.502657, Current loss=0.186444\n",
      "Epoch 2 (31.02s)\tAverage loss=0.502180, Current loss=0.184930\n",
      "Epoch 2 (31.07s)\tAverage loss=0.504017, Current loss=1.727519\n",
      "Epoch 2 (31.09s)\tAverage loss=0.503532, Current loss=0.179806\n",
      "Epoch 2 (31.16s)\tAverage loss=0.502819, Current loss=0.026885\n",
      "Epoch 2 (31.39s)\tAverage loss=0.502374, Current loss=0.204659\n",
      "Epoch 2 (31.42s)\tAverage loss=0.501883, Current loss=0.172448\n",
      "Epoch 2 (31.46s)\tAverage loss=0.501392, Current loss=0.172330\n",
      "Epoch 2 (31.48s)\tAverage loss=0.500894, Current loss=0.165853\n",
      "Epoch 3 (32.21s)\tAverage loss=1.874850, Current loss=1.874850\n",
      "Epoch 3 (32.23s)\tAverage loss=1.018922, Current loss=0.162995\n",
      "Epoch 3 (32.25s)\tAverage loss=1.306249, Current loss=1.880903\n",
      "Epoch 3 (32.28s)\tAverage loss=1.022142, Current loss=0.169820\n",
      "Epoch 3 (32.30s)\tAverage loss=0.852421, Current loss=0.173537\n",
      "Epoch 3 (32.36s)\tAverage loss=0.714755, Current loss=0.026424\n",
      "Epoch 3 (32.40s)\tAverage loss=0.638067, Current loss=0.177938\n",
      "Epoch 3 (32.43s)\tAverage loss=0.580265, Current loss=0.175652\n",
      "Epoch 3 (32.46s)\tAverage loss=0.716166, Current loss=1.803378\n",
      "Epoch 3 (32.50s)\tAverage loss=0.662376, Current loss=0.178259\n",
      "Epoch 3 (32.51s)\tAverage loss=0.619097, Current loss=0.186316\n",
      "Epoch 3 (32.55s)\tAverage loss=0.583298, Current loss=0.189505\n",
      "Epoch 3 (32.58s)\tAverage loss=0.673427, Current loss=1.754968\n",
      "Epoch 3 (32.61s)\tAverage loss=0.639287, Current loss=0.195478\n",
      "Epoch 3 (32.64s)\tAverage loss=0.612382, Current loss=0.235711\n",
      "Epoch 3 (32.67s)\tAverage loss=0.680624, Current loss=1.704247\n",
      "Epoch 3 (32.70s)\tAverage loss=0.652719, Current loss=0.206246\n",
      "Epoch 3 (32.72s)\tAverage loss=0.628143, Current loss=0.210344\n",
      "Epoch 3 (32.75s)\tAverage loss=0.607836, Current loss=0.242304\n",
      "Epoch 3 (32.77s)\tAverage loss=0.588214, Current loss=0.215409\n",
      "Epoch 3 (32.81s)\tAverage loss=0.570549, Current loss=0.217250\n",
      "Epoch 3 (32.86s)\tAverage loss=0.555340, Current loss=0.235936\n",
      "Epoch 3 (32.88s)\tAverage loss=0.540628, Current loss=0.216982\n",
      "Epoch 3 (32.90s)\tAverage loss=0.586428, Current loss=1.639830\n",
      "Epoch 3 (32.94s)\tAverage loss=0.572017, Current loss=0.226147\n",
      "Epoch 3 (32.99s)\tAverage loss=0.558202, Current loss=0.212813\n",
      "Epoch 3 (33.02s)\tAverage loss=0.545836, Current loss=0.224340\n",
      "Epoch 3 (33.06s)\tAverage loss=0.534180, Current loss=0.219447\n",
      "Epoch 3 (33.11s)\tAverage loss=0.523015, Current loss=0.210398\n",
      "Epoch 3 (33.15s)\tAverage loss=0.512724, Current loss=0.214279\n",
      "Epoch 3 (33.18s)\tAverage loss=0.550384, Current loss=1.680214\n",
      "Epoch 3 (33.21s)\tAverage loss=0.539847, Current loss=0.213184\n",
      "Epoch 3 (33.24s)\tAverage loss=0.573462, Current loss=1.649136\n",
      "Epoch 3 (33.25s)\tAverage loss=0.604002, Current loss=1.611837\n",
      "Epoch 3 (33.33s)\tAverage loss=0.587652, Current loss=0.031736\n",
      "Epoch 3 (33.57s)\tAverage loss=0.577748, Current loss=0.231120\n",
      "Epoch 3 (33.60s)\tAverage loss=0.604243, Current loss=1.558067\n",
      "Epoch 3 (33.63s)\tAverage loss=0.594005, Current loss=0.215189\n",
      "Epoch 3 (33.66s)\tAverage loss=0.585226, Current loss=0.251642\n",
      "Epoch 3 (33.74s)\tAverage loss=0.571579, Current loss=0.039320\n",
      "Epoch 3 (33.77s)\tAverage loss=0.564049, Current loss=0.262855\n",
      "Epoch 3 (33.82s)\tAverage loss=0.555916, Current loss=0.222483\n",
      "Epoch 3 (33.86s)\tAverage loss=0.576909, Current loss=1.458585\n",
      "Epoch 3 (33.89s)\tAverage loss=0.569976, Current loss=0.271863\n",
      "Epoch 3 (33.92s)\tAverage loss=0.563376, Current loss=0.272989\n",
      "Epoch 3 (33.98s)\tAverage loss=0.555932, Current loss=0.220952\n",
      "Epoch 3 (34.04s)\tAverage loss=0.550082, Current loss=0.280952\n",
      "Epoch 3 (34.11s)\tAverage loss=0.539424, Current loss=0.038502\n",
      "Epoch 3 (34.14s)\tAverage loss=0.534062, Current loss=0.276693\n",
      "Epoch 3 (34.20s)\tAverage loss=0.527518, Current loss=0.206854\n",
      "Epoch 3 (34.22s)\tAverage loss=0.522546, Current loss=0.273948\n",
      "Epoch 3 (34.24s)\tAverage loss=0.517684, Current loss=0.269739\n",
      "Epoch 3 (34.27s)\tAverage loss=0.512936, Current loss=0.266021\n",
      "Epoch 3 (34.31s)\tAverage loss=0.508216, Current loss=0.258083\n",
      "Epoch 3 (34.33s)\tAverage loss=0.503445, Current loss=0.245821\n",
      "Epoch 3 (34.36s)\tAverage loss=0.498961, Current loss=0.252304\n",
      "Epoch 3 (34.40s)\tAverage loss=0.494499, Current loss=0.244651\n",
      "Epoch 3 (34.47s)\tAverage loss=0.513536, Current loss=1.598663\n",
      "Epoch 3 (34.51s)\tAverage loss=0.508757, Current loss=0.231547\n",
      "Epoch 3 (34.55s)\tAverage loss=0.504169, Current loss=0.233490\n",
      "Epoch 3 (34.59s)\tAverage loss=0.522142, Current loss=1.600515\n",
      "Epoch 3 (34.60s)\tAverage loss=0.517330, Current loss=0.223824\n",
      "Epoch 3 (34.63s)\tAverage loss=0.512724, Current loss=0.227140\n",
      "Epoch 3 (34.74s)\tAverage loss=0.505673, Current loss=0.061434\n",
      "Epoch 3 (34.78s)\tAverage loss=0.501365, Current loss=0.225646\n",
      "Epoch 3 (34.82s)\tAverage loss=0.497124, Current loss=0.221512\n",
      "Epoch 3 (34.86s)\tAverage loss=0.492985, Current loss=0.219766\n",
      "Epoch 3 (34.88s)\tAverage loss=0.488955, Current loss=0.218938\n",
      "Epoch 3 (34.92s)\tAverage loss=0.484854, Current loss=0.206034\n",
      "Epoch 3 (34.97s)\tAverage loss=0.480943, Current loss=0.211051\n",
      "Epoch 3 (35.00s)\tAverage loss=0.477036, Current loss=0.203519\n",
      "Epoch 3 (35.04s)\tAverage loss=0.473144, Current loss=0.196820\n",
      "Epoch 3 (35.07s)\tAverage loss=0.469302, Current loss=0.192713\n",
      "Epoch 3 (35.10s)\tAverage loss=0.465412, Current loss=0.181401\n",
      "Epoch 3 (35.15s)\tAverage loss=0.461668, Current loss=0.184659\n",
      "Epoch 3 (35.18s)\tAverage loss=0.457931, Current loss=0.177622\n",
      "Epoch 3 (35.23s)\tAverage loss=0.454177, Current loss=0.168934\n",
      "Epoch 3 (35.25s)\tAverage loss=0.472303, Current loss=1.867990\n",
      "Epoch 3 (35.27s)\tAverage loss=0.468429, Current loss=0.166210\n",
      "Epoch 3 (35.32s)\tAverage loss=0.486376, Current loss=1.904178\n",
      "Epoch 3 (35.36s)\tAverage loss=0.482384, Current loss=0.163026\n",
      "Epoch 3 (35.40s)\tAverage loss=0.499968, Current loss=1.924278\n",
      "Epoch 3 (35.43s)\tAverage loss=0.496006, Current loss=0.171161\n",
      "Epoch 3 (35.45s)\tAverage loss=0.511944, Current loss=1.834794\n",
      "Epoch 3 (35.47s)\tAverage loss=0.508013, Current loss=0.177834\n",
      "Epoch 3 (35.50s)\tAverage loss=0.504265, Current loss=0.185670\n",
      "Epoch 3 (35.72s)\tAverage loss=0.500577, Current loss=0.183389\n",
      "Epoch 3 (35.75s)\tAverage loss=0.497013, Current loss=0.186912\n",
      "Epoch 3 (35.76s)\tAverage loss=0.493608, Current loss=0.194029\n",
      "Epoch 3 (35.78s)\tAverage loss=0.490323, Current loss=0.197923\n",
      "Epoch 3 (35.81s)\tAverage loss=0.487128, Current loss=0.199557\n",
      "Epoch 3 (35.83s)\tAverage loss=0.500583, Current loss=1.725061\n",
      "Epoch 3 (35.85s)\tAverage loss=0.513396, Current loss=1.692139\n",
      "Epoch 3 (35.87s)\tAverage loss=0.530452, Current loss=2.116688\n",
      "Epoch 3 (35.90s)\tAverage loss=0.527090, Current loss=0.211063\n",
      "Epoch 3 (35.91s)\tAverage loss=0.538472, Current loss=1.619709\n",
      "Epoch 3 (35.93s)\tAverage loss=0.535302, Current loss=0.230962\n",
      "Epoch 3 (36.00s)\tAverage loss=0.530141, Current loss=0.029598\n",
      "Epoch 3 (36.05s)\tAverage loss=0.526628, Current loss=0.182340\n",
      "Epoch 3 (36.07s)\tAverage loss=0.536186, Current loss=1.482455\n",
      "Epoch 3 (36.10s)\tAverage loss=0.545342, Current loss=1.460859\n",
      "Epoch 3 (36.15s)\tAverage loss=0.554199, Current loss=1.448767\n",
      "Epoch 3 (36.18s)\tAverage loss=0.551615, Current loss=0.288080\n",
      "Epoch 3 (36.20s)\tAverage loss=0.549201, Current loss=0.300520\n",
      "Epoch 3 (36.22s)\tAverage loss=0.546324, Current loss=0.247150\n",
      "Epoch 3 (36.27s)\tAverage loss=0.544111, Current loss=0.311801\n",
      "Epoch 3 (36.29s)\tAverage loss=0.542044, Current loss=0.322879\n",
      "Epoch 3 (36.35s)\tAverage loss=0.537486, Current loss=0.049819\n",
      "Epoch 3 (36.39s)\tAverage loss=0.535568, Current loss=0.328354\n",
      "Epoch 3 (36.42s)\tAverage loss=0.533677, Current loss=0.327548\n",
      "Epoch 3 (36.47s)\tAverage loss=0.531261, Current loss=0.265580\n",
      "Epoch 3 (36.51s)\tAverage loss=0.529445, Current loss=0.327848\n",
      "Epoch 3 (36.53s)\tAverage loss=0.535893, Current loss=1.258071\n",
      "Epoch 3 (36.55s)\tAverage loss=0.534080, Current loss=0.329166\n",
      "Epoch 3 (36.57s)\tAverage loss=0.532334, Current loss=0.333318\n",
      "Epoch 3 (36.58s)\tAverage loss=0.538535, Current loss=1.251613\n",
      "Epoch 3 (36.62s)\tAverage loss=0.544714, Current loss=1.261564\n",
      "Epoch 3 (36.67s)\tAverage loss=0.542917, Current loss=0.332655\n",
      "Epoch 3 (36.69s)\tAverage loss=0.541269, Current loss=0.346844\n",
      "Epoch 3 (36.71s)\tAverage loss=0.547032, Current loss=1.232774\n",
      "Epoch 3 (36.79s)\tAverage loss=0.543462, Current loss=0.115037\n",
      "Epoch 3 (36.80s)\tAverage loss=0.541898, Current loss=0.352725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (36.88s)\tAverage loss=0.537933, Current loss=0.054115\n",
      "Epoch 3 (36.95s)\tAverage loss=0.534025, Current loss=0.053353\n",
      "Epoch 3 (36.99s)\tAverage loss=0.531912, Current loss=0.269880\n",
      "Epoch 3 (37.03s)\tAverage loss=0.530534, Current loss=0.358358\n",
      "Epoch 3 (37.05s)\tAverage loss=0.529156, Current loss=0.355527\n",
      "Epoch 3 (37.09s)\tAverage loss=0.527777, Current loss=0.352661\n",
      "Epoch 3 (37.12s)\tAverage loss=0.526341, Current loss=0.342464\n",
      "Epoch 3 (37.20s)\tAverage loss=0.522626, Current loss=0.043393\n",
      "Epoch 3 (37.24s)\tAverage loss=0.521175, Current loss=0.332548\n",
      "Epoch 3 (37.27s)\tAverage loss=0.526767, Current loss=1.259372\n",
      "Epoch 3 (37.31s)\tAverage loss=0.525336, Current loss=0.336480\n",
      "Epoch 3 (37.36s)\tAverage loss=0.523150, Current loss=0.232410\n",
      "Epoch 3 (37.43s)\tAverage loss=0.519519, Current loss=0.032955\n",
      "Epoch 3 (37.45s)\tAverage loss=0.525148, Current loss=1.284967\n",
      "Epoch 3 (37.47s)\tAverage loss=0.523724, Current loss=0.330180\n",
      "Epoch 3 (37.50s)\tAverage loss=0.529180, Current loss=1.276539\n",
      "Epoch 3 (37.54s)\tAverage loss=0.527656, Current loss=0.317389\n",
      "Epoch 3 (37.57s)\tAverage loss=0.526218, Current loss=0.326393\n",
      "Epoch 3 (37.62s)\tAverage loss=0.524004, Current loss=0.213950\n",
      "Epoch 3 (37.65s)\tAverage loss=0.522610, Current loss=0.326048\n",
      "Epoch 3 (37.69s)\tAverage loss=0.521148, Current loss=0.313633\n",
      "Epoch 3 (37.91s)\tAverage loss=0.526436, Current loss=1.282612\n",
      "Epoch 3 (37.92s)\tAverage loss=0.531891, Current loss=1.317416\n",
      "Epoch 3 (37.95s)\tAverage loss=0.536864, Current loss=1.257905\n",
      "Epoch 3 (37.98s)\tAverage loss=0.535397, Current loss=0.321164\n",
      "Epoch 3 (38.01s)\tAverage loss=0.533983, Current loss=0.326169\n",
      "Epoch 3 (38.06s)\tAverage loss=0.532595, Current loss=0.327174\n",
      "Epoch 3 (38.13s)\tAverage loss=0.529224, Current loss=0.026928\n",
      "Epoch 3 (38.17s)\tAverage loss=0.527966, Current loss=0.339343\n",
      "Epoch 3 (38.18s)\tAverage loss=0.526658, Current loss=0.329098\n",
      "Epoch 3 (38.21s)\tAverage loss=0.525339, Current loss=0.324769\n",
      "Epoch 3 (38.24s)\tAverage loss=0.524130, Current loss=0.339270\n",
      "Epoch 3 (38.28s)\tAverage loss=0.522871, Current loss=0.328982\n",
      "Epoch 3 (38.30s)\tAverage loss=0.521662, Current loss=0.334240\n",
      "Epoch 3 (38.38s)\tAverage loss=0.518729, Current loss=0.061162\n",
      "Epoch 3 (38.41s)\tAverage loss=0.523644, Current loss=1.295321\n",
      "Epoch 3 (38.44s)\tAverage loss=0.528655, Current loss=1.320436\n",
      "Epoch 3 (38.47s)\tAverage loss=0.527298, Current loss=0.311536\n",
      "Epoch 3 (38.50s)\tAverage loss=0.525969, Current loss=0.313333\n",
      "Epoch 3 (38.53s)\tAverage loss=0.524570, Current loss=0.299310\n",
      "Epoch 3 (38.60s)\tAverage loss=0.521491, Current loss=0.022664\n",
      "Epoch 3 (38.68s)\tAverage loss=0.518435, Current loss=0.020330\n",
      "Epoch 3 (38.72s)\tAverage loss=0.516319, Current loss=0.169203\n",
      "Epoch 3 (38.74s)\tAverage loss=0.515101, Current loss=0.314172\n",
      "Epoch 3 (38.78s)\tAverage loss=0.520368, Current loss=1.394718\n",
      "Epoch 3 (38.83s)\tAverage loss=0.519095, Current loss=0.306486\n",
      "Epoch 3 (38.90s)\tAverage loss=0.516295, Current loss=0.045855\n",
      "Epoch 3 (38.98s)\tAverage loss=0.514069, Current loss=0.137966\n",
      "Epoch 3 (39.05s)\tAverage loss=0.511171, Current loss=0.018451\n",
      "Epoch 3 (39.07s)\tAverage loss=0.516033, Current loss=1.347495\n",
      "Epoch 3 (39.11s)\tAverage loss=0.514746, Current loss=0.293317\n",
      "Epoch 3 (39.18s)\tAverage loss=0.511887, Current loss=0.017325\n",
      "Epoch 3 (39.21s)\tAverage loss=0.510703, Current loss=0.304604\n",
      "Epoch 3 (39.24s)\tAverage loss=0.515506, Current loss=1.356137\n",
      "Epoch 3 (39.27s)\tAverage loss=0.514301, Current loss=0.302085\n",
      "Epoch 3 (39.30s)\tAverage loss=0.519037, Current loss=1.357341\n",
      "Epoch 3 (39.33s)\tAverage loss=0.517795, Current loss=0.296696\n",
      "Epoch 3 (39.35s)\tAverage loss=0.516598, Current loss=0.302431\n",
      "Epoch 3 (39.36s)\tAverage loss=0.515497, Current loss=0.317318\n",
      "Epoch 3 (39.40s)\tAverage loss=0.519994, Current loss=1.333845\n",
      "Epoch 3 (39.42s)\tAverage loss=0.524452, Current loss=1.335936\n",
      "Epoch 3 (39.45s)\tAverage loss=0.523330, Current loss=0.318003\n",
      "Epoch 3 (39.53s)\tAverage loss=0.520600, Current loss=0.018245\n",
      "Epoch 3 (39.55s)\tAverage loss=0.518689, Current loss=0.165100\n",
      "Epoch 3 (39.57s)\tAverage loss=0.522588, Current loss=1.247802\n",
      "Epoch 3 (39.60s)\tAverage loss=0.521640, Current loss=0.344482\n",
      "Epoch 3 (39.64s)\tAverage loss=0.520800, Current loss=0.362866\n",
      "Epoch 3 (39.71s)\tAverage loss=0.518151, Current loss=0.017424\n",
      "Epoch 3 (39.74s)\tAverage loss=0.521635, Current loss=1.183617\n",
      "Epoch 3 (39.77s)\tAverage loss=0.520825, Current loss=0.366078\n",
      "Epoch 3 (39.82s)\tAverage loss=0.518950, Current loss=0.158882\n",
      "Epoch 3 (39.84s)\tAverage loss=0.522275, Current loss=1.164120\n",
      "Epoch 3 (40.08s)\tAverage loss=0.520395, Current loss=0.155693\n",
      "Epoch 3 (40.09s)\tAverage loss=0.527492, Current loss=1.911386\n",
      "Epoch 3 (40.12s)\tAverage loss=0.526767, Current loss=0.384625\n",
      "Epoch 3 (40.17s)\tAverage loss=0.524994, Current loss=0.175673\n",
      "Epoch 3 (40.20s)\tAverage loss=0.524289, Current loss=0.384775\n",
      "Epoch 3 (40.23s)\tAverage loss=0.523607, Current loss=0.387867\n",
      "Epoch 3 (40.26s)\tAverage loss=0.522930, Current loss=0.387550\n",
      "Epoch 3 (40.28s)\tAverage loss=0.525808, Current loss=1.104210\n",
      "Epoch 3 (40.33s)\tAverage loss=0.531651, Current loss=1.711962\n",
      "Epoch 3 (40.35s)\tAverage loss=0.536539, Current loss=1.528811\n",
      "Epoch 3 (40.39s)\tAverage loss=0.536016, Current loss=0.429329\n",
      "Epoch 3 (40.41s)\tAverage loss=0.534769, Current loss=0.279085\n",
      "Epoch 3 (40.45s)\tAverage loss=0.534274, Current loss=0.432294\n",
      "Epoch 3 (40.48s)\tAverage loss=0.536529, Current loss=1.003486\n",
      "Epoch 3 (40.51s)\tAverage loss=0.536074, Current loss=0.441368\n",
      "Epoch 3 (40.55s)\tAverage loss=0.535587, Current loss=0.433726\n",
      "Epoch 3 (40.59s)\tAverage loss=0.535159, Current loss=0.445345\n",
      "Epoch 3 (40.61s)\tAverage loss=0.534726, Current loss=0.443357\n",
      "Epoch 3 (40.64s)\tAverage loss=0.534293, Current loss=0.442461\n",
      "Epoch 3 (40.66s)\tAverage loss=0.533844, Current loss=0.438153\n",
      "Epoch 3 (40.69s)\tAverage loss=0.536212, Current loss=1.043035\n",
      "Epoch 3 (40.73s)\tAverage loss=0.535672, Current loss=0.419622\n",
      "Epoch 3 (40.77s)\tAverage loss=0.535231, Current loss=0.439945\n",
      "Epoch 3 (40.79s)\tAverage loss=0.538193, Current loss=1.181001\n",
      "Epoch 3 (40.83s)\tAverage loss=0.537550, Current loss=0.397232\n",
      "Epoch 3 (40.87s)\tAverage loss=0.536980, Current loss=0.412197\n",
      "Epoch 3 (40.91s)\tAverage loss=0.536386, Current loss=0.405723\n",
      "Epoch 3 (40.95s)\tAverage loss=0.535826, Current loss=0.412182\n",
      "Epoch 3 (41.03s)\tAverage loss=0.533645, Current loss=0.049288\n",
      "Epoch 3 (41.08s)\tAverage loss=0.532987, Current loss=0.386411\n",
      "Epoch 3 (41.11s)\tAverage loss=0.532448, Current loss=0.411543\n",
      "Epoch 3 (41.18s)\tAverage loss=0.531160, Current loss=0.241385\n",
      "Epoch 3 (41.20s)\tAverage loss=0.530484, Current loss=0.377856\n",
      "Epoch 3 (41.22s)\tAverage loss=0.529765, Current loss=0.366409\n",
      "Epoch 3 (41.27s)\tAverage loss=0.529008, Current loss=0.356477\n",
      "Epoch 3 (41.34s)\tAverage loss=0.526866, Current loss=0.036403\n",
      "Epoch 3 (41.37s)\tAverage loss=0.529712, Current loss=1.184173\n",
      "Epoch 3 (41.39s)\tAverage loss=0.528852, Current loss=0.330334\n",
      "Epoch 3 (41.41s)\tAverage loss=0.528025, Current loss=0.335964\n",
      "Epoch 3 (41.48s)\tAverage loss=0.525897, Current loss=0.030160\n",
      "Epoch 3 (41.52s)\tAverage loss=0.525000, Current loss=0.315071\n",
      "Epoch 3 (41.57s)\tAverage loss=0.524072, Current loss=0.306091\n",
      "Epoch 3 (41.61s)\tAverage loss=0.523175, Current loss=0.311475\n",
      "Epoch 3 (41.64s)\tAverage loss=0.522245, Current loss=0.301690\n",
      "Epoch 3 (41.69s)\tAverage loss=0.520413, Current loss=0.084571\n",
      "Epoch 3 (41.72s)\tAverage loss=0.524027, Current loss=1.387650\n",
      "Epoch 3 (41.77s)\tAverage loss=0.522972, Current loss=0.269809\n",
      "Epoch 3 (41.80s)\tAverage loss=0.521972, Current loss=0.281073\n",
      "Epoch 3 (41.84s)\tAverage loss=0.525828, Current loss=1.458960\n",
      "Epoch 3 (41.89s)\tAverage loss=0.524743, Current loss=0.260938\n",
      "Epoch 3 (41.92s)\tAverage loss=0.523728, Current loss=0.276191\n",
      "Epoch 3 (41.94s)\tAverage loss=0.522638, Current loss=0.255624\n",
      "Epoch 3 (41.97s)\tAverage loss=0.521610, Current loss=0.268742\n",
      "Epoch 3 (41.99s)\tAverage loss=0.520578, Current loss=0.265612\n",
      "Epoch 3 (42.03s)\tAverage loss=0.519492, Current loss=0.250185\n",
      "Epoch 3 (42.24s)\tAverage loss=0.518400, Current loss=0.246586\n",
      "Epoch 3 (42.29s)\tAverage loss=0.517296, Current loss=0.241219\n",
      "Epoch 3 (42.31s)\tAverage loss=0.516215, Current loss=0.244815\n",
      "Epoch 3 (42.39s)\tAverage loss=0.514295, Current loss=0.030479\n",
      "Epoch 3 (42.42s)\tAverage loss=0.513148, Current loss=0.222886\n",
      "Epoch 3 (42.49s)\tAverage loss=0.511184, Current loss=0.012326\n",
      "Epoch 3 (42.52s)\tAverage loss=0.510037, Current loss=0.217645\n",
      "Epoch 3 (42.59s)\tAverage loss=0.508096, Current loss=0.011040\n",
      "Epoch 3 (42.64s)\tAverage loss=0.512562, Current loss=1.660551\n",
      "Epoch 3 (42.66s)\tAverage loss=0.511362, Current loss=0.201683\n",
      "Epoch 3 (42.69s)\tAverage loss=0.510142, Current loss=0.194232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (42.72s)\tAverage loss=0.514587, Current loss=1.670167\n",
      "Epoch 3 (42.74s)\tAverage loss=0.513413, Current loss=0.206921\n",
      "Epoch 3 (42.78s)\tAverage loss=0.512214, Current loss=0.198280\n",
      "Epoch 3 (42.81s)\tAverage loss=0.516407, Current loss=1.619012\n",
      "Epoch 3 (42.85s)\tAverage loss=0.520896, Current loss=1.706007\n",
      "Epoch 3 (42.88s)\tAverage loss=0.524923, Current loss=1.592206\n",
      "Epoch 3 (42.90s)\tAverage loss=0.523814, Current loss=0.228818\n",
      "Epoch 3 (42.93s)\tAverage loss=0.522785, Current loss=0.247859\n",
      "Epoch 3 (42.96s)\tAverage loss=0.521730, Current loss=0.238946\n",
      "Epoch 3 (42.99s)\tAverage loss=0.520695, Current loss=0.242313\n",
      "Epoch 3 (43.01s)\tAverage loss=0.519249, Current loss=0.128898\n",
      "Epoch 3 (43.03s)\tAverage loss=0.518264, Current loss=0.251326\n",
      "Epoch 3 (43.08s)\tAverage loss=0.517249, Current loss=0.241300\n",
      "Epoch 3 (43.11s)\tAverage loss=0.516230, Current loss=0.237937\n",
      "Epoch 3 (43.18s)\tAverage loss=0.514403, Current loss=0.013798\n",
      "Epoch 3 (43.21s)\tAverage loss=0.513427, Current loss=0.245160\n",
      "Epoch 3 (43.24s)\tAverage loss=0.512477, Current loss=0.250293\n",
      "Epoch 3 (43.28s)\tAverage loss=0.511469, Current loss=0.232033\n",
      "Epoch 3 (43.30s)\tAverage loss=0.510459, Current loss=0.229653\n",
      "Epoch 3 (43.35s)\tAverage loss=0.514363, Current loss=1.603560\n",
      "Epoch 3 (43.39s)\tAverage loss=0.513410, Current loss=0.246803\n",
      "Epoch 3 (43.43s)\tAverage loss=0.512411, Current loss=0.231622\n",
      "Epoch 3 (43.46s)\tAverage loss=0.516260, Current loss=1.601666\n",
      "Epoch 3 (43.48s)\tAverage loss=0.519801, Current loss=1.521925\n",
      "Epoch 3 (43.51s)\tAverage loss=0.518836, Current loss=0.244675\n",
      "Epoch 3 (43.54s)\tAverage loss=0.517889, Current loss=0.247902\n",
      "Epoch 3 (43.56s)\tAverage loss=0.516932, Current loss=0.243330\n",
      "Epoch 3 (43.60s)\tAverage loss=0.515967, Current loss=0.238926\n",
      "Epoch 3 (43.62s)\tAverage loss=0.515014, Current loss=0.240759\n",
      "Epoch 3 (43.65s)\tAverage loss=0.514051, Current loss=0.235549\n",
      "Epoch 3 (43.68s)\tAverage loss=0.513099, Current loss=0.237070\n",
      "Epoch 3 (43.70s)\tAverage loss=0.516618, Current loss=1.540639\n",
      "Epoch 3 (43.72s)\tAverage loss=0.515737, Current loss=0.258474\n",
      "Epoch 3 (43.77s)\tAverage loss=0.514122, Current loss=0.041097\n",
      "Epoch 3 (43.82s)\tAverage loss=0.517488, Current loss=1.507100\n",
      "Epoch 3 (43.86s)\tAverage loss=0.516527, Current loss=0.233010\n",
      "Epoch 3 (43.88s)\tAverage loss=0.515629, Current loss=0.249633\n",
      "Epoch 3 (43.93s)\tAverage loss=0.514791, Current loss=0.266195\n",
      "Epoch 3 (43.95s)\tAverage loss=0.518084, Current loss=1.499205\n",
      "Epoch 3 (44.00s)\tAverage loss=0.517175, Current loss=0.245547\n",
      "Epoch 3 (44.02s)\tAverage loss=0.516295, Current loss=0.252182\n",
      "Epoch 3 (44.05s)\tAverage loss=0.515501, Current loss=0.276659\n",
      "Epoch 3 (44.07s)\tAverage loss=0.514642, Current loss=0.254961\n",
      "Epoch 3 (44.12s)\tAverage loss=0.513734, Current loss=0.238614\n",
      "Epoch 3 (44.15s)\tAverage loss=0.516761, Current loss=1.437051\n",
      "Epoch 3 (44.19s)\tAverage loss=0.520167, Current loss=1.558927\n",
      "Epoch 3 (44.40s)\tAverage loss=0.519266, Current loss=0.243729\n",
      "Epoch 3 (44.42s)\tAverage loss=0.518391, Current loss=0.249830\n",
      "Epoch 3 (44.44s)\tAverage loss=0.521472, Current loss=1.470204\n",
      "Epoch 3 (44.46s)\tAverage loss=0.524537, Current loss=1.471741\n",
      "Epoch 3 (44.53s)\tAverage loss=0.522897, Current loss=0.014501\n",
      "Epoch 3 (44.60s)\tAverage loss=0.521263, Current loss=0.012995\n",
      "Epoch 3 (44.63s)\tAverage loss=0.520505, Current loss=0.283903\n",
      "Epoch 3 (44.70s)\tAverage loss=0.518890, Current loss=0.013448\n",
      "Epoch 3 (44.72s)\tAverage loss=0.518133, Current loss=0.280674\n",
      "Epoch 3 (44.76s)\tAverage loss=0.517435, Current loss=0.297412\n",
      "Epoch 3 (44.84s)\tAverage loss=0.515845, Current loss=0.013436\n",
      "Epoch 3 (44.87s)\tAverage loss=0.515114, Current loss=0.283379\n",
      "Epoch 3 (44.90s)\tAverage loss=0.517889, Current loss=1.400365\n",
      "Epoch 3 (44.98s)\tAverage loss=0.516318, Current loss=0.015205\n",
      "Epoch 3 (45.00s)\tAverage loss=0.515639, Current loss=0.298175\n",
      "Epoch 3 (45.03s)\tAverage loss=0.514966, Current loss=0.298939\n",
      "Epoch 3 (45.07s)\tAverage loss=0.514333, Current loss=0.310636\n",
      "Epoch 3 (45.10s)\tAverage loss=0.516998, Current loss=1.377615\n",
      "Epoch 3 (45.12s)\tAverage loss=0.516358, Current loss=0.309151\n",
      "Epoch 3 (45.16s)\tAverage loss=0.515696, Current loss=0.300566\n",
      "Epoch 3 (45.18s)\tAverage loss=0.518288, Current loss=1.363234\n",
      "Epoch 3 (45.23s)\tAverage loss=0.517816, Current loss=0.363488\n",
      "Epoch 3 (45.30s)\tAverage loss=0.516288, Current loss=0.014954\n",
      "Epoch 3 (45.37s)\tAverage loss=0.514768, Current loss=0.014939\n",
      "Epoch 3 (45.41s)\tAverage loss=0.514167, Current loss=0.315746\n",
      "Epoch 3 (45.48s)\tAverage loss=0.512663, Current loss=0.014645\n",
      "Epoch 3 (45.51s)\tAverage loss=0.512069, Current loss=0.315126\n",
      "Epoch 3 (45.55s)\tAverage loss=0.511623, Current loss=0.363081\n",
      "Epoch 3 (45.59s)\tAverage loss=0.511170, Current loss=0.359675\n",
      "Epoch 3 (45.65s)\tAverage loss=0.513263, Current loss=1.214645\n",
      "Epoch 3 (45.69s)\tAverage loss=0.512622, Current loss=0.297242\n",
      "Epoch 3 (45.74s)\tAverage loss=0.512146, Current loss=0.351661\n",
      "Epoch 3 (45.78s)\tAverage loss=0.511664, Current loss=0.348860\n",
      "Epoch 3 (45.82s)\tAverage loss=0.511021, Current loss=0.292921\n",
      "Epoch 3 (45.84s)\tAverage loss=0.510521, Current loss=0.340543\n",
      "Epoch 3 (45.87s)\tAverage loss=0.509888, Current loss=0.294027\n",
      "Epoch 3 (45.94s)\tAverage loss=0.509349, Current loss=0.324849\n",
      "Epoch 3 (45.99s)\tAverage loss=0.508664, Current loss=0.273796\n",
      "Epoch 3 (46.02s)\tAverage loss=0.507995, Current loss=0.277746\n",
      "Epoch 3 (46.04s)\tAverage loss=0.507301, Current loss=0.268046\n",
      "Epoch 3 (46.09s)\tAverage loss=0.506567, Current loss=0.252685\n",
      "Epoch 3 (46.12s)\tAverage loss=0.505815, Current loss=0.244674\n",
      "Epoch 3 (46.15s)\tAverage loss=0.505079, Current loss=0.249180\n",
      "Epoch 3 (46.17s)\tAverage loss=0.504339, Current loss=0.245791\n",
      "Epoch 3 (46.20s)\tAverage loss=0.507282, Current loss=1.537459\n",
      "Epoch 3 (46.22s)\tAverage loss=0.510236, Current loss=1.547172\n",
      "Epoch 3 (46.27s)\tAverage loss=0.509460, Current loss=0.236133\n",
      "Epoch 3 (46.33s)\tAverage loss=0.508674, Current loss=0.231219\n",
      "Epoch 3 (46.56s)\tAverage loss=0.507881, Current loss=0.227272\n",
      "Epoch 3 (46.59s)\tAverage loss=0.507092, Current loss=0.227024\n",
      "Epoch 3 (46.61s)\tAverage loss=0.510110, Current loss=1.584488\n",
      "Epoch 3 (46.64s)\tAverage loss=0.513103, Current loss=1.581518\n",
      "Epoch 3 (46.66s)\tAverage loss=0.512327, Current loss=0.234549\n",
      "Epoch 3 (46.70s)\tAverage loss=0.511530, Current loss=0.225619\n",
      "Epoch 3 (46.78s)\tAverage loss=0.510135, Current loss=0.008002\n",
      "Epoch 3 (46.83s)\tAverage loss=0.509338, Current loss=0.221407\n",
      "Epoch 3 (46.86s)\tAverage loss=0.508614, Current loss=0.246695\n",
      "Epoch 3 (46.97s)\tAverage loss=0.507850, Current loss=0.230237\n",
      "Epoch 3 (47.00s)\tAverage loss=0.507114, Current loss=0.239212\n",
      "Epoch 3 (47.06s)\tAverage loss=0.506345, Current loss=0.225948\n",
      "Epoch 3 (47.12s)\tAverage loss=0.505606, Current loss=0.235058\n",
      "Epoch 3 (47.15s)\tAverage loss=0.504864, Current loss=0.232474\n",
      "Epoch 3 (47.21s)\tAverage loss=0.507672, Current loss=1.541057\n",
      "Epoch 3 (47.29s)\tAverage loss=0.506959, Current loss=0.243711\n",
      "Epoch 3 (47.39s)\tAverage loss=0.506256, Current loss=0.246416\n",
      "Epoch 3 (47.44s)\tAverage loss=0.505463, Current loss=0.211213\n",
      "Epoch 3 (47.50s)\tAverage loss=0.504678, Current loss=0.212505\n",
      "Epoch 3 (47.67s)\tAverage loss=0.503344, Current loss=0.005986\n",
      "Epoch 3 (47.77s)\tAverage loss=0.506376, Current loss=1.640043\n",
      "Epoch 3 (47.86s)\tAverage loss=0.509371, Current loss=1.632549\n",
      "Epoch 3 (47.91s)\tAverage loss=0.512748, Current loss=1.782738\n",
      "Epoch 3 (47.95s)\tAverage loss=0.515683, Current loss=1.622102\n",
      "Epoch 3 (47.98s)\tAverage loss=0.514950, Current loss=0.237947\n",
      "Epoch 3 (48.00s)\tAverage loss=0.514239, Current loss=0.244785\n",
      "Epoch 3 (48.03s)\tAverage loss=0.513515, Current loss=0.238230\n",
      "Epoch 3 (48.07s)\tAverage loss=0.512828, Current loss=0.251250\n",
      "Epoch 3 (48.10s)\tAverage loss=0.512170, Current loss=0.260452\n",
      "Epoch 3 (48.15s)\tAverage loss=0.511571, Current loss=0.282095\n",
      "Epoch 3 (48.19s)\tAverage loss=0.510927, Current loss=0.263977\n",
      "Epoch 3 (48.20s)\tAverage loss=0.510278, Current loss=0.260380\n",
      "Epoch 3 (48.29s)\tAverage loss=0.508988, Current loss=0.011051\n",
      "Epoch 3 (48.38s)\tAverage loss=0.507708, Current loss=0.012175\n",
      "Epoch 3 (48.45s)\tAverage loss=0.507113, Current loss=0.276197\n",
      "Epoch 3 (48.47s)\tAverage loss=0.506507, Current loss=0.270780\n",
      "Epoch 3 (48.50s)\tAverage loss=0.505851, Current loss=0.250117\n",
      "Epoch 3 (48.52s)\tAverage loss=0.505219, Current loss=0.258030\n",
      "Epoch 3 (48.75s)\tAverage loss=0.504027, Current loss=0.036995\n",
      "Epoch 3 (48.80s)\tAverage loss=0.503402, Current loss=0.257696\n",
      "Epoch 3 (48.87s)\tAverage loss=0.502770, Current loss=0.253616\n",
      "Epoch 3 (48.89s)\tAverage loss=0.505419, Current loss=1.551775\n",
      "Epoch 3 (48.94s)\tAverage loss=0.504759, Current loss=0.243291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (48.96s)\tAverage loss=0.504073, Current loss=0.231956\n",
      "Epoch 3 (48.99s)\tAverage loss=0.506689, Current loss=1.547766\n",
      "Epoch 3 (49.04s)\tAverage loss=0.506008, Current loss=0.234533\n",
      "Epoch 3 (49.08s)\tAverage loss=0.505321, Current loss=0.230197\n",
      "Epoch 3 (49.12s)\tAverage loss=0.504658, Current loss=0.238936\n",
      "Epoch 3 (49.21s)\tAverage loss=0.503429, Current loss=0.009230\n",
      "Epoch 3 (49.29s)\tAverage loss=0.502205, Current loss=0.009120\n",
      "Epoch 3 (49.31s)\tAverage loss=0.501517, Current loss=0.223545\n",
      "Epoch 3 (49.36s)\tAverage loss=0.500856, Current loss=0.233102\n",
      "Epoch 3 (49.38s)\tAverage loss=0.503494, Current loss=1.574458\n",
      "Epoch 3 (49.42s)\tAverage loss=0.502845, Current loss=0.238626\n",
      "Epoch 3 (49.45s)\tAverage loss=0.502188, Current loss=0.234207\n",
      "Epoch 3 (49.49s)\tAverage loss=0.501508, Current loss=0.223564\n",
      "Epoch 3 (49.53s)\tAverage loss=0.500824, Current loss=0.220287\n",
      "Epoch 3 (49.56s)\tAverage loss=0.500155, Current loss=0.225154\n",
      "Epoch 3 (49.61s)\tAverage loss=0.499472, Current loss=0.217904\n",
      "Epoch 3 (49.64s)\tAverage loss=0.498795, Current loss=0.219403\n",
      "Epoch 3 (49.70s)\tAverage loss=0.498117, Current loss=0.217403\n",
      "Epoch 3 (49.72s)\tAverage loss=0.497420, Current loss=0.208145\n",
      "Epoch 3 (49.75s)\tAverage loss=0.496732, Current loss=0.210517\n",
      "Epoch 3 (49.79s)\tAverage loss=0.496030, Current loss=0.203241\n",
      "Epoch 3 (49.81s)\tAverage loss=0.495318, Current loss=0.197968\n",
      "Epoch 3 (49.85s)\tAverage loss=0.494595, Current loss=0.191432\n",
      "Epoch 3 (49.89s)\tAverage loss=0.497487, Current loss=1.712213\n",
      "Epoch 3 (49.93s)\tAverage loss=0.496747, Current loss=0.185302\n",
      "Epoch 3 (49.96s)\tAverage loss=0.496034, Current loss=0.195165\n",
      "Epoch 3 (49.99s)\tAverage loss=0.495316, Current loss=0.191644\n",
      "Epoch 3 (50.03s)\tAverage loss=0.494601, Current loss=0.191052\n",
      "Epoch 3 (50.05s)\tAverage loss=0.493875, Current loss=0.185699\n",
      "Epoch 3 (50.08s)\tAverage loss=0.496908, Current loss=1.788838\n",
      "Epoch 3 (50.12s)\tAverage loss=0.496144, Current loss=0.169833\n",
      "Epoch 3 (50.14s)\tAverage loss=0.495390, Current loss=0.172518\n",
      "Epoch 3 (50.18s)\tAverage loss=0.494645, Current loss=0.175214\n",
      "Epoch 3 (50.22s)\tAverage loss=0.493900, Current loss=0.173764\n",
      "Epoch 3 (50.27s)\tAverage loss=0.493134, Current loss=0.162863\n",
      "Epoch 3 (50.29s)\tAverage loss=0.496181, Current loss=1.812426\n",
      "Epoch 3 (50.38s)\tAverage loss=0.495051, Current loss=0.005653\n",
      "Epoch 3 (50.40s)\tAverage loss=0.494298, Current loss=0.167531\n",
      "Epoch 3 (50.47s)\tAverage loss=0.493546, Current loss=0.166375\n",
      "Epoch 3 (50.51s)\tAverage loss=0.492828, Current loss=0.180093\n",
      "Epoch 3 (50.54s)\tAverage loss=0.492079, Current loss=0.164793\n",
      "Epoch 3 (50.59s)\tAverage loss=0.491364, Current loss=0.177790\n",
      "Epoch 3 (50.64s)\tAverage loss=0.490597, Current loss=0.153848\n",
      "Epoch 3 (50.86s)\tAverage loss=0.489521, Current loss=0.016370\n",
      "Epoch 3 (50.90s)\tAverage loss=0.492518, Current loss=1.814337\n",
      "Epoch 3 (50.95s)\tAverage loss=0.491781, Current loss=0.165953\n",
      "Epoch 3 (51.00s)\tAverage loss=0.491010, Current loss=0.149452\n",
      "Epoch 3 (51.04s)\tAverage loss=0.490286, Current loss=0.168614\n",
      "Epoch 3 (51.07s)\tAverage loss=0.493248, Current loss=1.811500\n",
      "Epoch 3 (51.13s)\tAverage loss=0.492521, Current loss=0.168362\n",
      "Epoch 3 (51.18s)\tAverage loss=0.491753, Current loss=0.148132\n",
      "Epoch 3 (51.21s)\tAverage loss=0.491075, Current loss=0.187437\n",
      "Epoch 3 (51.23s)\tAverage loss=0.490336, Current loss=0.158578\n",
      "Epoch 3 (51.29s)\tAverage loss=0.489571, Current loss=0.145128\n",
      "Epoch 3 (51.33s)\tAverage loss=0.492516, Current loss=1.820864\n",
      "Epoch 3 (51.42s)\tAverage loss=0.491439, Current loss=0.004685\n",
      "Epoch 3 (51.44s)\tAverage loss=0.494249, Current loss=1.766985\n",
      "Epoch 3 (51.52s)\tAverage loss=0.493174, Current loss=0.005116\n",
      "Epoch 3 (51.55s)\tAverage loss=0.492461, Current loss=0.168428\n",
      "Epoch 3 (51.58s)\tAverage loss=0.491757, Current loss=0.170500\n",
      "Epoch 3 (51.63s)\tAverage loss=0.494974, Current loss=1.965020\n",
      "Epoch 3 (51.70s)\tAverage loss=0.493907, Current loss=0.005539\n",
      "Epoch 3 (51.73s)\tAverage loss=0.493226, Current loss=0.180426\n",
      "Epoch 3 (51.76s)\tAverage loss=0.495702, Current loss=1.634752\n",
      "Epoch 3 (51.79s)\tAverage loss=0.498259, Current loss=1.677250\n",
      "Epoch 3 (51.82s)\tAverage loss=0.497609, Current loss=0.197006\n",
      "Epoch 3 (51.85s)\tAverage loss=0.497050, Current loss=0.238198\n",
      "Epoch 3 (51.88s)\tAverage loss=0.496469, Current loss=0.227206\n",
      "Epoch 3 (51.91s)\tAverage loss=0.495938, Current loss=0.248677\n",
      "Epoch 3 (51.94s)\tAverage loss=0.495329, Current loss=0.211787\n",
      "Epoch 3 (51.98s)\tAverage loss=0.494752, Current loss=0.225037\n",
      "Epoch 3 (52.03s)\tAverage loss=0.494145, Current loss=0.210343\n",
      "Epoch 3 (52.06s)\tAverage loss=0.493614, Current loss=0.244484\n",
      "Epoch 3 (52.11s)\tAverage loss=0.493025, Current loss=0.216026\n",
      "Epoch 3 (52.14s)\tAverage loss=0.492367, Current loss=0.182621\n",
      "Epoch 3 (52.19s)\tAverage loss=0.491820, Current loss=0.233833\n",
      "Epoch 3 (52.23s)\tAverage loss=0.491108, Current loss=0.154051\n",
      "Epoch 3 (52.32s)\tAverage loss=0.490085, Current loss=0.005409\n",
      "Epoch 3 (52.35s)\tAverage loss=0.489389, Current loss=0.158703\n",
      "Epoch 3 (52.38s)\tAverage loss=0.488653, Current loss=0.138097\n",
      "Epoch 3 (52.41s)\tAverage loss=0.487873, Current loss=0.115923\n",
      "Epoch 3 (52.49s)\tAverage loss=0.486864, Current loss=0.004667\n",
      "Epoch 3 (52.52s)\tAverage loss=0.490390, Current loss=2.179352\n",
      "Epoch 3 (52.56s)\tAverage loss=0.489597, Current loss=0.108943\n",
      "Epoch 3 (52.63s)\tAverage loss=0.488591, Current loss=0.004509\n",
      "Epoch 3 (52.71s)\tAverage loss=0.487588, Current loss=0.004158\n",
      "Epoch 3 (52.74s)\tAverage loss=0.486860, Current loss=0.135218\n",
      "Epoch 3 (52.77s)\tAverage loss=0.486116, Current loss=0.125943\n",
      "Epoch 3 (52.81s)\tAverage loss=0.485364, Current loss=0.120778\n",
      "Epoch 3 (52.85s)\tAverage loss=0.484611, Current loss=0.118804\n",
      "Epoch 3 (53.06s)\tAverage loss=0.483879, Current loss=0.127124\n",
      "Epoch 3 (53.11s)\tAverage loss=0.483093, Current loss=0.099475\n",
      "Epoch 3 (53.19s)\tAverage loss=0.482115, Current loss=0.004027\n",
      "Epoch 3 (53.22s)\tAverage loss=0.486008, Current loss=2.393522\n",
      "Epoch 3 (53.24s)\tAverage loss=0.485273, Current loss=0.124661\n",
      "Epoch 3 (53.29s)\tAverage loss=0.484488, Current loss=0.098158\n",
      "Epoch 3 (53.32s)\tAverage loss=0.488183, Current loss=2.309733\n",
      "Epoch 3 (53.37s)\tAverage loss=0.487423, Current loss=0.112239\n",
      "Epoch 3 (53.45s)\tAverage loss=0.486450, Current loss=0.004388\n",
      "Epoch 3 (53.49s)\tAverage loss=0.485703, Current loss=0.115511\n",
      "Epoch 3 (53.53s)\tAverage loss=0.488873, Current loss=2.064145\n",
      "Epoch 3 (53.58s)\tAverage loss=0.488176, Current loss=0.141263\n",
      "Epoch 3 (53.63s)\tAverage loss=0.487505, Current loss=0.152574\n",
      "Epoch 3 (53.67s)\tAverage loss=0.486830, Current loss=0.149192\n",
      "Epoch 3 (53.70s)\tAverage loss=0.489561, Current loss=1.857711\n",
      "Epoch 3 (53.73s)\tAverage loss=0.488935, Current loss=0.174930\n",
      "Epoch 3 (53.81s)\tAverage loss=0.487979, Current loss=0.007012\n",
      "Epoch 3 (53.83s)\tAverage loss=0.487328, Current loss=0.159052\n",
      "Epoch 3 (53.88s)\tAverage loss=0.486730, Current loss=0.184820\n",
      "Epoch 3 (53.92s)\tAverage loss=0.486141, Current loss=0.188006\n",
      "Epoch 3 (53.96s)\tAverage loss=0.485530, Current loss=0.176183\n",
      "Epoch 3 (53.98s)\tAverage loss=0.487927, Current loss=1.705364\n",
      "Epoch 3 (54.00s)\tAverage loss=0.487369, Current loss=0.203360\n",
      "Epoch 3 (54.03s)\tAverage loss=0.486805, Current loss=0.199067\n",
      "Epoch 3 (54.05s)\tAverage loss=0.489167, Current loss=1.696135\n",
      "Epoch 3 (54.07s)\tAverage loss=0.488611, Current loss=0.204399\n",
      "Epoch 3 (54.10s)\tAverage loss=0.488076, Current loss=0.213174\n",
      "Epoch 3 (54.19s)\tAverage loss=0.487148, Current loss=0.010341\n",
      "Epoch 3 (54.21s)\tAverage loss=0.486593, Current loss=0.200944\n",
      "Epoch 3 (54.24s)\tAverage loss=0.488722, Current loss=1.586853\n",
      "Epoch 3 (54.28s)\tAverage loss=0.488216, Current loss=0.226923\n",
      "Epoch 3 (54.31s)\tAverage loss=0.487734, Current loss=0.238027\n",
      "Epoch 3 (54.33s)\tAverage loss=0.487232, Current loss=0.226896\n",
      "Epoch 3 (54.36s)\tAverage loss=0.486759, Current loss=0.240701\n",
      "Epoch 3 (54.39s)\tAverage loss=0.486288, Current loss=0.240571\n",
      "Epoch 3 (54.43s)\tAverage loss=0.485804, Current loss=0.233141\n",
      "Epoch 3 (54.47s)\tAverage loss=0.485303, Current loss=0.223390\n",
      "Epoch 3 (54.50s)\tAverage loss=0.484827, Current loss=0.235237\n",
      "Epoch 3 (54.58s)\tAverage loss=0.483921, Current loss=0.008732\n",
      "Epoch 3 (54.62s)\tAverage loss=0.483411, Current loss=0.214864\n",
      "Epoch 3 (54.66s)\tAverage loss=0.482912, Current loss=0.220212\n",
      "Epoch 3 (54.68s)\tAverage loss=0.482335, Current loss=0.177502\n",
      "Epoch 3 (54.76s)\tAverage loss=0.481460, Current loss=0.018808\n",
      "Epoch 3 (54.81s)\tAverage loss=0.480893, Current loss=0.180059\n",
      "Epoch 3 (54.86s)\tAverage loss=0.480294, Current loss=0.162248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (54.94s)\tAverage loss=0.479405, Current loss=0.006656\n",
      "Epoch 3 (54.98s)\tAverage loss=0.478805, Current loss=0.158648\n",
      "Epoch 3 (55.22s)\tAverage loss=0.478185, Current loss=0.147263\n",
      "Epoch 3 (55.25s)\tAverage loss=0.477620, Current loss=0.175534\n",
      "Epoch 3 (55.29s)\tAverage loss=0.479999, Current loss=1.755067\n",
      "Epoch 3 (55.33s)\tAverage loss=0.482567, Current loss=1.861445\n",
      "Epoch 3 (55.35s)\tAverage loss=0.485014, Current loss=1.801681\n",
      "Epoch 3 (55.39s)\tAverage loss=0.484457, Current loss=0.184358\n",
      "Epoch 3 (55.44s)\tAverage loss=0.483828, Current loss=0.143998\n",
      "Epoch 3 (55.49s)\tAverage loss=0.486638, Current loss=2.006935\n",
      "Epoch 3 (55.54s)\tAverage loss=0.486093, Current loss=0.190686\n",
      "Epoch 3 (55.57s)\tAverage loss=0.485503, Current loss=0.164780\n",
      "Epoch 3 (55.65s)\tAverage loss=0.484624, Current loss=0.006870\n",
      "Epoch 3 (55.67s)\tAverage loss=0.484158, Current loss=0.230048\n",
      "Epoch 3 (55.71s)\tAverage loss=0.483628, Current loss=0.194494\n",
      "Epoch 3 (55.76s)\tAverage loss=0.483100, Current loss=0.193787\n",
      "Epoch 3 (55.78s)\tAverage loss=0.482633, Current loss=0.226951\n",
      "Epoch 3 (55.82s)\tAverage loss=0.484556, Current loss=1.540247\n",
      "Epoch 3 (55.84s)\tAverage loss=0.484086, Current loss=0.225839\n",
      "Epoch 3 (55.89s)\tAverage loss=0.483541, Current loss=0.182880\n",
      "Epoch 3 (55.92s)\tAverage loss=0.483088, Current loss=0.233333\n",
      "Epoch 3 (55.99s)\tAverage loss=0.482342, Current loss=0.069662\n",
      "Epoch 3 (56.04s)\tAverage loss=0.484586, Current loss=1.727601\n",
      "Epoch 3 (56.09s)\tAverage loss=0.484048, Current loss=0.185627\n",
      "Epoch 3 (56.11s)\tAverage loss=0.483620, Current loss=0.245460\n",
      "Epoch 3 (56.13s)\tAverage loss=0.483173, Current loss=0.234484\n",
      "Epoch 3 (56.16s)\tAverage loss=0.485043, Current loss=1.528292\n",
      "Epoch 3 (56.24s)\tAverage loss=0.484188, Current loss=0.006576\n",
      "Epoch 3 (56.32s)\tAverage loss=0.483441, Current loss=0.064648\n",
      "Epoch 3 (56.35s)\tAverage loss=0.485305, Current loss=1.531281\n",
      "Epoch 3 (56.40s)\tAverage loss=0.484796, Current loss=0.198669\n",
      "Epoch 3 (56.45s)\tAverage loss=0.484413, Current loss=0.268830\n",
      "Epoch 3 (56.50s)\tAverage loss=0.484038, Current loss=0.272588\n",
      "Epoch 3 (56.55s)\tAverage loss=0.483544, Current loss=0.204404\n",
      "Epoch 3 (56.58s)\tAverage loss=0.483183, Current loss=0.278850\n",
      "Epoch 3 (56.62s)\tAverage loss=0.482797, Current loss=0.263763\n",
      "Epoch 3 (56.65s)\tAverage loss=0.484517, Current loss=1.461638\n",
      "Epoch 3 (56.68s)\tAverage loss=0.484134, Current loss=0.266437\n",
      "Epoch 3 (56.71s)\tAverage loss=0.483760, Current loss=0.270579\n",
      "Epoch 3 (56.74s)\tAverage loss=0.483378, Current loss=0.264814\n",
      "Epoch 3 (56.77s)\tAverage loss=0.482878, Current loss=0.196980\n",
      "Epoch 3 (56.86s)\tAverage loss=0.482070, Current loss=0.019113\n",
      "Epoch 3 (56.91s)\tAverage loss=0.481561, Current loss=0.189245\n",
      "Epoch 3 (56.93s)\tAverage loss=0.483401, Current loss=1.541348\n",
      "Epoch 3 (56.97s)\tAverage loss=0.482973, Current loss=0.236970\n",
      "Epoch 3 (57.02s)\tAverage loss=0.482602, Current loss=0.268095\n",
      "Epoch 3 (57.05s)\tAverage loss=0.484301, Current loss=1.466557\n",
      "Epoch 3 (57.10s)\tAverage loss=0.483781, Current loss=0.182785\n",
      "Epoch 3 (57.13s)\tAverage loss=0.485344, Current loss=1.391573\n",
      "Epoch 3 (57.18s)\tAverage loss=0.484827, Current loss=0.184762\n",
      "Epoch 3 (57.39s)\tAverage loss=0.484466, Current loss=0.274334\n",
      "Epoch 3 (57.42s)\tAverage loss=0.484132, Current loss=0.289453\n",
      "Epoch 3 (57.47s)\tAverage loss=0.483801, Current loss=0.290262\n",
      "Epoch 3 (57.50s)\tAverage loss=0.483408, Current loss=0.253721\n",
      "Epoch 3 (57.52s)\tAverage loss=0.483080, Current loss=0.290979\n",
      "Epoch 3 (57.56s)\tAverage loss=0.482682, Current loss=0.248584\n",
      "Epoch 3 (57.63s)\tAverage loss=0.481892, Current loss=0.017589\n",
      "Epoch 3 (57.67s)\tAverage loss=0.481500, Current loss=0.250495\n",
      "Epoch 3 (57.75s)\tAverage loss=0.480712, Current loss=0.016189\n",
      "Epoch 3 (57.78s)\tAverage loss=0.482432, Current loss=1.498680\n",
      "Epoch 3 (57.84s)\tAverage loss=0.482072, Current loss=0.269055\n",
      "Epoch 3 (57.91s)\tAverage loss=0.481275, Current loss=0.008294\n",
      "Epoch 3 (57.94s)\tAverage loss=0.480894, Current loss=0.254889\n",
      "Epoch 3 (58.03s)\tAverage loss=0.488856, Current loss=5.225910\n",
      "Epoch 3 (58.07s)\tAverage loss=0.488514, Current loss=0.284984\n",
      "Epoch 3 (58.11s)\tAverage loss=0.488145, Current loss=0.267961\n",
      "Epoch 3 (58.15s)\tAverage loss=0.487743, Current loss=0.247055\n",
      "Epoch 3 (58.17s)\tAverage loss=0.489950, Current loss=1.812112\n",
      "Epoch 3 (58.22s)\tAverage loss=0.489428, Current loss=0.176012\n",
      "Epoch 3 (58.24s)\tAverage loss=0.489080, Current loss=0.280079\n",
      "Epoch 3 (58.27s)\tAverage loss=0.488706, Current loss=0.263711\n",
      "Epoch 3 (58.31s)\tAverage loss=0.488359, Current loss=0.278942\n",
      "Epoch 3 (58.35s)\tAverage loss=0.487994, Current loss=0.267919\n",
      "Epoch 3 (58.37s)\tAverage loss=0.489628, Current loss=1.477594\n",
      "Epoch 3 (58.41s)\tAverage loss=0.489277, Current loss=0.276812\n",
      "Epoch 3 (58.45s)\tAverage loss=0.488911, Current loss=0.266519\n",
      "Epoch 3 (58.48s)\tAverage loss=0.488524, Current loss=0.253311\n",
      "Epoch 3 (58.52s)\tAverage loss=0.488154, Current loss=0.262951\n",
      "Epoch 3 (58.54s)\tAverage loss=0.489711, Current loss=1.439656\n",
      "Epoch 3 (58.62s)\tAverage loss=0.488978, Current loss=0.041009\n",
      "Epoch 3 (58.66s)\tAverage loss=0.488590, Current loss=0.250879\n",
      "Epoch 3 (58.74s)\tAverage loss=0.487867, Current loss=0.044884\n",
      "Epoch 3 (58.76s)\tAverage loss=0.487332, Current loss=0.158772\n",
      "Epoch 3 (58.84s)\tAverage loss=0.486614, Current loss=0.045158\n",
      "Epoch 3 (58.87s)\tAverage loss=0.488513, Current loss=1.658056\n",
      "Epoch 3 (58.91s)\tAverage loss=0.488173, Current loss=0.278773\n",
      "Epoch 3 (58.95s)\tAverage loss=0.487730, Current loss=0.213456\n",
      "Epoch 3 (58.98s)\tAverage loss=0.487341, Current loss=0.247084\n",
      "Epoch 3 (59.00s)\tAverage loss=0.486980, Current loss=0.263038\n",
      "Epoch 3 (59.05s)\tAverage loss=0.486544, Current loss=0.215663\n",
      "Epoch 3 (59.10s)\tAverage loss=0.486110, Current loss=0.216112\n",
      "Epoch 3 (59.17s)\tAverage loss=0.485403, Current loss=0.044861\n",
      "Epoch 3 (59.21s)\tAverage loss=0.485020, Current loss=0.245887\n",
      "Epoch 3 (59.23s)\tAverage loss=0.484629, Current loss=0.240531\n",
      "Epoch 3 (59.30s)\tAverage loss=0.483921, Current loss=0.040747\n",
      "Epoch 3 (59.56s)\tAverage loss=0.483214, Current loss=0.039878\n",
      "Epoch 3 (59.59s)\tAverage loss=0.482801, Current loss=0.223249\n",
      "Epoch 3 (59.63s)\tAverage loss=0.482333, Current loss=0.187906\n",
      "Epoch 3 (59.66s)\tAverage loss=0.481910, Current loss=0.215674\n",
      "Epoch 3 (59.69s)\tAverage loss=0.481443, Current loss=0.186807\n",
      "Epoch 3 (59.76s)\tAverage loss=0.486135, Current loss=3.451659\n",
      "Epoch 3 (59.79s)\tAverage loss=0.485698, Current loss=0.209028\n",
      "Epoch 3 (59.87s)\tAverage loss=0.485006, Current loss=0.046183\n",
      "Epoch 3 (59.91s)\tAverage loss=0.487084, Current loss=1.806630\n",
      "Epoch 3 (59.97s)\tAverage loss=0.489154, Current loss=1.805360\n",
      "Epoch 3 (59.99s)\tAverage loss=0.488728, Current loss=0.217637\n",
      "Epoch 3 (60.03s)\tAverage loss=0.490443, Current loss=1.584791\n",
      "Epoch 3 (60.07s)\tAverage loss=0.490032, Current loss=0.227546\n",
      "Epoch 3 (60.10s)\tAverage loss=0.491626, Current loss=1.511544\n",
      "Epoch 3 (60.13s)\tAverage loss=0.493211, Current loss=1.509242\n",
      "Epoch 3 (60.18s)\tAverage loss=0.492829, Current loss=0.247755\n",
      "Epoch 3 (60.20s)\tAverage loss=0.492488, Current loss=0.273030\n",
      "Epoch 3 (60.25s)\tAverage loss=0.492137, Current loss=0.266275\n",
      "Epoch 3 (60.28s)\tAverage loss=0.491803, Current loss=0.276102\n",
      "Epoch 3 (60.32s)\tAverage loss=0.491486, Current loss=0.286699\n",
      "Epoch 3 (60.34s)\tAverage loss=0.491200, Current loss=0.306520\n",
      "Epoch 3 (60.38s)\tAverage loss=0.492565, Current loss=1.376556\n",
      "Epoch 3 (60.41s)\tAverage loss=0.492260, Current loss=0.294254\n",
      "Epoch 3 (60.44s)\tAverage loss=0.491960, Current loss=0.297245\n",
      "Epoch 3 (60.49s)\tAverage loss=0.491652, Current loss=0.290931\n",
      "Epoch 3 (60.51s)\tAverage loss=0.491355, Current loss=0.297659\n",
      "Epoch 3 (60.54s)\tAverage loss=0.491058, Current loss=0.297128\n",
      "Epoch 3 (60.60s)\tAverage loss=0.490530, Current loss=0.145777\n",
      "Epoch 3 (60.66s)\tAverage loss=0.490227, Current loss=0.291747\n",
      "Epoch 3 (60.71s)\tAverage loss=0.489927, Current loss=0.293096\n",
      "Epoch 3 (60.75s)\tAverage loss=0.489634, Current loss=0.297114\n",
      "Epoch 3 (60.83s)\tAverage loss=0.489100, Current loss=0.137637\n",
      "Epoch 3 (60.86s)\tAverage loss=0.490425, Current loss=1.363543\n",
      "Epoch 3 (60.90s)\tAverage loss=0.490081, Current loss=0.262673\n",
      "Epoch 3 (60.94s)\tAverage loss=0.489752, Current loss=0.272338\n",
      "Epoch 3 (60.97s)\tAverage loss=0.489413, Current loss=0.265479\n",
      "Epoch 3 (61.00s)\tAverage loss=0.489070, Current loss=0.261212\n",
      "Epoch 3 (61.02s)\tAverage loss=0.488737, Current loss=0.267670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 (61.09s)\tAverage loss=0.488172, Current loss=0.112747\n",
      "Epoch 3 (61.12s)\tAverage loss=0.487804, Current loss=0.242386\n",
      "Epoch 3 (61.20s)\tAverage loss=0.487225, Current loss=0.101458\n",
      "Epoch 3 (61.27s)\tAverage loss=0.489965, Current loss=2.320127\n",
      "Epoch 3 (61.29s)\tAverage loss=0.489588, Current loss=0.237556\n",
      "Epoch 3 (61.32s)\tAverage loss=0.491179, Current loss=1.557074\n",
      "Epoch 3 (61.35s)\tAverage loss=0.490814, Current loss=0.245717\n",
      "Epoch 3 (61.38s)\tAverage loss=0.492333, Current loss=1.512739\n",
      "Epoch 4 (62.20s)\tAverage loss=0.239929, Current loss=0.239929\n",
      "Epoch 4 (62.23s)\tAverage loss=0.244496, Current loss=0.249064\n",
      "Epoch 4 (62.27s)\tAverage loss=0.242327, Current loss=0.237987\n",
      "Epoch 4 (62.30s)\tAverage loss=0.240899, Current loss=0.236616\n",
      "Epoch 4 (62.32s)\tAverage loss=0.485324, Current loss=1.463025\n",
      "Epoch 4 (62.35s)\tAverage loss=0.445286, Current loss=0.245097\n",
      "Epoch 4 (62.42s)\tAverage loss=0.404522, Current loss=0.159939\n",
      "Epoch 4 (62.44s)\tAverage loss=0.385756, Current loss=0.254392\n",
      "Epoch 4 (62.45s)\tAverage loss=0.371995, Current loss=0.261909\n",
      "Epoch 4 (62.49s)\tAverage loss=0.485484, Current loss=1.506880\n",
      "Epoch 4 (62.52s)\tAverage loss=0.463841, Current loss=0.247409\n",
      "Epoch 4 (62.56s)\tAverage loss=0.445276, Current loss=0.241064\n",
      "Epoch 4 (62.59s)\tAverage loss=0.430416, Current loss=0.252102\n",
      "Epoch 4 (62.61s)\tAverage loss=0.504275, Current loss=1.464433\n",
      "Epoch 4 (62.65s)\tAverage loss=0.486746, Current loss=0.241345\n",
      "Epoch 4 (62.68s)\tAverage loss=0.471731, Current loss=0.246508\n",
      "Epoch 4 (62.74s)\tAverage loss=0.454127, Current loss=0.172463\n",
      "Epoch 4 (62.78s)\tAverage loss=0.442272, Current loss=0.240741\n",
      "Epoch 4 (62.81s)\tAverage loss=0.432027, Current loss=0.247610\n",
      "Epoch 4 (62.83s)\tAverage loss=0.423525, Current loss=0.261985\n",
      "Epoch 4 (62.87s)\tAverage loss=0.414392, Current loss=0.231736\n",
      "Epoch 4 (62.89s)\tAverage loss=0.407992, Current loss=0.273600\n",
      "Epoch 4 (62.93s)\tAverage loss=0.460386, Current loss=1.613048\n",
      "Epoch 4 (62.96s)\tAverage loss=0.451906, Current loss=0.256860\n",
      "Epoch 4 (62.98s)\tAverage loss=0.491982, Current loss=1.453809\n",
      "Epoch 4 (63.05s)\tAverage loss=0.478452, Current loss=0.140192\n",
      "Epoch 4 (63.07s)\tAverage loss=0.513697, Current loss=1.430076\n",
      "Epoch 4 (63.11s)\tAverage loss=0.550481, Current loss=1.543658\n",
      "Epoch 4 (63.12s)\tAverage loss=0.580689, Current loss=1.426506\n",
      "Epoch 4 (63.14s)\tAverage loss=0.571412, Current loss=0.302378\n",
      "Epoch 4 (63.19s)\tAverage loss=0.562892, Current loss=0.307304\n",
      "Epoch 4 (63.21s)\tAverage loss=0.588176, Current loss=1.371955\n",
      "Epoch 4 (63.24s)\tAverage loss=0.579787, Current loss=0.311342\n",
      "Epoch 4 (63.27s)\tAverage loss=0.572993, Current loss=0.348790\n",
      "Epoch 4 (63.30s)\tAverage loss=0.566712, Current loss=0.353182\n",
      "Epoch 4 (63.32s)\tAverage loss=0.584677, Current loss=1.213425\n",
      "Epoch 4 (63.39s)\tAverage loss=0.574684, Current loss=0.214944\n",
      "Epoch 4 (63.43s)\tAverage loss=0.568690, Current loss=0.346906\n",
      "Epoch 4 (63.46s)\tAverage loss=0.563139, Current loss=0.352205\n",
      "Epoch 4 (63.53s)\tAverage loss=0.554861, Current loss=0.232042\n",
      "Epoch 4 (63.55s)\tAverage loss=0.550343, Current loss=0.369610\n",
      "Epoch 4 (63.58s)\tAverage loss=0.546333, Current loss=0.381916\n",
      "Epoch 4 (63.61s)\tAverage loss=0.541765, Current loss=0.349902\n",
      "Epoch 4 (63.63s)\tAverage loss=0.537514, Current loss=0.354720\n",
      "Epoch 4 (63.67s)\tAverage loss=0.533101, Current loss=0.338957\n",
      "Epoch 4 (63.89s)\tAverage loss=0.529390, Current loss=0.362382\n",
      "Epoch 4 (63.92s)\tAverage loss=0.525215, Current loss=0.333174\n",
      "Epoch 4 (63.93s)\tAverage loss=0.520897, Current loss=0.317922\n",
      "Epoch 4 (63.96s)\tAverage loss=0.516343, Current loss=0.297742\n",
      "Epoch 4 (63.99s)\tAverage loss=0.512150, Current loss=0.306698\n",
      "Epoch 4 (64.01s)\tAverage loss=0.508338, Current loss=0.317744\n",
      "Epoch 4 (64.03s)\tAverage loss=0.525789, Current loss=1.415808\n",
      "Epoch 4 (64.05s)\tAverage loss=0.521141, Current loss=0.279419\n",
      "Epoch 4 (64.09s)\tAverage loss=0.516773, Current loss=0.285268\n",
      "Epoch 4 (64.11s)\tAverage loss=0.532977, Current loss=1.408019\n",
      "Epoch 4 (64.14s)\tAverage loss=0.528332, Current loss=0.272866\n",
      "Epoch 4 (64.17s)\tAverage loss=0.523795, Current loss=0.269740\n",
      "Epoch 4 (64.20s)\tAverage loss=0.519327, Current loss=0.264615\n",
      "Epoch 4 (64.24s)\tAverage loss=0.515049, Current loss=0.266929\n",
      "Epoch 4 (64.27s)\tAverage loss=0.510726, Current loss=0.255693\n",
      "Epoch 4 (64.31s)\tAverage loss=0.506308, Current loss=0.241180\n",
      "Epoch 4 (64.36s)\tAverage loss=0.502087, Current loss=0.244617\n",
      "Epoch 4 (64.40s)\tAverage loss=0.497793, Current loss=0.231587\n",
      "Epoch 4 (64.44s)\tAverage loss=0.493696, Current loss=0.235586\n",
      "Epoch 4 (64.47s)\tAverage loss=0.489477, Current loss=0.219453\n",
      "Epoch 4 (64.50s)\tAverage loss=0.485545, Current loss=0.229932\n",
      "Epoch 4 (64.53s)\tAverage loss=0.481639, Current loss=0.223896\n",
      "Epoch 4 (64.56s)\tAverage loss=0.477733, Current loss=0.216042\n",
      "Epoch 4 (64.59s)\tAverage loss=0.473794, Current loss=0.205913\n",
      "Epoch 4 (64.63s)\tAverage loss=0.469768, Current loss=0.191996\n",
      "Epoch 4 (64.66s)\tAverage loss=0.465750, Current loss=0.184442\n",
      "Epoch 4 (64.68s)\tAverage loss=0.461755, Current loss=0.178171\n",
      "Epoch 4 (64.71s)\tAverage loss=0.479140, Current loss=1.730859\n",
      "Epoch 4 (64.73s)\tAverage loss=0.475246, Current loss=0.190957\n",
      "Epoch 4 (64.78s)\tAverage loss=0.471293, Current loss=0.178793\n",
      "Epoch 4 (64.80s)\tAverage loss=0.467242, Current loss=0.163424\n",
      "Epoch 4 (64.83s)\tAverage loss=0.463414, Current loss=0.172438\n",
      "Epoch 4 (64.85s)\tAverage loss=0.480657, Current loss=1.808398\n",
      "Epoch 4 (64.92s)\tAverage loss=0.475417, Current loss=0.066694\n",
      "Epoch 4 (64.96s)\tAverage loss=0.471385, Current loss=0.152833\n",
      "Epoch 4 (64.98s)\tAverage loss=0.487824, Current loss=1.802965\n",
      "Epoch 4 (65.01s)\tAverage loss=0.484011, Current loss=0.175153\n",
      "Epoch 4 (65.06s)\tAverage loss=0.480192, Current loss=0.167038\n",
      "Epoch 4 (65.09s)\tAverage loss=0.476471, Current loss=0.167625\n",
      "Epoch 4 (65.12s)\tAverage loss=0.472977, Current loss=0.179495\n",
      "Epoch 4 (65.15s)\tAverage loss=0.469406, Current loss=0.165817\n",
      "Epoch 4 (65.18s)\tAverage loss=0.465743, Current loss=0.150740\n",
      "Epoch 4 (65.21s)\tAverage loss=0.480131, Current loss=1.731927\n",
      "Epoch 4 (65.23s)\tAverage loss=0.476589, Current loss=0.164876\n",
      "Epoch 4 (65.26s)\tAverage loss=0.473222, Current loss=0.173512\n",
      "Epoch 4 (65.31s)\tAverage loss=0.469796, Current loss=0.161488\n",
      "Epoch 4 (65.37s)\tAverage loss=0.465327, Current loss=0.058696\n",
      "Epoch 4 (65.44s)\tAverage loss=0.460964, Current loss=0.059557\n",
      "Epoch 4 (65.48s)\tAverage loss=0.475898, Current loss=1.864739\n",
      "Epoch 4 (65.55s)\tAverage loss=0.471502, Current loss=0.058298\n",
      "Epoch 4 (65.58s)\tAverage loss=0.486475, Current loss=1.908900\n",
      "Epoch 4 (65.60s)\tAverage loss=0.483015, Current loss=0.150812\n",
      "Epoch 4 (65.64s)\tAverage loss=0.479769, Current loss=0.164906\n",
      "Epoch 4 (65.67s)\tAverage loss=0.476778, Current loss=0.183710\n",
      "Epoch 4 (65.69s)\tAverage loss=0.474062, Current loss=0.205200\n",
      "Epoch 4 (65.72s)\tAverage loss=0.471257, Current loss=0.190739\n",
      "Epoch 4 (65.74s)\tAverage loss=0.468474, Current loss=0.187355\n",
      "Epoch 4 (65.76s)\tAverage loss=0.479204, Current loss=1.573676\n",
      "Epoch 4 (65.80s)\tAverage loss=0.476254, Current loss=0.172425\n",
      "Epoch 4 (65.82s)\tAverage loss=0.473314, Current loss=0.167497\n",
      "Epoch 4 (66.07s)\tAverage loss=0.469378, Current loss=0.056165\n",
      "Epoch 4 (66.09s)\tAverage loss=0.466384, Current loss=0.148964\n",
      "Epoch 4 (66.11s)\tAverage loss=0.477482, Current loss=1.664943\n",
      "Epoch 4 (66.18s)\tAverage loss=0.473583, Current loss=0.052485\n",
      "Epoch 4 (66.21s)\tAverage loss=0.471188, Current loss=0.210169\n",
      "Epoch 4 (66.25s)\tAverage loss=0.468519, Current loss=0.174906\n",
      "Epoch 4 (66.28s)\tAverage loss=0.466050, Current loss=0.192057\n",
      "Epoch 4 (66.30s)\tAverage loss=0.463839, Current loss=0.216156\n",
      "Epoch 4 (66.35s)\tAverage loss=0.461293, Current loss=0.173573\n",
      "Epoch 4 (66.40s)\tAverage loss=0.458805, Current loss=0.175198\n",
      "Epoch 4 (66.47s)\tAverage loss=0.455290, Current loss=0.051133\n",
      "Epoch 4 (66.50s)\tAverage loss=0.452946, Current loss=0.181013\n",
      "Epoch 4 (66.52s)\tAverage loss=0.463181, Current loss=1.660714\n",
      "Epoch 4 (66.54s)\tAverage loss=0.460366, Current loss=0.128166\n",
      "Epoch 4 (66.59s)\tAverage loss=0.474250, Current loss=2.126461\n",
      "Epoch 4 (66.62s)\tAverage loss=0.485204, Current loss=1.799622\n",
      "Epoch 4 (66.66s)\tAverage loss=0.482447, Current loss=0.148855\n",
      "Epoch 4 (66.69s)\tAverage loss=0.492161, Current loss=1.677333\n",
      "Epoch 4 (66.72s)\tAverage loss=0.489513, Current loss=0.163791\n",
      "Epoch 4 (66.76s)\tAverage loss=0.487133, Current loss=0.192038\n",
      "Epoch 4 (66.80s)\tAverage loss=0.496744, Current loss=1.698078\n",
      "Epoch 4 (66.83s)\tAverage loss=0.504854, Current loss=1.526711\n",
      "Epoch 4 (66.86s)\tAverage loss=0.503102, Current loss=0.280548\n",
      "Epoch 4 (66.89s)\tAverage loss=0.510474, Current loss=1.454162\n",
      "Epoch 4 (66.91s)\tAverage loss=0.508441, Current loss=0.246109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (66.98s)\tAverage loss=0.505209, Current loss=0.085124\n",
      "Epoch 4 (67.03s)\tAverage loss=0.512347, Current loss=1.447414\n",
      "Epoch 4 (67.10s)\tAverage loss=0.509193, Current loss=0.092804\n",
      "Epoch 4 (67.17s)\tAverage loss=0.506110, Current loss=0.096085\n",
      "Epoch 4 (67.21s)\tAverage loss=0.504810, Current loss=0.330606\n",
      "Epoch 4 (67.25s)\tAverage loss=0.503635, Current loss=0.345057\n",
      "Epoch 4 (67.32s)\tAverage loss=0.500986, Current loss=0.140740\n",
      "Epoch 4 (67.35s)\tAverage loss=0.499977, Current loss=0.361769\n",
      "Epoch 4 (67.39s)\tAverage loss=0.498986, Current loss=0.362158\n",
      "Epoch 4 (67.41s)\tAverage loss=0.504224, Current loss=1.232293\n",
      "Epoch 4 (67.46s)\tAverage loss=0.502784, Current loss=0.301150\n",
      "Epoch 4 (67.51s)\tAverage loss=0.501350, Current loss=0.299184\n",
      "Epoch 4 (67.54s)\tAverage loss=0.500357, Current loss=0.359425\n",
      "Epoch 4 (67.57s)\tAverage loss=0.505092, Current loss=1.182202\n",
      "Epoch 4 (67.60s)\tAverage loss=0.504103, Current loss=0.361594\n",
      "Epoch 4 (67.64s)\tAverage loss=0.503127, Current loss=0.361665\n",
      "Epoch 4 (67.65s)\tAverage loss=0.502183, Current loss=0.364271\n",
      "Epoch 4 (67.67s)\tAverage loss=0.501155, Current loss=0.350055\n",
      "Epoch 4 (67.72s)\tAverage loss=0.500123, Current loss=0.347458\n",
      "Epoch 4 (67.77s)\tAverage loss=0.499023, Current loss=0.335168\n",
      "Epoch 4 (67.79s)\tAverage loss=0.505063, Current loss=1.411024\n",
      "Epoch 4 (67.82s)\tAverage loss=0.504024, Current loss=0.347084\n",
      "Epoch 4 (67.84s)\tAverage loss=0.503020, Current loss=0.350499\n",
      "Epoch 4 (67.92s)\tAverage loss=0.500388, Current loss=0.097627\n",
      "Epoch 4 (67.96s)\tAverage loss=0.499381, Current loss=0.344242\n",
      "Epoch 4 (68.19s)\tAverage loss=0.504300, Current loss=1.266812\n",
      "Epoch 4 (68.22s)\tAverage loss=0.509028, Current loss=1.246638\n",
      "Epoch 4 (68.24s)\tAverage loss=0.513745, Current loss=1.254306\n",
      "Epoch 4 (68.27s)\tAverage loss=0.518374, Current loss=1.249689\n",
      "Epoch 4 (68.29s)\tAverage loss=0.517345, Current loss=0.353817\n",
      "Epoch 4 (68.31s)\tAverage loss=0.516308, Current loss=0.350353\n",
      "Epoch 4 (68.34s)\tAverage loss=0.515352, Current loss=0.361501\n",
      "Epoch 4 (68.41s)\tAverage loss=0.512668, Current loss=0.077815\n",
      "Epoch 4 (68.42s)\tAverage loss=0.516812, Current loss=1.192296\n",
      "Epoch 4 (68.46s)\tAverage loss=0.515828, Current loss=0.354427\n",
      "Epoch 4 (68.48s)\tAverage loss=0.519813, Current loss=1.177346\n",
      "Epoch 4 (68.53s)\tAverage loss=0.518604, Current loss=0.317938\n",
      "Epoch 4 (68.61s)\tAverage loss=0.515948, Current loss=0.072327\n",
      "Epoch 4 (68.64s)\tAverage loss=0.515114, Current loss=0.374937\n",
      "Epoch 4 (68.67s)\tAverage loss=0.514294, Current loss=0.375833\n",
      "Epoch 4 (68.69s)\tAverage loss=0.517995, Current loss=1.147077\n",
      "Epoch 4 (68.74s)\tAverage loss=0.516858, Current loss=0.322522\n",
      "Epoch 4 (68.77s)\tAverage loss=0.515729, Current loss=0.321448\n",
      "Epoch 4 (68.81s)\tAverage loss=0.514941, Current loss=0.378687\n",
      "Epoch 4 (68.83s)\tAverage loss=0.514111, Current loss=0.369728\n",
      "Epoch 4 (68.87s)\tAverage loss=0.517682, Current loss=1.142497\n",
      "Epoch 4 (68.91s)\tAverage loss=0.516900, Current loss=0.379287\n",
      "Epoch 4 (68.95s)\tAverage loss=0.516045, Current loss=0.364715\n",
      "Epoch 4 (68.98s)\tAverage loss=0.515145, Current loss=0.354943\n",
      "Epoch 4 (69.00s)\tAverage loss=0.514327, Current loss=0.367889\n",
      "Epoch 4 (69.01s)\tAverage loss=0.513529, Current loss=0.369933\n",
      "Epoch 4 (69.08s)\tAverage loss=0.511060, Current loss=0.064146\n",
      "Epoch 4 (69.13s)\tAverage loss=0.509822, Current loss=0.284490\n",
      "Epoch 4 (69.18s)\tAverage loss=0.508870, Current loss=0.334662\n",
      "Epoch 4 (69.21s)\tAverage loss=0.507900, Current loss=0.329505\n",
      "Epoch 4 (69.24s)\tAverage loss=0.506975, Current loss=0.335845\n",
      "Epoch 4 (69.27s)\tAverage loss=0.506012, Current loss=0.326830\n",
      "Epoch 4 (69.29s)\tAverage loss=0.504935, Current loss=0.303527\n",
      "Epoch 4 (69.32s)\tAverage loss=0.509451, Current loss=1.358419\n",
      "Epoch 4 (69.40s)\tAverage loss=0.507198, Current loss=0.081519\n",
      "Epoch 4 (69.43s)\tAverage loss=0.511666, Current loss=1.360542\n",
      "Epoch 4 (69.47s)\tAverage loss=0.516341, Current loss=1.409316\n",
      "Epoch 4 (69.50s)\tAverage loss=0.520536, Current loss=1.325962\n",
      "Epoch 4 (69.57s)\tAverage loss=0.518085, Current loss=0.044937\n",
      "Epoch 4 (69.65s)\tAverage loss=0.530696, Current loss=2.977278\n",
      "Epoch 4 (69.68s)\tAverage loss=0.529639, Current loss=0.323594\n",
      "Epoch 4 (69.75s)\tAverage loss=0.527224, Current loss=0.053737\n",
      "Epoch 4 (69.77s)\tAverage loss=0.526300, Current loss=0.344378\n",
      "Epoch 4 (69.81s)\tAverage loss=0.530060, Current loss=1.274554\n",
      "Epoch 4 (69.86s)\tAverage loss=0.528793, Current loss=0.276651\n",
      "Epoch 4 (69.89s)\tAverage loss=0.527948, Current loss=0.358952\n",
      "Epoch 4 (69.91s)\tAverage loss=0.527078, Current loss=0.352125\n",
      "Epoch 4 (69.93s)\tAverage loss=0.526233, Current loss=0.355609\n",
      "Epoch 4 (69.96s)\tAverage loss=0.529416, Current loss=1.175480\n",
      "Epoch 4 (70.00s)\tAverage loss=0.528244, Current loss=0.289218\n",
      "Epoch 4 (70.04s)\tAverage loss=0.527494, Current loss=0.373707\n",
      "Epoch 4 (70.11s)\tAverage loss=0.525445, Current loss=0.103443\n",
      "Epoch 4 (70.17s)\tAverage loss=0.524302, Current loss=0.287695\n",
      "Epoch 4 (70.38s)\tAverage loss=0.523597, Current loss=0.376875\n",
      "Epoch 4 (70.41s)\tAverage loss=0.522843, Current loss=0.365396\n",
      "Epoch 4 (70.43s)\tAverage loss=0.521689, Current loss=0.279342\n",
      "Epoch 4 (70.45s)\tAverage loss=0.520947, Current loss=0.364371\n",
      "Epoch 4 (70.46s)\tAverage loss=0.520150, Current loss=0.351075\n",
      "Epoch 4 (70.49s)\tAverage loss=0.519350, Current loss=0.348977\n",
      "Epoch 4 (70.53s)\tAverage loss=0.518125, Current loss=0.256002\n",
      "Epoch 4 (70.57s)\tAverage loss=0.517312, Current loss=0.342405\n",
      "Epoch 4 (70.58s)\tAverage loss=0.516480, Current loss=0.336864\n",
      "Epoch 4 (70.60s)\tAverage loss=0.515560, Current loss=0.315842\n",
      "Epoch 4 (70.63s)\tAverage loss=0.514661, Current loss=0.318758\n",
      "Epoch 4 (70.66s)\tAverage loss=0.513742, Current loss=0.312571\n",
      "Epoch 4 (70.70s)\tAverage loss=0.512816, Current loss=0.308927\n",
      "Epoch 4 (70.75s)\tAverage loss=0.511449, Current loss=0.209377\n",
      "Epoch 4 (70.78s)\tAverage loss=0.510455, Current loss=0.289810\n",
      "Epoch 4 (70.80s)\tAverage loss=0.514324, Current loss=1.377164\n",
      "Epoch 4 (70.84s)\tAverage loss=0.513210, Current loss=0.263584\n",
      "Epoch 4 (70.88s)\tAverage loss=0.512139, Current loss=0.271286\n",
      "Epoch 4 (70.91s)\tAverage loss=0.511069, Current loss=0.269075\n",
      "Epoch 4 (70.95s)\tAverage loss=0.509954, Current loss=0.256977\n",
      "Epoch 4 (71.02s)\tAverage loss=0.508020, Current loss=0.067051\n",
      "Epoch 4 (71.05s)\tAverage loss=0.506912, Current loss=0.253172\n",
      "Epoch 4 (71.09s)\tAverage loss=0.505728, Current loss=0.233317\n",
      "Epoch 4 (71.12s)\tAverage loss=0.504616, Current loss=0.247789\n",
      "Epoch 4 (71.15s)\tAverage loss=0.503540, Current loss=0.253989\n",
      "Epoch 4 (71.17s)\tAverage loss=0.502427, Current loss=0.242985\n",
      "Epoch 4 (71.19s)\tAverage loss=0.501279, Current loss=0.232787\n",
      "Epoch 4 (71.22s)\tAverage loss=0.500048, Current loss=0.210686\n",
      "Epoch 4 (71.24s)\tAverage loss=0.504649, Current loss=1.590516\n",
      "Epoch 4 (71.28s)\tAverage loss=0.503448, Current loss=0.218678\n",
      "Epoch 4 (71.33s)\tAverage loss=0.501917, Current loss=0.137732\n",
      "Epoch 4 (71.34s)\tAverage loss=0.500694, Current loss=0.208357\n",
      "Epoch 4 (71.38s)\tAverage loss=0.505721, Current loss=1.712226\n",
      "Epoch 4 (71.41s)\tAverage loss=0.504478, Current loss=0.204783\n",
      "Epoch 4 (71.43s)\tAverage loss=0.502940, Current loss=0.130841\n",
      "Epoch 4 (71.45s)\tAverage loss=0.507778, Current loss=1.683286\n",
      "Epoch 4 (71.47s)\tAverage loss=0.506553, Current loss=0.207786\n",
      "Epoch 4 (71.49s)\tAverage loss=0.512900, Current loss=2.067733\n",
      "Epoch 4 (71.54s)\tAverage loss=0.511639, Current loss=0.201474\n",
      "Epoch 4 (71.57s)\tAverage loss=0.510489, Current loss=0.226413\n",
      "Epoch 4 (71.60s)\tAverage loss=0.509324, Current loss=0.220538\n",
      "Epoch 4 (71.63s)\tAverage loss=0.508185, Current loss=0.224593\n",
      "Epoch 4 (71.65s)\tAverage loss=0.506993, Current loss=0.208854\n",
      "Epoch 4 (71.68s)\tAverage loss=0.505843, Current loss=0.217248\n",
      "Epoch 4 (71.70s)\tAverage loss=0.504427, Current loss=0.147512\n",
      "Epoch 4 (71.73s)\tAverage loss=0.503332, Current loss=0.226465\n",
      "Epoch 4 (71.75s)\tAverage loss=0.507656, Current loss=1.605984\n",
      "Epoch 4 (71.77s)\tAverage loss=0.512118, Current loss=1.649877\n",
      "Epoch 4 (71.85s)\tAverage loss=0.510324, Current loss=0.051184\n",
      "Epoch 4 (71.89s)\tAverage loss=0.509120, Current loss=0.199522\n",
      "Epoch 4 (71.94s)\tAverage loss=0.514723, Current loss=1.960436\n",
      "Epoch 4 (71.97s)\tAverage loss=0.513554, Current loss=0.210697\n",
      "Epoch 4 (72.00s)\tAverage loss=0.517807, Current loss=1.623585\n",
      "Epoch 4 (72.02s)\tAverage loss=0.522674, Current loss=1.792922\n",
      "Epoch 4 (72.05s)\tAverage loss=0.526552, Current loss=1.542548\n",
      "Epoch 4 (72.08s)\tAverage loss=0.525499, Current loss=0.248591\n",
      "Epoch 4 (72.10s)\tAverage loss=0.524145, Current loss=0.166699\n",
      "Epoch 4 (72.14s)\tAverage loss=0.522953, Current loss=0.207035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (72.17s)\tAverage loss=0.522003, Current loss=0.269468\n",
      "Epoch 4 (72.21s)\tAverage loss=0.521087, Current loss=0.276296\n",
      "Epoch 4 (72.29s)\tAverage loss=0.519436, Current loss=0.077095\n",
      "Epoch 4 (72.32s)\tAverage loss=0.518477, Current loss=0.260504\n",
      "Epoch 4 (72.53s)\tAverage loss=0.521693, Current loss=1.389921\n",
      "Epoch 4 (72.55s)\tAverage loss=0.520835, Current loss=0.288395\n",
      "Epoch 4 (72.59s)\tAverage loss=0.519781, Current loss=0.233044\n",
      "Epoch 4 (72.61s)\tAverage loss=0.518919, Current loss=0.283659\n",
      "Epoch 4 (72.64s)\tAverage loss=0.518050, Current loss=0.279917\n",
      "Epoch 4 (72.66s)\tAverage loss=0.517181, Current loss=0.278245\n",
      "Epoch 4 (72.69s)\tAverage loss=0.516350, Current loss=0.286987\n",
      "Epoch 4 (72.71s)\tAverage loss=0.519431, Current loss=1.372765\n",
      "Epoch 4 (72.75s)\tAverage loss=0.518583, Current loss=0.282906\n",
      "Epoch 4 (72.77s)\tAverage loss=0.517767, Current loss=0.290217\n",
      "Epoch 4 (72.80s)\tAverage loss=0.516864, Current loss=0.264001\n",
      "Epoch 4 (72.82s)\tAverage loss=0.516055, Current loss=0.288605\n",
      "Epoch 4 (72.84s)\tAverage loss=0.519164, Current loss=1.396033\n",
      "Epoch 4 (72.87s)\tAverage loss=0.518319, Current loss=0.279080\n",
      "Epoch 4 (72.90s)\tAverage loss=0.517436, Current loss=0.266723\n",
      "Epoch 4 (72.92s)\tAverage loss=0.516572, Current loss=0.270420\n",
      "Epoch 4 (72.95s)\tAverage loss=0.515739, Current loss=0.277354\n",
      "Epoch 4 (72.99s)\tAverage loss=0.514915, Current loss=0.278575\n",
      "Epoch 4 (73.01s)\tAverage loss=0.514070, Current loss=0.270451\n",
      "Epoch 4 (73.04s)\tAverage loss=0.517327, Current loss=1.458665\n",
      "Epoch 4 (73.07s)\tAverage loss=0.516358, Current loss=0.235289\n",
      "Epoch 4 (73.10s)\tAverage loss=0.515489, Current loss=0.262781\n",
      "Epoch 4 (73.13s)\tAverage loss=0.518588, Current loss=1.423502\n",
      "Epoch 4 (73.16s)\tAverage loss=0.517638, Current loss=0.239255\n",
      "Epoch 4 (73.18s)\tAverage loss=0.516768, Current loss=0.261035\n",
      "Epoch 4 (73.23s)\tAverage loss=0.515751, Current loss=0.215661\n",
      "Epoch 4 (73.28s)\tAverage loss=0.519564, Current loss=1.648245\n",
      "Epoch 4 (73.31s)\tAverage loss=0.518643, Current loss=0.245199\n",
      "Epoch 4 (73.36s)\tAverage loss=0.517635, Current loss=0.217276\n",
      "Epoch 4 (73.39s)\tAverage loss=0.516793, Current loss=0.264927\n",
      "Epoch 4 (73.42s)\tAverage loss=0.515855, Current loss=0.234344\n",
      "Epoch 4 (73.44s)\tAverage loss=0.515005, Current loss=0.259341\n",
      "Epoch 4 (73.51s)\tAverage loss=0.513581, Current loss=0.083332\n",
      "Epoch 4 (73.56s)\tAverage loss=0.512745, Current loss=0.259527\n",
      "Epoch 4 (73.58s)\tAverage loss=0.516208, Current loss=1.568973\n",
      "Epoch 4 (73.60s)\tAverage loss=0.515280, Current loss=0.232341\n",
      "Epoch 4 (73.64s)\tAverage loss=0.514436, Current loss=0.256028\n",
      "Epoch 4 (73.68s)\tAverage loss=0.513524, Current loss=0.233472\n",
      "Epoch 4 (73.71s)\tAverage loss=0.512613, Current loss=0.232060\n",
      "Epoch 4 (73.78s)\tAverage loss=0.511117, Current loss=0.048927\n",
      "Epoch 4 (73.86s)\tAverage loss=0.509697, Current loss=0.069450\n",
      "Epoch 4 (73.89s)\tAverage loss=0.512963, Current loss=1.528678\n",
      "Epoch 4 (73.93s)\tAverage loss=0.516373, Current loss=1.580490\n",
      "Epoch 4 (73.97s)\tAverage loss=0.515480, Current loss=0.235797\n",
      "Epoch 4 (74.01s)\tAverage loss=0.514546, Current loss=0.221466\n",
      "Epoch 4 (74.04s)\tAverage loss=0.513616, Current loss=0.220574\n",
      "Epoch 4 (74.08s)\tAverage loss=0.512781, Current loss=0.248925\n",
      "Epoch 4 (74.15s)\tAverage loss=0.511470, Current loss=0.095945\n",
      "Epoch 4 (74.17s)\tAverage loss=0.510699, Current loss=0.265482\n",
      "Epoch 4 (74.19s)\tAverage loss=0.513984, Current loss=1.561782\n",
      "Epoch 4 (74.24s)\tAverage loss=0.513036, Current loss=0.209584\n",
      "Epoch 4 (74.26s)\tAverage loss=0.512121, Current loss=0.218661\n",
      "Epoch 4 (74.31s)\tAverage loss=0.511183, Current loss=0.209030\n",
      "Epoch 4 (74.35s)\tAverage loss=0.510250, Current loss=0.208702\n",
      "Epoch 4 (74.38s)\tAverage loss=0.509448, Current loss=0.249893\n",
      "Epoch 4 (74.43s)\tAverage loss=0.508648, Current loss=0.248564\n",
      "Epoch 4 (74.65s)\tAverage loss=0.507233, Current loss=0.046010\n",
      "Epoch 4 (74.71s)\tAverage loss=0.506420, Current loss=0.240424\n",
      "Epoch 4 (74.74s)\tAverage loss=0.505618, Current loss=0.242444\n",
      "Epoch 4 (74.77s)\tAverage loss=0.504692, Current loss=0.200198\n",
      "Epoch 4 (74.79s)\tAverage loss=0.507822, Current loss=1.540741\n",
      "Epoch 4 (74.86s)\tAverage loss=0.506415, Current loss=0.040553\n",
      "Epoch 4 (74.93s)\tAverage loss=0.505020, Current loss=0.041865\n",
      "Epoch 4 (74.97s)\tAverage loss=0.504206, Current loss=0.233128\n",
      "Epoch 4 (74.99s)\tAverage loss=0.503401, Current loss=0.234568\n",
      "Epoch 4 (75.02s)\tAverage loss=0.502572, Current loss=0.224954\n",
      "Epoch 4 (75.05s)\tAverage loss=0.501653, Current loss=0.192911\n",
      "Epoch 4 (75.08s)\tAverage loss=0.500876, Current loss=0.238973\n",
      "Epoch 4 (75.12s)\tAverage loss=0.500044, Current loss=0.218728\n",
      "Epoch 4 (75.14s)\tAverage loss=0.499219, Current loss=0.219654\n",
      "Epoch 4 (75.17s)\tAverage loss=0.498284, Current loss=0.180306\n",
      "Epoch 4 (75.20s)\tAverage loss=0.497363, Current loss=0.183438\n",
      "Epoch 4 (75.23s)\tAverage loss=0.500620, Current loss=1.614565\n",
      "Epoch 4 (75.25s)\tAverage loss=0.499727, Current loss=0.193245\n",
      "Epoch 4 (75.30s)\tAverage loss=0.498863, Current loss=0.201620\n",
      "Epoch 4 (75.32s)\tAverage loss=0.502251, Current loss=1.671096\n",
      "Epoch 4 (75.36s)\tAverage loss=0.501393, Current loss=0.204610\n",
      "Epoch 4 (75.39s)\tAverage loss=0.500585, Current loss=0.220061\n",
      "Epoch 4 (75.41s)\tAverage loss=0.503838, Current loss=1.636197\n",
      "Epoch 4 (75.44s)\tAverage loss=0.502878, Current loss=0.167843\n",
      "Epoch 4 (75.48s)\tAverage loss=0.501903, Current loss=0.160354\n",
      "Epoch 4 (75.56s)\tAverage loss=0.500556, Current loss=0.027855\n",
      "Epoch 4 (75.59s)\tAverage loss=0.499727, Current loss=0.207828\n",
      "Epoch 4 (75.67s)\tAverage loss=0.498391, Current loss=0.026991\n",
      "Epoch 4 (75.70s)\tAverage loss=0.501874, Current loss=1.734966\n",
      "Epoch 4 (75.73s)\tAverage loss=0.501053, Current loss=0.209583\n",
      "Epoch 4 (75.80s)\tAverage loss=0.499718, Current loss=0.024315\n",
      "Epoch 4 (75.82s)\tAverage loss=0.498795, Current loss=0.169278\n",
      "Epoch 4 (75.85s)\tAverage loss=0.497995, Current loss=0.211426\n",
      "Epoch 4 (75.90s)\tAverage loss=0.497045, Current loss=0.156006\n",
      "Epoch 4 (75.92s)\tAverage loss=0.496246, Current loss=0.208862\n",
      "Epoch 4 (76.00s)\tAverage loss=0.494945, Current loss=0.025316\n",
      "Epoch 4 (76.03s)\tAverage loss=0.494034, Current loss=0.164152\n",
      "Epoch 4 (76.08s)\tAverage loss=0.493116, Current loss=0.159871\n",
      "Epoch 4 (76.12s)\tAverage loss=0.492165, Current loss=0.145831\n",
      "Epoch 4 (76.16s)\tAverage loss=0.491346, Current loss=0.192608\n",
      "Epoch 4 (76.21s)\tAverage loss=0.490418, Current loss=0.150901\n",
      "Epoch 4 (76.27s)\tAverage loss=0.489145, Current loss=0.021945\n",
      "Epoch 4 (76.29s)\tAverage loss=0.492845, Current loss=1.854235\n",
      "Epoch 4 (76.34s)\tAverage loss=0.492107, Current loss=0.219936\n",
      "Epoch 4 (76.38s)\tAverage loss=0.491339, Current loss=0.206909\n",
      "Epoch 4 (76.42s)\tAverage loss=0.490386, Current loss=0.136968\n",
      "Epoch 4 (76.44s)\tAverage loss=0.489647, Current loss=0.214720\n",
      "Epoch 4 (76.49s)\tAverage loss=0.488721, Current loss=0.143537\n",
      "Epoch 4 (76.53s)\tAverage loss=0.487942, Current loss=0.196295\n",
      "Epoch 4 (76.56s)\tAverage loss=0.487145, Current loss=0.188234\n",
      "Epoch 4 (76.63s)\tAverage loss=0.485905, Current loss=0.019903\n",
      "Epoch 4 (76.86s)\tAverage loss=0.484700, Current loss=0.030347\n",
      "Epoch 4 (76.89s)\tAverage loss=0.483897, Current loss=0.180332\n",
      "Epoch 4 (76.92s)\tAverage loss=0.483134, Current loss=0.193897\n",
      "Epoch 4 (76.94s)\tAverage loss=0.482294, Current loss=0.163370\n",
      "Epoch 4 (76.96s)\tAverage loss=0.481546, Current loss=0.196378\n",
      "Epoch 4 (76.99s)\tAverage loss=0.480751, Current loss=0.177049\n",
      "Epoch 4 (77.01s)\tAverage loss=0.484548, Current loss=1.938607\n",
      "Epoch 4 (77.05s)\tAverage loss=0.489002, Current loss=2.199689\n",
      "Epoch 4 (77.08s)\tAverage loss=0.488242, Current loss=0.195444\n",
      "Epoch 4 (77.12s)\tAverage loss=0.487431, Current loss=0.174590\n",
      "Epoch 4 (77.16s)\tAverage loss=0.486645, Current loss=0.182298\n",
      "Epoch 4 (77.19s)\tAverage loss=0.485859, Current loss=0.180854\n",
      "Epoch 4 (77.22s)\tAverage loss=0.489046, Current loss=1.728808\n",
      "Epoch 4 (77.30s)\tAverage loss=0.487842, Current loss=0.018124\n",
      "Epoch 4 (77.32s)\tAverage loss=0.487118, Current loss=0.204035\n",
      "Epoch 4 (77.35s)\tAverage loss=0.486389, Current loss=0.200920\n",
      "Epoch 4 (77.37s)\tAverage loss=0.489427, Current loss=1.683099\n",
      "Epoch 4 (77.40s)\tAverage loss=0.488641, Current loss=0.179019\n",
      "Epoch 4 (77.44s)\tAverage loss=0.487926, Current loss=0.205660\n",
      "Epoch 4 (77.48s)\tAverage loss=0.487021, Current loss=0.128377\n",
      "Epoch 4 (77.52s)\tAverage loss=0.486304, Current loss=0.201679\n",
      "Epoch 4 (77.55s)\tAverage loss=0.485505, Current loss=0.167654\n",
      "Epoch 4 (77.58s)\tAverage loss=0.484764, Current loss=0.189299\n",
      "Epoch 4 (77.61s)\tAverage loss=0.484049, Current loss=0.197961\n",
      "Epoch 4 (77.65s)\tAverage loss=0.483164, Current loss=0.128034\n",
      "Epoch 4 (77.68s)\tAverage loss=0.482361, Current loss=0.159832\n",
      "Epoch 4 (77.70s)\tAverage loss=0.481657, Current loss=0.197781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (77.73s)\tAverage loss=0.480925, Current loss=0.185173\n",
      "Epoch 4 (77.76s)\tAverage loss=0.480124, Current loss=0.155853\n",
      "Epoch 4 (77.78s)\tAverage loss=0.479369, Current loss=0.172717\n",
      "Epoch 4 (77.80s)\tAverage loss=0.478490, Current loss=0.120606\n",
      "Epoch 4 (77.88s)\tAverage loss=0.477363, Current loss=0.017661\n",
      "Epoch 4 (77.90s)\tAverage loss=0.476604, Current loss=0.166229\n",
      "Epoch 4 (77.92s)\tAverage loss=0.475821, Current loss=0.154811\n",
      "Epoch 4 (77.96s)\tAverage loss=0.475063, Current loss=0.163491\n",
      "Epoch 4 (77.98s)\tAverage loss=0.478120, Current loss=1.737414\n",
      "Epoch 4 (78.01s)\tAverage loss=0.477278, Current loss=0.129736\n",
      "Epoch 4 (78.03s)\tAverage loss=0.480735, Current loss=1.911988\n",
      "Epoch 4 (78.06s)\tAverage loss=0.479890, Current loss=0.128949\n",
      "Epoch 4 (78.10s)\tAverage loss=0.479144, Current loss=0.168871\n",
      "Epoch 4 (78.17s)\tAverage loss=0.478034, Current loss=0.015210\n",
      "Epoch 4 (78.22s)\tAverage loss=0.477209, Current loss=0.132642\n",
      "Epoch 4 (78.27s)\tAverage loss=0.476315, Current loss=0.101644\n",
      "Epoch 4 (78.31s)\tAverage loss=0.475554, Current loss=0.155795\n",
      "Epoch 4 (78.35s)\tAverage loss=0.474725, Current loss=0.125663\n",
      "Epoch 4 (78.37s)\tAverage loss=0.473980, Current loss=0.159538\n",
      "Epoch 4 (78.44s)\tAverage loss=0.482881, Current loss=4.248089\n",
      "Epoch 4 (78.47s)\tAverage loss=0.482168, Current loss=0.180123\n",
      "Epoch 4 (78.54s)\tAverage loss=0.481079, Current loss=0.017924\n",
      "Epoch 4 (78.61s)\tAverage loss=0.480003, Current loss=0.021679\n",
      "Epoch 4 (78.65s)\tAverage loss=0.483774, Current loss=2.094049\n",
      "Epoch 4 (78.72s)\tAverage loss=0.482702, Current loss=0.024050\n",
      "Epoch 4 (78.80s)\tAverage loss=0.481682, Current loss=0.043813\n",
      "Epoch 4 (78.82s)\tAverage loss=0.480941, Current loss=0.162472\n",
      "Epoch 4 (79.03s)\tAverage loss=0.480233, Current loss=0.175147\n",
      "Epoch 4 (79.07s)\tAverage loss=0.479509, Current loss=0.166585\n",
      "Epoch 4 (79.10s)\tAverage loss=0.478873, Current loss=0.203443\n",
      "Epoch 4 (79.13s)\tAverage loss=0.478072, Current loss=0.130763\n",
      "Epoch 4 (79.18s)\tAverage loss=0.477259, Current loss=0.123348\n",
      "Epoch 4 (79.20s)\tAverage loss=0.476588, Current loss=0.183891\n",
      "Epoch 4 (79.24s)\tAverage loss=0.475876, Current loss=0.164732\n",
      "Epoch 4 (79.27s)\tAverage loss=0.479113, Current loss=1.897205\n",
      "Epoch 4 (79.31s)\tAverage loss=0.478397, Current loss=0.163821\n",
      "Epoch 4 (79.36s)\tAverage loss=0.481439, Current loss=1.819909\n",
      "Epoch 4 (79.38s)\tAverage loss=0.484165, Current loss=1.686707\n",
      "Epoch 4 (79.42s)\tAverage loss=0.483531, Current loss=0.203138\n",
      "Epoch 4 (79.44s)\tAverage loss=0.482889, Current loss=0.198557\n",
      "Epoch 4 (79.48s)\tAverage loss=0.485571, Current loss=1.676330\n",
      "Epoch 4 (79.52s)\tAverage loss=0.484920, Current loss=0.195223\n",
      "Epoch 4 (79.57s)\tAverage loss=0.484151, Current loss=0.141032\n",
      "Epoch 4 (79.64s)\tAverage loss=0.483181, Current loss=0.049721\n",
      "Epoch 4 (79.67s)\tAverage loss=0.482564, Current loss=0.206128\n",
      "Epoch 4 (79.69s)\tAverage loss=0.485106, Current loss=1.626270\n",
      "Epoch 4 (79.73s)\tAverage loss=0.484361, Current loss=0.149339\n",
      "Epoch 4 (79.77s)\tAverage loss=0.483796, Current loss=0.229029\n",
      "Epoch 4 (79.79s)\tAverage loss=0.486197, Current loss=1.571536\n",
      "Epoch 4 (79.82s)\tAverage loss=0.485609, Current loss=0.218997\n",
      "Epoch 4 (79.84s)\tAverage loss=0.485081, Current loss=0.245563\n",
      "Epoch 4 (79.89s)\tAverage loss=0.484546, Current loss=0.240843\n",
      "Epoch 4 (79.93s)\tAverage loss=0.484014, Current loss=0.241753\n",
      "Epoch 4 (79.96s)\tAverage loss=0.486193, Current loss=1.481910\n",
      "Epoch 4 (80.00s)\tAverage loss=0.485692, Current loss=0.255952\n",
      "Epoch 4 (80.03s)\tAverage loss=0.485178, Current loss=0.249405\n",
      "Epoch 4 (80.10s)\tAverage loss=0.484247, Current loss=0.055869\n",
      "Epoch 4 (80.14s)\tAverage loss=0.483703, Current loss=0.233094\n",
      "Epoch 4 (80.17s)\tAverage loss=0.485851, Current loss=1.478142\n",
      "Epoch 4 (80.21s)\tAverage loss=0.485360, Current loss=0.257935\n",
      "Epoch 4 (80.23s)\tAverage loss=0.484861, Current loss=0.253471\n",
      "Epoch 4 (80.26s)\tAverage loss=0.484380, Current loss=0.260721\n",
      "Epoch 4 (80.29s)\tAverage loss=0.483825, Current loss=0.225249\n",
      "Epoch 4 (80.36s)\tAverage loss=0.482907, Current loss=0.053981\n",
      "Epoch 4 (80.38s)\tAverage loss=0.485795, Current loss=1.837750\n",
      "Epoch 4 (80.41s)\tAverage loss=0.485267, Current loss=0.237360\n",
      "Epoch 4 (80.47s)\tAverage loss=0.484586, Current loss=0.164340\n",
      "Epoch 4 (80.49s)\tAverage loss=0.484002, Current loss=0.209024\n",
      "Epoch 4 (80.52s)\tAverage loss=0.483549, Current loss=0.270024\n",
      "Epoch 4 (80.55s)\tAverage loss=0.482895, Current loss=0.173271\n",
      "Epoch 4 (80.56s)\tAverage loss=0.482411, Current loss=0.253074\n",
      "Epoch 4 (80.59s)\tAverage loss=0.481938, Current loss=0.257410\n",
      "Epoch 4 (80.63s)\tAverage loss=0.481398, Current loss=0.224207\n",
      "Epoch 4 (80.67s)\tAverage loss=0.483620, Current loss=1.543558\n",
      "Epoch 4 (80.74s)\tAverage loss=0.482715, Current loss=0.049964\n",
      "Epoch 4 (80.78s)\tAverage loss=0.485090, Current loss=1.623055\n",
      "Epoch 4 (80.80s)\tAverage loss=0.487457, Current loss=1.623669\n",
      "Epoch 4 (80.84s)\tAverage loss=0.486981, Current loss=0.257868\n",
      "Epoch 4 (80.86s)\tAverage loss=0.486451, Current loss=0.230993\n",
      "Epoch 4 (80.88s)\tAverage loss=0.485938, Current loss=0.238153\n",
      "Epoch 4 (80.95s)\tAverage loss=0.485048, Current loss=0.054144\n",
      "Epoch 4 (80.99s)\tAverage loss=0.484559, Current loss=0.247334\n",
      "Epoch 4 (81.22s)\tAverage loss=0.483675, Current loss=0.054223\n",
      "Epoch 4 (81.25s)\tAverage loss=0.483200, Current loss=0.251989\n",
      "Epoch 4 (81.30s)\tAverage loss=0.482730, Current loss=0.253113\n",
      "Epoch 4 (81.33s)\tAverage loss=0.482286, Current loss=0.265328\n",
      "Epoch 4 (81.39s)\tAverage loss=0.481439, Current loss=0.066493\n",
      "Epoch 4 (81.42s)\tAverage loss=0.480974, Current loss=0.252485\n",
      "Epoch 4 (81.45s)\tAverage loss=0.480451, Current loss=0.223417\n",
      "Epoch 4 (81.51s)\tAverage loss=0.479612, Current loss=0.065996\n",
      "Epoch 4 (81.58s)\tAverage loss=0.478737, Current loss=0.046222\n",
      "Epoch 4 (81.63s)\tAverage loss=0.478102, Current loss=0.164013\n",
      "Epoch 4 (81.64s)\tAverage loss=0.477611, Current loss=0.233956\n",
      "Epoch 4 (81.67s)\tAverage loss=0.479609, Current loss=1.472440\n",
      "Epoch 4 (81.71s)\tAverage loss=0.479144, Current loss=0.247407\n",
      "Epoch 4 (81.76s)\tAverage loss=0.478494, Current loss=0.154338\n",
      "Epoch 4 (81.80s)\tAverage loss=0.481453, Current loss=1.961221\n",
      "Epoch 4 (81.82s)\tAverage loss=0.480998, Current loss=0.252931\n",
      "Epoch 4 (81.86s)\tAverage loss=0.480564, Current loss=0.262769\n",
      "Epoch 4 (81.93s)\tAverage loss=0.479712, Current loss=0.050837\n",
      "Epoch 4 (81.97s)\tAverage loss=0.479209, Current loss=0.225841\n",
      "Epoch 4 (82.00s)\tAverage loss=0.478702, Current loss=0.222564\n",
      "Epoch 4 (82.03s)\tAverage loss=0.478194, Current loss=0.221300\n",
      "Epoch 4 (82.05s)\tAverage loss=0.480170, Current loss=1.482055\n",
      "Epoch 4 (82.09s)\tAverage loss=0.482262, Current loss=1.544695\n",
      "Epoch 4 (82.10s)\tAverage loss=0.481761, Current loss=0.227149\n",
      "Epoch 4 (82.13s)\tAverage loss=0.483722, Current loss=1.483375\n",
      "Epoch 4 (82.16s)\tAverage loss=0.485603, Current loss=1.447132\n",
      "Epoch 4 (82.19s)\tAverage loss=0.485060, Current loss=0.206950\n",
      "Epoch 4 (82.23s)\tAverage loss=0.484582, Current loss=0.239357\n",
      "Epoch 4 (82.27s)\tAverage loss=0.484062, Current loss=0.216805\n",
      "Epoch 4 (82.30s)\tAverage loss=0.483625, Current loss=0.258448\n",
      "Epoch 4 (82.32s)\tAverage loss=0.483075, Current loss=0.199592\n",
      "Epoch 4 (82.37s)\tAverage loss=0.482646, Current loss=0.260845\n",
      "Epoch 4 (82.41s)\tAverage loss=0.482176, Current loss=0.238574\n",
      "Epoch 4 (82.48s)\tAverage loss=0.481323, Current loss=0.038519\n",
      "Epoch 4 (82.51s)\tAverage loss=0.480919, Current loss=0.270786\n",
      "Epoch 4 (82.53s)\tAverage loss=0.482614, Current loss=1.365600\n",
      "Epoch 4 (82.57s)\tAverage loss=0.482130, Current loss=0.229854\n",
      "Epoch 4 (82.62s)\tAverage loss=0.481670, Current loss=0.240978\n",
      "Epoch 4 (82.66s)\tAverage loss=0.481090, Current loss=0.176926\n",
      "Epoch 4 (82.70s)\tAverage loss=0.480602, Current loss=0.224845\n",
      "Epoch 4 (82.74s)\tAverage loss=0.480018, Current loss=0.172423\n",
      "Epoch 4 (82.77s)\tAverage loss=0.479501, Current loss=0.207463\n",
      "Epoch 4 (82.85s)\tAverage loss=0.478656, Current loss=0.032399\n",
      "Epoch 4 (82.87s)\tAverage loss=0.478173, Current loss=0.222429\n",
      "Epoch 4 (82.92s)\tAverage loss=0.477736, Current loss=0.246452\n",
      "Epoch 4 (82.94s)\tAverage loss=0.477143, Current loss=0.161910\n",
      "Epoch 4 (82.96s)\tAverage loss=0.476525, Current loss=0.148008\n",
      "Epoch 4 (82.99s)\tAverage loss=0.476018, Current loss=0.205873\n",
      "Epoch 4 (83.06s)\tAverage loss=0.475177, Current loss=0.025837\n",
      "Epoch 4 (83.09s)\tAverage loss=0.477686, Current loss=1.820030\n",
      "Epoch 4 (83.12s)\tAverage loss=0.477259, Current loss=0.248321\n",
      "Epoch 4 (83.15s)\tAverage loss=0.476732, Current loss=0.194027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 (83.39s)\tAverage loss=0.475893, Current loss=0.024448\n",
      "Epoch 4 (83.41s)\tAverage loss=0.475421, Current loss=0.220781\n",
      "Epoch 4 (83.44s)\tAverage loss=0.477083, Current loss=1.374710\n",
      "Epoch 4 (83.49s)\tAverage loss=0.476444, Current loss=0.130628\n",
      "Epoch 4 (83.52s)\tAverage loss=0.479187, Current loss=1.965692\n",
      "Epoch 4 (83.54s)\tAverage loss=0.478834, Current loss=0.287382\n",
      "Epoch 4 (83.58s)\tAverage loss=0.478376, Current loss=0.229124\n",
      "Epoch 4 (83.62s)\tAverage loss=0.481270, Current loss=2.058419\n",
      "Epoch 4 (83.67s)\tAverage loss=0.480750, Current loss=0.197065\n",
      "Epoch 4 (83.74s)\tAverage loss=0.479921, Current loss=0.026171\n",
      "Epoch 4 (83.78s)\tAverage loss=0.479318, Current loss=0.148883\n",
      "Epoch 4 (83.82s)\tAverage loss=0.478843, Current loss=0.218313\n",
      "Epoch 4 (83.90s)\tAverage loss=0.478043, Current loss=0.038109\n",
      "Epoch 4 (83.95s)\tAverage loss=0.480579, Current loss=1.877666\n",
      "Epoch 4 (83.97s)\tAverage loss=0.480054, Current loss=0.190626\n",
      "Epoch 4 (84.02s)\tAverage loss=0.479518, Current loss=0.182892\n",
      "Epoch 4 (84.05s)\tAverage loss=0.478942, Current loss=0.159694\n",
      "Epoch 4 (84.07s)\tAverage loss=0.480626, Current loss=1.415185\n",
      "Epoch 4 (84.14s)\tAverage loss=0.479817, Current loss=0.030143\n",
      "Epoch 4 (84.16s)\tAverage loss=0.479516, Current loss=0.311714\n",
      "Epoch 4 (84.19s)\tAverage loss=0.479088, Current loss=0.240458\n",
      "Epoch 4 (84.23s)\tAverage loss=0.478593, Current loss=0.201693\n",
      "Epoch 4 (84.25s)\tAverage loss=0.478006, Current loss=0.149387\n",
      "Epoch 4 (84.30s)\tAverage loss=0.477426, Current loss=0.152368\n",
      "Epoch 4 (84.34s)\tAverage loss=0.476948, Current loss=0.208123\n",
      "Epoch 4 (84.35s)\tAverage loss=0.478566, Current loss=1.389318\n",
      "Epoch 4 (84.38s)\tAverage loss=0.480099, Current loss=1.345091\n",
      "Epoch 4 (84.40s)\tAverage loss=0.479606, Current loss=0.200588\n",
      "Epoch 4 (84.43s)\tAverage loss=0.482010, Current loss=1.842807\n",
      "Epoch 4 (84.50s)\tAverage loss=0.481313, Current loss=0.086246\n",
      "Epoch 4 (84.52s)\tAverage loss=0.480639, Current loss=0.097823\n",
      "Epoch 4 (84.56s)\tAverage loss=0.480144, Current loss=0.198491\n",
      "Epoch 4 (84.59s)\tAverage loss=0.479684, Current loss=0.217197\n",
      "Epoch 4 (84.64s)\tAverage loss=0.479152, Current loss=0.175605\n",
      "Epoch 4 (84.66s)\tAverage loss=0.478733, Current loss=0.239371\n",
      "Epoch 4 (84.69s)\tAverage loss=0.480646, Current loss=1.576506\n",
      "Epoch 4 (84.71s)\tAverage loss=0.480297, Current loss=0.279891\n",
      "Epoch 4 (84.74s)\tAverage loss=0.479768, Current loss=0.175500\n",
      "Epoch 4 (84.79s)\tAverage loss=0.479307, Current loss=0.213799\n",
      "Epoch 4 (84.81s)\tAverage loss=0.478824, Current loss=0.200205\n",
      "Epoch 4 (84.89s)\tAverage loss=0.478073, Current loss=0.043935\n",
      "Epoch 4 (84.91s)\tAverage loss=0.477866, Current loss=0.358381\n",
      "Epoch 4 (84.94s)\tAverage loss=0.477656, Current loss=0.355478\n",
      "Epoch 4 (84.99s)\tAverage loss=0.477213, Current loss=0.220078\n",
      "Epoch 4 (85.07s)\tAverage loss=0.476463, Current loss=0.040156\n",
      "Epoch 4 (85.10s)\tAverage loss=0.475924, Current loss=0.161480\n",
      "Epoch 4 (85.14s)\tAverage loss=0.477556, Current loss=1.430419\n",
      "Epoch 4 (85.16s)\tAverage loss=0.477103, Current loss=0.212417\n",
      "Epoch 4 (85.20s)\tAverage loss=0.479219, Current loss=1.719270\n",
      "Epoch 4 (85.22s)\tAverage loss=0.478669, Current loss=0.155832\n",
      "Epoch 4 (85.26s)\tAverage loss=0.478300, Current loss=0.261108\n",
      "Epoch 4 (85.28s)\tAverage loss=0.479774, Current loss=1.348127\n",
      "Epoch 4 (85.48s)\tAverage loss=0.479270, Current loss=0.181485\n",
      "Epoch 4 (85.55s)\tAverage loss=0.478798, Current loss=0.200113\n",
      "Epoch 4 (85.58s)\tAverage loss=0.479933, Current loss=1.151927\n",
      "Epoch 4 (85.65s)\tAverage loss=0.479257, Current loss=0.078208\n",
      "Epoch 4 (85.68s)\tAverage loss=0.478934, Current loss=0.287179\n",
      "Epoch 4 (85.72s)\tAverage loss=0.480821, Current loss=1.603763\n",
      "Epoch 4 (85.79s)\tAverage loss=0.480109, Current loss=0.055646\n",
      "Epoch 4 (85.83s)\tAverage loss=0.479741, Current loss=0.260187\n",
      "Epoch 4 (85.88s)\tAverage loss=0.479318, Current loss=0.226117\n",
      "Epoch 4 (85.95s)\tAverage loss=0.478585, Current loss=0.039777\n",
      "Epoch 4 (85.97s)\tAverage loss=0.478390, Current loss=0.361274\n",
      "Epoch 4 (86.01s)\tAverage loss=0.478103, Current loss=0.305392\n",
      "Epoch 4 (86.03s)\tAverage loss=0.477788, Current loss=0.288211\n",
      "Epoch 4 (86.09s)\tAverage loss=0.477045, Current loss=0.028960\n",
      "Epoch 4 (86.13s)\tAverage loss=0.476636, Current loss=0.229701\n",
      "Epoch 4 (86.21s)\tAverage loss=0.475932, Current loss=0.050083\n",
      "Epoch 4 (86.26s)\tAverage loss=0.478174, Current loss=1.836976\n",
      "Epoch 4 (86.31s)\tAverage loss=0.480062, Current loss=1.625650\n",
      "Epoch 4 (86.34s)\tAverage loss=0.479688, Current loss=0.252273\n",
      "Epoch 4 (86.37s)\tAverage loss=0.479416, Current loss=0.314030\n",
      "Epoch 4 (86.38s)\tAverage loss=0.480485, Current loss=1.132685\n",
      "Epoch 4 (86.42s)\tAverage loss=0.480334, Current loss=0.387860\n",
      "Epoch 4 (86.44s)\tAverage loss=0.479829, Current loss=0.171055\n",
      "Epoch 4 (86.47s)\tAverage loss=0.479615, Current loss=0.348175\n",
      "Epoch 4 (86.51s)\tAverage loss=0.479244, Current loss=0.251525\n",
      "Epoch 4 (86.54s)\tAverage loss=0.479040, Current loss=0.353639\n",
      "Epoch 4 (86.56s)\tAverage loss=0.480821, Current loss=1.577669\n",
      "Epoch 4 (86.58s)\tAverage loss=0.480652, Current loss=0.376663\n",
      "Epoch 4 (86.63s)\tAverage loss=0.482253, Current loss=1.471806\n",
      "Epoch 4 (86.67s)\tAverage loss=0.481941, Current loss=0.288315\n",
      "Epoch 4 (86.71s)\tAverage loss=0.481652, Current loss=0.302877\n",
      "Epoch 4 (86.75s)\tAverage loss=0.481349, Current loss=0.292723\n",
      "Epoch 4 (86.80s)\tAverage loss=0.481074, Current loss=0.310202\n",
      "Epoch 4 (86.84s)\tAverage loss=0.480744, Current loss=0.275159\n",
      "Epoch 4 (86.87s)\tAverage loss=0.481862, Current loss=1.179488\n",
      "Epoch 4 (86.92s)\tAverage loss=0.481532, Current loss=0.275714\n",
      "Epoch 4 (86.93s)\tAverage loss=0.481400, Current loss=0.398154\n",
      "Epoch 4 (86.96s)\tAverage loss=0.481107, Current loss=0.297723\n",
      "Epoch 4 (86.98s)\tAverage loss=0.482178, Current loss=1.155026\n",
      "Epoch 4 (87.05s)\tAverage loss=0.486504, Current loss=3.207140\n",
      "Epoch 4 (87.08s)\tAverage loss=0.487678, Current loss=1.227266\n",
      "Epoch 4 (87.12s)\tAverage loss=0.489040, Current loss=1.348288\n",
      "Epoch 4 (87.16s)\tAverage loss=0.488735, Current loss=0.296053\n",
      "Epoch 4 (87.20s)\tAverage loss=0.488410, Current loss=0.282784\n",
      "Epoch 4 (87.24s)\tAverage loss=0.488162, Current loss=0.330883\n",
      "Epoch 4 (87.32s)\tAverage loss=0.487488, Current loss=0.059870\n",
      "Epoch 4 (87.34s)\tAverage loss=0.487326, Current loss=0.384176\n",
      "Epoch 4 (87.37s)\tAverage loss=0.488294, Current loss=1.104999\n",
      "Epoch 4 (87.42s)\tAverage loss=0.488055, Current loss=0.335333\n",
      "Epoch 4 (87.44s)\tAverage loss=0.488960, Current loss=1.067206\n",
      "Epoch 4 (87.48s)\tAverage loss=0.488703, Current loss=0.324136\n",
      "Epoch 4 (87.70s)\tAverage loss=0.488511, Current loss=0.365452\n",
      "Epoch 4 (87.73s)\tAverage loss=0.488327, Current loss=0.370682\n",
      "Epoch 4 (87.78s)\tAverage loss=0.487735, Current loss=0.106843\n",
      "Epoch 4 (87.80s)\tAverage loss=0.487537, Current loss=0.359723\n",
      "Epoch 4 (87.83s)\tAverage loss=0.487294, Current loss=0.330993\n",
      "Epoch 4 (87.87s)\tAverage loss=0.487029, Current loss=0.315886\n",
      "Epoch 4 (87.89s)\tAverage loss=0.486941, Current loss=0.429593\n",
      "Epoch 4 (87.93s)\tAverage loss=0.486660, Current loss=0.304791\n",
      "Epoch 4 (87.95s)\tAverage loss=0.487588, Current loss=1.089610\n",
      "Epoch 4 (88.03s)\tAverage loss=0.487014, Current loss=0.114394\n",
      "Epoch 4 (88.08s)\tAverage loss=0.486671, Current loss=0.263000\n",
      "Epoch 4 (88.15s)\tAverage loss=0.486017, Current loss=0.059589\n",
      "Epoch 4 (88.18s)\tAverage loss=0.485690, Current loss=0.272280\n",
      "Epoch 4 (88.20s)\tAverage loss=0.485240, Current loss=0.190890\n",
      "Epoch 4 (88.24s)\tAverage loss=0.486222, Current loss=1.129326\n",
      "Epoch 4 (88.29s)\tAverage loss=0.485570, Current loss=0.058281\n",
      "Epoch 4 (88.36s)\tAverage loss=0.484959, Current loss=0.083116\n",
      "Epoch 4 (88.40s)\tAverage loss=0.484519, Current loss=0.194871\n",
      "Epoch 4 (88.43s)\tAverage loss=0.486203, Current loss=1.596473\n",
      "Epoch 4 (88.46s)\tAverage loss=0.488089, Current loss=1.732451\n",
      "Epoch 4 (88.49s)\tAverage loss=0.487718, Current loss=0.242751\n",
      "Epoch 4 (88.56s)\tAverage loss=0.487134, Current loss=0.100407\n",
      "Epoch 4 (88.61s)\tAverage loss=0.488804, Current loss=1.596009\n",
      "Epoch 4 (88.64s)\tAverage loss=0.488480, Current loss=0.273226\n",
      "Epoch 4 (88.68s)\tAverage loss=0.488173, Current loss=0.284359\n",
      "Epoch 4 (88.71s)\tAverage loss=0.488129, Current loss=0.458900\n",
      "Epoch 4 (88.78s)\tAverage loss=0.487601, Current loss=0.135194\n",
      "Epoch 4 (88.81s)\tAverage loss=0.487521, Current loss=0.433963\n",
      "Epoch 4 (88.84s)\tAverage loss=0.487260, Current loss=0.313066\n",
      "Epoch 4 (88.91s)\tAverage loss=0.486748, Current loss=0.143359\n",
      "Epoch 4 (88.95s)\tAverage loss=0.486551, Current loss=0.354370\n",
      "Epoch 4 (88.98s)\tAverage loss=0.487738, Current loss=1.285144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (89.74s)\tAverage loss=0.264785, Current loss=0.264785\n",
      "Epoch 5 (89.86s)\tAverage loss=0.210941, Current loss=0.157098\n",
      "Epoch 5 (89.92s)\tAverage loss=0.159343, Current loss=0.056146\n",
      "Epoch 5 (89.94s)\tAverage loss=0.199986, Current loss=0.321915\n",
      "Epoch 5 (89.99s)\tAverage loss=0.208772, Current loss=0.243914\n",
      "Epoch 5 (90.00s)\tAverage loss=0.350496, Current loss=1.059117\n",
      "Epoch 5 (90.04s)\tAverage loss=0.558906, Current loss=1.809369\n",
      "Epoch 5 (90.06s)\tAverage loss=0.519714, Current loss=0.245367\n",
      "Epoch 5 (90.08s)\tAverage loss=0.512322, Current loss=0.453189\n",
      "Epoch 5 (90.11s)\tAverage loss=0.506933, Current loss=0.458430\n",
      "Epoch 5 (90.13s)\tAverage loss=0.497968, Current loss=0.408322\n",
      "Epoch 5 (90.16s)\tAverage loss=0.477400, Current loss=0.251147\n",
      "Epoch 5 (90.18s)\tAverage loss=0.541985, Current loss=1.317000\n",
      "Epoch 5 (90.21s)\tAverage loss=0.531431, Current loss=0.394233\n",
      "Epoch 5 (90.27s)\tAverage loss=0.500593, Current loss=0.068866\n",
      "Epoch 5 (90.30s)\tAverage loss=0.500209, Current loss=0.494443\n",
      "Epoch 5 (90.33s)\tAverage loss=0.487742, Current loss=0.288269\n",
      "Epoch 5 (90.37s)\tAverage loss=0.473492, Current loss=0.231239\n",
      "Epoch 5 (90.41s)\tAverage loss=0.466906, Current loss=0.348365\n",
      "Epoch 5 (90.43s)\tAverage loss=0.466418, Current loss=0.457152\n",
      "Epoch 5 (90.47s)\tAverage loss=0.457450, Current loss=0.278081\n",
      "Epoch 5 (90.51s)\tAverage loss=0.449438, Current loss=0.281188\n",
      "Epoch 5 (90.55s)\tAverage loss=0.493307, Current loss=1.458415\n",
      "Epoch 5 (90.62s)\tAverage loss=0.475390, Current loss=0.063303\n",
      "Epoch 5 (90.65s)\tAverage loss=0.463316, Current loss=0.173552\n",
      "Epoch 5 (90.67s)\tAverage loss=0.455497, Current loss=0.260002\n",
      "Epoch 5 (90.71s)\tAverage loss=0.445495, Current loss=0.185443\n",
      "Epoch 5 (90.74s)\tAverage loss=0.438048, Current loss=0.236986\n",
      "Epoch 5 (90.79s)\tAverage loss=0.476955, Current loss=1.566352\n",
      "Epoch 5 (90.83s)\tAverage loss=0.465259, Current loss=0.126074\n",
      "Epoch 5 (90.85s)\tAverage loss=0.455380, Current loss=0.159016\n",
      "Epoch 5 (90.86s)\tAverage loss=0.448124, Current loss=0.223190\n",
      "Epoch 5 (90.89s)\tAverage loss=0.476998, Current loss=1.400973\n",
      "Epoch 5 (90.96s)\tAverage loss=0.463790, Current loss=0.027918\n",
      "Epoch 5 (90.99s)\tAverage loss=0.455741, Current loss=0.182056\n",
      "Epoch 5 (91.03s)\tAverage loss=0.448710, Current loss=0.202643\n",
      "Epoch 5 (91.05s)\tAverage loss=0.438895, Current loss=0.085557\n",
      "Epoch 5 (91.07s)\tAverage loss=0.430642, Current loss=0.125260\n",
      "Epoch 5 (91.12s)\tAverage loss=0.453166, Current loss=1.309083\n",
      "Epoch 5 (91.15s)\tAverage loss=0.446858, Current loss=0.200844\n",
      "Epoch 5 (91.18s)\tAverage loss=0.439469, Current loss=0.143931\n",
      "Epoch 5 (91.20s)\tAverage loss=0.457786, Current loss=1.208781\n",
      "Epoch 5 (91.22s)\tAverage loss=0.454117, Current loss=0.300000\n",
      "Epoch 5 (91.24s)\tAverage loss=0.445342, Current loss=0.068041\n",
      "Epoch 5 (91.27s)\tAverage loss=0.460364, Current loss=1.121307\n",
      "Epoch 5 (91.32s)\tAverage loss=0.459133, Current loss=0.403732\n",
      "Epoch 5 (91.36s)\tAverage loss=0.452514, Current loss=0.148057\n",
      "Epoch 5 (91.39s)\tAverage loss=0.445452, Current loss=0.113555\n",
      "Epoch 5 (91.42s)\tAverage loss=0.466718, Current loss=1.487464\n",
      "Epoch 5 (91.47s)\tAverage loss=0.459022, Current loss=0.081942\n",
      "Epoch 5 (91.50s)\tAverage loss=0.488445, Current loss=1.959586\n",
      "Epoch 5 (91.52s)\tAverage loss=0.498847, Current loss=1.029330\n",
      "Epoch 5 (91.54s)\tAverage loss=0.510066, Current loss=1.093452\n",
      "Epoch 5 (91.56s)\tAverage loss=0.509803, Current loss=0.495902\n",
      "Epoch 5 (91.59s)\tAverage loss=0.503036, Current loss=0.137612\n",
      "Epoch 5 (91.62s)\tAverage loss=0.502600, Current loss=0.478581\n",
      "Epoch 5 (91.65s)\tAverage loss=0.508186, Current loss=0.821041\n",
      "Epoch 5 (91.66s)\tAverage loss=0.506913, Current loss=0.434360\n",
      "Epoch 5 (91.70s)\tAverage loss=0.509841, Current loss=0.679656\n",
      "Epoch 5 (91.76s)\tAverage loss=0.503010, Current loss=0.099975\n",
      "Epoch 5 (91.78s)\tAverage loss=0.496547, Current loss=0.108786\n",
      "Epoch 5 (92.02s)\tAverage loss=0.489929, Current loss=0.086182\n",
      "Epoch 5 (92.07s)\tAverage loss=0.484237, Current loss=0.131384\n",
      "Epoch 5 (92.10s)\tAverage loss=0.479668, Current loss=0.191818\n",
      "Epoch 5 (92.12s)\tAverage loss=0.480105, Current loss=0.508020\n",
      "Epoch 5 (92.19s)\tAverage loss=0.474967, Current loss=0.141046\n",
      "Epoch 5 (92.22s)\tAverage loss=0.474708, Current loss=0.457601\n",
      "Epoch 5 (92.27s)\tAverage loss=0.477193, Current loss=0.643660\n",
      "Epoch 5 (92.30s)\tAverage loss=0.479385, Current loss=0.628479\n",
      "Epoch 5 (92.34s)\tAverage loss=0.498905, Current loss=1.845771\n",
      "Epoch 5 (92.38s)\tAverage loss=0.526149, Current loss=2.433245\n",
      "Epoch 5 (92.43s)\tAverage loss=0.525267, Current loss=0.462639\n",
      "Epoch 5 (92.46s)\tAverage loss=0.519791, Current loss=0.125485\n",
      "Epoch 5 (92.47s)\tAverage loss=0.523046, Current loss=0.760684\n",
      "Epoch 5 (92.50s)\tAverage loss=0.517802, Current loss=0.129727\n",
      "Epoch 5 (92.53s)\tAverage loss=0.519411, Current loss=0.640112\n",
      "Epoch 5 (92.56s)\tAverage loss=0.514589, Current loss=0.148120\n",
      "Epoch 5 (92.58s)\tAverage loss=0.522992, Current loss=1.169988\n",
      "Epoch 5 (92.60s)\tAverage loss=0.517690, Current loss=0.104183\n",
      "Epoch 5 (92.64s)\tAverage loss=0.514725, Current loss=0.280469\n",
      "Epoch 5 (92.67s)\tAverage loss=0.514583, Current loss=0.503237\n",
      "Epoch 5 (92.69s)\tAverage loss=0.509831, Current loss=0.124871\n",
      "Epoch 5 (92.70s)\tAverage loss=0.511481, Current loss=0.646767\n",
      "Epoch 5 (92.72s)\tAverage loss=0.514495, Current loss=0.764652\n",
      "Epoch 5 (92.77s)\tAverage loss=0.509281, Current loss=0.071305\n",
      "Epoch 5 (92.80s)\tAverage loss=0.506529, Current loss=0.272612\n",
      "Epoch 5 (92.83s)\tAverage loss=0.521474, Current loss=1.806808\n",
      "Epoch 5 (92.87s)\tAverage loss=0.533369, Current loss=1.568234\n",
      "Epoch 5 (92.94s)\tAverage loss=0.528317, Current loss=0.083719\n",
      "Epoch 5 (93.02s)\tAverage loss=0.523381, Current loss=0.084042\n",
      "Epoch 5 (93.06s)\tAverage loss=0.521132, Current loss=0.318739\n",
      "Epoch 5 (93.08s)\tAverage loss=0.523097, Current loss=0.701971\n",
      "Epoch 5 (93.10s)\tAverage loss=0.524063, Current loss=0.612903\n",
      "Epoch 5 (93.13s)\tAverage loss=0.523287, Current loss=0.451085\n",
      "Epoch 5 (93.17s)\tAverage loss=0.522303, Current loss=0.429833\n",
      "Epoch 5 (93.21s)\tAverage loss=0.520972, Current loss=0.394573\n",
      "Epoch 5 (93.24s)\tAverage loss=0.518729, Current loss=0.303370\n",
      "Epoch 5 (93.26s)\tAverage loss=0.518637, Current loss=0.509673\n",
      "Epoch 5 (93.31s)\tAverage loss=0.517227, Current loss=0.379118\n",
      "Epoch 5 (93.35s)\tAverage loss=0.514052, Current loss=0.199642\n",
      "Epoch 5 (93.37s)\tAverage loss=0.517439, Current loss=0.856152\n",
      "Epoch 5 (93.40s)\tAverage loss=0.515443, Current loss=0.313834\n",
      "Epoch 5 (93.44s)\tAverage loss=0.511921, Current loss=0.152683\n",
      "Epoch 5 (93.51s)\tAverage loss=0.507767, Current loss=0.079988\n",
      "Epoch 5 (93.54s)\tAverage loss=0.515339, Current loss=1.302792\n",
      "Epoch 5 (93.60s)\tAverage loss=0.511118, Current loss=0.067897\n",
      "Epoch 5 (93.63s)\tAverage loss=0.509466, Current loss=0.334327\n",
      "Epoch 5 (93.71s)\tAverage loss=0.505361, Current loss=0.066152\n",
      "Epoch 5 (93.73s)\tAverage loss=0.502530, Current loss=0.196797\n",
      "Epoch 5 (93.78s)\tAverage loss=0.499837, Current loss=0.206243\n",
      "Epoch 5 (93.81s)\tAverage loss=0.498533, Current loss=0.355145\n",
      "Epoch 5 (93.84s)\tAverage loss=0.497165, Current loss=0.345340\n",
      "Epoch 5 (93.91s)\tAverage loss=0.493124, Current loss=0.040545\n",
      "Epoch 5 (93.93s)\tAverage loss=0.491561, Current loss=0.314885\n",
      "Epoch 5 (93.96s)\tAverage loss=0.488625, Current loss=0.153898\n",
      "Epoch 5 (93.98s)\tAverage loss=0.485338, Current loss=0.107422\n",
      "Epoch 5 (94.19s)\tAverage loss=0.482353, Current loss=0.136040\n",
      "Epoch 5 (94.22s)\tAverage loss=0.479681, Current loss=0.167038\n",
      "Epoch 5 (94.26s)\tAverage loss=0.476568, Current loss=0.109205\n",
      "Epoch 5 (94.30s)\tAverage loss=0.496835, Current loss=2.908604\n",
      "Epoch 5 (94.33s)\tAverage loss=0.493799, Current loss=0.129564\n",
      "Epoch 5 (94.36s)\tAverage loss=0.490738, Current loss=0.120290\n",
      "Epoch 5 (94.40s)\tAverage loss=0.503771, Current loss=2.093863\n",
      "Epoch 5 (94.42s)\tAverage loss=0.512491, Current loss=1.585060\n",
      "Epoch 5 (94.43s)\tAverage loss=0.509421, Current loss=0.128654\n",
      "Epoch 5 (94.46s)\tAverage loss=0.507336, Current loss=0.246754\n",
      "Epoch 5 (94.49s)\tAverage loss=0.505004, Current loss=0.211179\n",
      "Epoch 5 (94.53s)\tAverage loss=0.514528, Current loss=1.724089\n",
      "Epoch 5 (94.59s)\tAverage loss=0.510699, Current loss=0.020569\n",
      "Epoch 5 (94.64s)\tAverage loss=0.507925, Current loss=0.150041\n",
      "Epoch 5 (94.71s)\tAverage loss=0.504203, Current loss=0.020343\n",
      "Epoch 5 (94.75s)\tAverage loss=0.510975, Current loss=1.398117\n",
      "Epoch 5 (94.79s)\tAverage loss=0.508408, Current loss=0.169622\n",
      "Epoch 5 (94.82s)\tAverage loss=0.508022, Current loss=0.456609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (94.86s)\tAverage loss=0.506823, Current loss=0.346193\n",
      "Epoch 5 (94.89s)\tAverage loss=0.506554, Current loss=0.470183\n",
      "Epoch 5 (94.91s)\tAverage loss=0.504231, Current loss=0.188431\n",
      "Epoch 5 (94.93s)\tAverage loss=0.501956, Current loss=0.190265\n",
      "Epoch 5 (94.95s)\tAverage loss=0.498933, Current loss=0.081700\n",
      "Epoch 5 (94.98s)\tAverage loss=0.497448, Current loss=0.291093\n",
      "Epoch 5 (95.01s)\tAverage loss=0.497090, Current loss=0.446965\n",
      "Epoch 5 (95.08s)\tAverage loss=0.493737, Current loss=0.020866\n",
      "Epoch 5 (95.12s)\tAverage loss=0.492614, Current loss=0.333193\n",
      "Epoch 5 (95.16s)\tAverage loss=0.496349, Current loss=1.030474\n",
      "Epoch 5 (95.19s)\tAverage loss=0.494112, Current loss=0.172004\n",
      "Epoch 5 (95.23s)\tAverage loss=0.492425, Current loss=0.247739\n",
      "Epoch 5 (95.28s)\tAverage loss=0.489980, Current loss=0.133061\n",
      "Epoch 5 (95.30s)\tAverage loss=0.497431, Current loss=1.592653\n",
      "Epoch 5 (95.33s)\tAverage loss=0.505355, Current loss=1.678184\n",
      "Epoch 5 (95.36s)\tAverage loss=0.503712, Current loss=0.258928\n",
      "Epoch 5 (95.38s)\tAverage loss=0.502165, Current loss=0.270016\n",
      "Epoch 5 (95.42s)\tAverage loss=0.499713, Current loss=0.129539\n",
      "Epoch 5 (95.45s)\tAverage loss=0.503573, Current loss=1.090301\n",
      "Epoch 5 (95.52s)\tAverage loss=0.500414, Current loss=0.017033\n",
      "Epoch 5 (95.56s)\tAverage loss=0.508300, Current loss=1.722757\n",
      "Epoch 5 (95.63s)\tAverage loss=0.505233, Current loss=0.029852\n",
      "Epoch 5 (95.66s)\tAverage loss=0.504017, Current loss=0.314295\n",
      "Epoch 5 (95.70s)\tAverage loss=0.501695, Current loss=0.137133\n",
      "Epoch 5 (95.74s)\tAverage loss=0.505502, Current loss=1.107076\n",
      "Epoch 5 (95.77s)\tAverage loss=0.504403, Current loss=0.329666\n",
      "Epoch 5 (95.81s)\tAverage loss=0.502153, Current loss=0.142121\n",
      "Epoch 5 (95.84s)\tAverage loss=0.500675, Current loss=0.262702\n",
      "Epoch 5 (95.86s)\tAverage loss=0.499190, Current loss=0.258728\n",
      "Epoch 5 (95.90s)\tAverage loss=0.498229, Current loss=0.341463\n",
      "Epoch 5 (95.94s)\tAverage loss=0.497806, Current loss=0.428502\n",
      "Epoch 5 (95.98s)\tAverage loss=0.496948, Current loss=0.355310\n",
      "Epoch 5 (96.02s)\tAverage loss=0.496468, Current loss=0.416852\n",
      "Epoch 5 (96.04s)\tAverage loss=0.495848, Current loss=0.392332\n",
      "Epoch 5 (96.06s)\tAverage loss=0.498838, Current loss=1.001112\n",
      "Epoch 5 (96.11s)\tAverage loss=0.496833, Current loss=0.157914\n",
      "Epoch 5 (96.34s)\tAverage loss=0.494015, Current loss=0.015038\n",
      "Epoch 5 (96.36s)\tAverage loss=0.493523, Current loss=0.409439\n",
      "Epoch 5 (96.39s)\tAverage loss=0.492033, Current loss=0.235656\n",
      "Epoch 5 (96.43s)\tAverage loss=0.489767, Current loss=0.097824\n",
      "Epoch 5 (96.50s)\tAverage loss=0.487040, Current loss=0.012537\n",
      "Epoch 5 (96.52s)\tAverage loss=0.494656, Current loss=1.827490\n",
      "Epoch 5 (96.54s)\tAverage loss=0.492641, Current loss=0.137918\n",
      "Epoch 5 (96.57s)\tAverage loss=0.491570, Current loss=0.302076\n",
      "Epoch 5 (96.59s)\tAverage loss=0.489663, Current loss=0.150125\n",
      "Epoch 5 (96.66s)\tAverage loss=0.487006, Current loss=0.011476\n",
      "Epoch 5 (96.70s)\tAverage loss=0.486175, Current loss=0.336532\n",
      "Epoch 5 (96.72s)\tAverage loss=0.485176, Current loss=0.304304\n",
      "Epoch 5 (96.74s)\tAverage loss=0.483776, Current loss=0.229141\n",
      "Epoch 5 (96.79s)\tAverage loss=0.481774, Current loss=0.115283\n",
      "Epoch 5 (96.83s)\tAverage loss=0.490276, Current loss=2.054772\n",
      "Epoch 5 (96.87s)\tAverage loss=0.488514, Current loss=0.162378\n",
      "Epoch 5 (96.90s)\tAverage loss=0.487539, Current loss=0.306293\n",
      "Epoch 5 (96.93s)\tAverage loss=0.485538, Current loss=0.111373\n",
      "Epoch 5 (96.97s)\tAverage loss=0.483798, Current loss=0.156557\n",
      "Epoch 5 (96.99s)\tAverage loss=0.483177, Current loss=0.365879\n",
      "Epoch 5 (97.03s)\tAverage loss=0.482160, Current loss=0.288899\n",
      "Epoch 5 (97.08s)\tAverage loss=0.487696, Current loss=1.545190\n",
      "Epoch 5 (97.15s)\tAverage loss=0.485224, Current loss=0.010477\n",
      "Epoch 5 (97.18s)\tAverage loss=0.483585, Current loss=0.167250\n",
      "Epoch 5 (97.22s)\tAverage loss=0.481774, Current loss=0.130538\n",
      "Epoch 5 (97.24s)\tAverage loss=0.480181, Current loss=0.169554\n",
      "Epoch 5 (97.26s)\tAverage loss=0.485671, Current loss=1.561595\n",
      "Epoch 5 (97.30s)\tAverage loss=0.484433, Current loss=0.240527\n",
      "Epoch 5 (97.31s)\tAverage loss=0.482794, Current loss=0.158324\n",
      "Epoch 5 (97.38s)\tAverage loss=0.480424, Current loss=0.008904\n",
      "Epoch 5 (97.42s)\tAverage loss=0.478570, Current loss=0.107589\n",
      "Epoch 5 (97.47s)\tAverage loss=0.476706, Current loss=0.102084\n",
      "Epoch 5 (97.54s)\tAverage loss=0.474400, Current loss=0.008607\n",
      "Epoch 5 (97.62s)\tAverage loss=0.472128, Current loss=0.010875\n",
      "Epoch 5 (97.65s)\tAverage loss=0.471210, Current loss=0.283970\n",
      "Epoch 5 (97.68s)\tAverage loss=0.470480, Current loss=0.320879\n",
      "Epoch 5 (97.69s)\tAverage loss=0.476659, Current loss=1.749494\n",
      "Epoch 5 (97.72s)\tAverage loss=0.475334, Current loss=0.200998\n",
      "Epoch 5 (97.76s)\tAverage loss=0.474358, Current loss=0.271413\n",
      "Epoch 5 (97.79s)\tAverage loss=0.473071, Current loss=0.204157\n",
      "Epoch 5 (97.82s)\tAverage loss=0.472283, Current loss=0.306875\n",
      "Epoch 5 (97.87s)\tAverage loss=0.471017, Current loss=0.203830\n",
      "Epoch 5 (97.94s)\tAverage loss=0.468842, Current loss=0.007767\n",
      "Epoch 5 (97.97s)\tAverage loss=0.467223, Current loss=0.122244\n",
      "Epoch 5 (97.99s)\tAverage loss=0.466400, Current loss=0.290436\n",
      "Epoch 5 (98.01s)\tAverage loss=0.464953, Current loss=0.153653\n",
      "Epoch 5 (98.08s)\tAverage loss=0.462843, Current loss=0.007253\n",
      "Epoch 5 (98.12s)\tAverage loss=0.461534, Current loss=0.177399\n",
      "Epoch 5 (98.15s)\tAverage loss=0.459858, Current loss=0.094452\n",
      "Epoch 5 (98.19s)\tAverage loss=0.465752, Current loss=1.756524\n",
      "Epoch 5 (98.23s)\tAverage loss=0.463917, Current loss=0.060253\n",
      "Epoch 5 (98.25s)\tAverage loss=0.468548, Current loss=1.492048\n",
      "Epoch 5 (98.29s)\tAverage loss=0.467050, Current loss=0.134524\n",
      "Epoch 5 (98.51s)\tAverage loss=0.477835, Current loss=2.882794\n",
      "Epoch 5 (98.54s)\tAverage loss=0.476607, Current loss=0.201663\n",
      "Epoch 5 (98.57s)\tAverage loss=0.474870, Current loss=0.083964\n",
      "Epoch 5 (98.60s)\tAverage loss=0.474029, Current loss=0.284034\n",
      "Epoch 5 (98.62s)\tAverage loss=0.478600, Current loss=1.516104\n",
      "Epoch 5 (98.68s)\tAverage loss=0.476560, Current loss=0.011466\n",
      "Epoch 5 (98.71s)\tAverage loss=0.475110, Current loss=0.143084\n",
      "Epoch 5 (98.73s)\tAverage loss=0.473593, Current loss=0.124690\n",
      "Epoch 5 (98.78s)\tAverage loss=0.472136, Current loss=0.135448\n",
      "Epoch 5 (98.80s)\tAverage loss=0.471451, Current loss=0.312678\n",
      "Epoch 5 (98.84s)\tAverage loss=0.469968, Current loss=0.124329\n",
      "Epoch 5 (98.88s)\tAverage loss=0.476639, Current loss=2.037673\n",
      "Epoch 5 (98.95s)\tAverage loss=0.474729, Current loss=0.026038\n",
      "Epoch 5 (98.99s)\tAverage loss=0.473599, Current loss=0.206749\n",
      "Epoch 5 (99.06s)\tAverage loss=0.471674, Current loss=0.015484\n",
      "Epoch 5 (99.09s)\tAverage loss=0.470752, Current loss=0.251355\n",
      "Epoch 5 (99.12s)\tAverage loss=0.469826, Current loss=0.248601\n",
      "Epoch 5 (99.14s)\tAverage loss=0.468581, Current loss=0.169580\n",
      "Epoch 5 (99.17s)\tAverage loss=0.467386, Current loss=0.179513\n",
      "Epoch 5 (99.20s)\tAverage loss=0.466208, Current loss=0.181207\n",
      "Epoch 5 (99.22s)\tAverage loss=0.471262, Current loss=1.699255\n",
      "Epoch 5 (99.29s)\tAverage loss=0.469384, Current loss=0.011053\n",
      "Epoch 5 (99.32s)\tAverage loss=0.468078, Current loss=0.148219\n",
      "Epoch 5 (99.36s)\tAverage loss=0.466721, Current loss=0.132880\n",
      "Epoch 5 (99.38s)\tAverage loss=0.465326, Current loss=0.120835\n",
      "Epoch 5 (99.40s)\tAverage loss=0.470729, Current loss=1.810587\n",
      "Epoch 5 (99.44s)\tAverage loss=0.474879, Current loss=1.508268\n",
      "Epoch 5 (99.47s)\tAverage loss=0.473885, Current loss=0.225450\n",
      "Epoch 5 (99.50s)\tAverage loss=0.476524, Current loss=1.138785\n",
      "Epoch 5 (99.53s)\tAverage loss=0.475644, Current loss=0.253903\n",
      "Epoch 5 (99.57s)\tAverage loss=0.474566, Current loss=0.201939\n",
      "Epoch 5 (99.61s)\tAverage loss=0.474162, Current loss=0.371593\n",
      "Epoch 5 (99.65s)\tAverage loss=0.473072, Current loss=0.195047\n",
      "Epoch 5 (99.68s)\tAverage loss=0.475587, Current loss=1.119284\n",
      "Epoch 5 (99.72s)\tAverage loss=0.474969, Current loss=0.316125\n",
      "Epoch 5 (99.77s)\tAverage loss=0.473220, Current loss=0.022079\n",
      "Epoch 5 (99.80s)\tAverage loss=0.472996, Current loss=0.414959\n",
      "Epoch 5 (99.87s)\tAverage loss=0.471308, Current loss=0.032429\n",
      "Epoch 5 (99.92s)\tAverage loss=0.470123, Current loss=0.160785\n",
      "Epoch 5 (99.98s)\tAverage loss=0.468438, Current loss=0.027159\n",
      "Epoch 5 (100.01s)\tAverage loss=0.470740, Current loss=1.075953\n",
      "Epoch 5 (100.09s)\tAverage loss=0.469064, Current loss=0.026837\n",
      "Epoch 5 (100.16s)\tAverage loss=0.467401, Current loss=0.026595\n",
      "Epoch 5 (100.20s)\tAverage loss=0.467373, Current loss=0.459994\n",
      "Epoch 5 (100.23s)\tAverage loss=0.467324, Current loss=0.454191\n",
      "Epoch 5 (100.26s)\tAverage loss=0.466319, Current loss=0.196907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (100.31s)\tAverage loss=0.465981, Current loss=0.374993\n",
      "Epoch 5 (100.38s)\tAverage loss=0.464347, Current loss=0.023282\n",
      "Epoch 5 (100.43s)\tAverage loss=0.463080, Current loss=0.119810\n",
      "Epoch 5 (100.67s)\tAverage loss=0.461462, Current loss=0.021291\n",
      "Epoch 5 (100.72s)\tAverage loss=0.460413, Current loss=0.173950\n",
      "Epoch 5 (100.75s)\tAverage loss=0.459725, Current loss=0.271403\n",
      "Epoch 5 (100.80s)\tAverage loss=0.462835, Current loss=1.318012\n",
      "Epoch 5 (100.83s)\tAverage loss=0.462503, Current loss=0.370730\n",
      "Epoch 5 (100.86s)\tAverage loss=0.464780, Current loss=1.095531\n",
      "Epoch 5 (100.88s)\tAverage loss=0.463721, Current loss=0.169293\n",
      "Epoch 5 (100.92s)\tAverage loss=0.462544, Current loss=0.134362\n",
      "Epoch 5 (100.94s)\tAverage loss=0.465069, Current loss=1.171866\n",
      "Epoch 5 (100.97s)\tAverage loss=0.464917, Current loss=0.422165\n",
      "Epoch 5 (101.00s)\tAverage loss=0.464788, Current loss=0.428577\n",
      "Epoch 5 (101.04s)\tAverage loss=0.464653, Current loss=0.426316\n",
      "Epoch 5 (101.06s)\tAverage loss=0.468874, Current loss=1.667655\n",
      "Epoch 5 (101.13s)\tAverage loss=0.467268, Current loss=0.009572\n",
      "Epoch 5 (101.17s)\tAverage loss=0.467072, Current loss=0.411099\n",
      "Epoch 5 (101.20s)\tAverage loss=0.469179, Current loss=1.073979\n",
      "Epoch 5 (101.22s)\tAverage loss=0.467917, Current loss=0.104318\n",
      "Epoch 5 (101.26s)\tAverage loss=0.467781, Current loss=0.428538\n",
      "Epoch 5 (101.33s)\tAverage loss=0.466221, Current loss=0.013791\n",
      "Epoch 5 (101.35s)\tAverage loss=0.465863, Current loss=0.361815\n",
      "Epoch 5 (101.38s)\tAverage loss=0.465745, Current loss=0.431014\n",
      "Epoch 5 (101.40s)\tAverage loss=0.464532, Current loss=0.109209\n",
      "Epoch 5 (101.44s)\tAverage loss=0.463941, Current loss=0.290234\n",
      "Epoch 5 (101.48s)\tAverage loss=0.463523, Current loss=0.340152\n",
      "Epoch 5 (101.51s)\tAverage loss=0.463018, Current loss=0.313736\n",
      "Epoch 5 (101.53s)\tAverage loss=0.462063, Current loss=0.178183\n",
      "Epoch 5 (101.55s)\tAverage loss=0.461844, Current loss=0.396755\n",
      "Epoch 5 (101.59s)\tAverage loss=0.461045, Current loss=0.222079\n",
      "Epoch 5 (101.61s)\tAverage loss=0.459989, Current loss=0.143261\n",
      "Epoch 5 (101.63s)\tAverage loss=0.462826, Current loss=1.316749\n",
      "Epoch 5 (101.66s)\tAverage loss=0.462487, Current loss=0.360080\n",
      "Epoch 5 (101.69s)\tAverage loss=0.466881, Current loss=1.798363\n",
      "Epoch 5 (101.73s)\tAverage loss=0.466533, Current loss=0.360655\n",
      "Epoch 5 (101.81s)\tAverage loss=0.465063, Current loss=0.016680\n",
      "Epoch 5 (101.88s)\tAverage loss=0.463585, Current loss=0.011199\n",
      "Epoch 5 (101.91s)\tAverage loss=0.463089, Current loss=0.310876\n",
      "Epoch 5 (101.94s)\tAverage loss=0.462285, Current loss=0.214688\n",
      "Epoch 5 (101.99s)\tAverage loss=0.461869, Current loss=0.333310\n",
      "Epoch 5 (102.07s)\tAverage loss=0.460444, Current loss=0.018612\n",
      "Epoch 5 (102.09s)\tAverage loss=0.463659, Current loss=1.463709\n",
      "Epoch 5 (102.13s)\tAverage loss=0.462610, Current loss=0.135300\n",
      "Epoch 5 (102.16s)\tAverage loss=0.461517, Current loss=0.119317\n",
      "Epoch 5 (102.20s)\tAverage loss=0.460911, Current loss=0.270701\n",
      "Epoch 5 (102.22s)\tAverage loss=0.459709, Current loss=0.081098\n",
      "Epoch 5 (102.26s)\tAverage loss=0.458512, Current loss=0.080213\n",
      "Epoch 5 (102.30s)\tAverage loss=0.457229, Current loss=0.050547\n",
      "Epoch 5 (102.33s)\tAverage loss=0.456862, Current loss=0.340046\n",
      "Epoch 5 (102.38s)\tAverage loss=0.456236, Current loss=0.256538\n",
      "Epoch 5 (102.40s)\tAverage loss=0.459620, Current loss=1.542716\n",
      "Epoch 5 (102.43s)\tAverage loss=0.458594, Current loss=0.129070\n",
      "Epoch 5 (102.46s)\tAverage loss=0.458176, Current loss=0.323779\n",
      "Epoch 5 (102.54s)\tAverage loss=0.456815, Current loss=0.017214\n",
      "Epoch 5 (102.57s)\tAverage loss=0.455691, Current loss=0.091304\n",
      "Epoch 5 (102.60s)\tAverage loss=0.454592, Current loss=0.097503\n",
      "Epoch 5 (102.63s)\tAverage loss=0.453683, Current loss=0.157397\n",
      "Epoch 5 (102.84s)\tAverage loss=0.452436, Current loss=0.044450\n",
      "Epoch 5 (102.86s)\tAverage loss=0.451184, Current loss=0.040827\n",
      "Epoch 5 (102.89s)\tAverage loss=0.450554, Current loss=0.243243\n",
      "Epoch 5 (102.96s)\tAverage loss=0.449235, Current loss=0.013805\n",
      "Epoch 5 (102.99s)\tAverage loss=0.448579, Current loss=0.231575\n",
      "Epoch 5 (103.01s)\tAverage loss=0.451517, Current loss=1.426989\n",
      "Epoch 5 (103.04s)\tAverage loss=0.450268, Current loss=0.034083\n",
      "Epoch 5 (103.07s)\tAverage loss=0.449698, Current loss=0.259593\n",
      "Epoch 5 (103.09s)\tAverage loss=0.448427, Current loss=0.022430\n",
      "Epoch 5 (103.11s)\tAverage loss=0.451270, Current loss=1.406425\n",
      "Epoch 5 (103.15s)\tAverage loss=0.450170, Current loss=0.079652\n",
      "Epoch 5 (103.20s)\tAverage loss=0.448912, Current loss=0.023669\n",
      "Epoch 5 (103.22s)\tAverage loss=0.448131, Current loss=0.183311\n",
      "Epoch 5 (103.25s)\tAverage loss=0.447017, Current loss=0.068498\n",
      "Epoch 5 (103.30s)\tAverage loss=0.446131, Current loss=0.143987\n",
      "Epoch 5 (103.33s)\tAverage loss=0.445212, Current loss=0.130625\n",
      "Epoch 5 (103.36s)\tAverage loss=0.444245, Current loss=0.112861\n",
      "Epoch 5 (103.39s)\tAverage loss=0.443120, Current loss=0.055860\n",
      "Epoch 5 (103.41s)\tAverage loss=0.448015, Current loss=2.136806\n",
      "Epoch 5 (103.44s)\tAverage loss=0.447571, Current loss=0.294197\n",
      "Epoch 5 (103.49s)\tAverage loss=0.446466, Current loss=0.062994\n",
      "Epoch 5 (103.51s)\tAverage loss=0.448975, Current loss=1.322136\n",
      "Epoch 5 (103.55s)\tAverage loss=0.448585, Current loss=0.312440\n",
      "Epoch 5 (103.58s)\tAverage loss=0.448190, Current loss=0.309959\n",
      "Epoch 5 (103.65s)\tAverage loss=0.462213, Current loss=5.384040\n",
      "Epoch 5 (103.72s)\tAverage loss=0.460920, Current loss=0.005993\n",
      "Epoch 5 (103.74s)\tAverage loss=0.460538, Current loss=0.325701\n",
      "Epoch 5 (103.81s)\tAverage loss=0.459294, Current loss=0.018716\n",
      "Epoch 5 (103.84s)\tAverage loss=0.461950, Current loss=1.404802\n",
      "Epoch 5 (103.88s)\tAverage loss=0.464220, Current loss=1.272361\n",
      "Epoch 5 (103.95s)\tAverage loss=0.462958, Current loss=0.012357\n",
      "Epoch 5 (103.98s)\tAverage loss=0.465003, Current loss=1.197451\n",
      "Epoch 5 (104.00s)\tAverage loss=0.464704, Current loss=0.357353\n",
      "Epoch 5 (104.04s)\tAverage loss=0.463854, Current loss=0.157565\n",
      "Epoch 5 (104.07s)\tAverage loss=0.463642, Current loss=0.387265\n",
      "Epoch 5 (104.10s)\tAverage loss=0.465440, Current loss=1.116422\n",
      "Epoch 5 (104.11s)\tAverage loss=0.465157, Current loss=0.362407\n",
      "Epoch 5 (104.14s)\tAverage loss=0.464447, Current loss=0.206038\n",
      "Epoch 5 (104.19s)\tAverage loss=0.463616, Current loss=0.159981\n",
      "Epoch 5 (104.22s)\tAverage loss=0.462916, Current loss=0.206778\n",
      "Epoch 5 (104.24s)\tAverage loss=0.462607, Current loss=0.349308\n",
      "Epoch 5 (104.27s)\tAverage loss=0.465727, Current loss=1.613921\n",
      "Epoch 5 (104.34s)\tAverage loss=0.464567, Current loss=0.036587\n",
      "Epoch 5 (104.36s)\tAverage loss=0.463506, Current loss=0.071022\n",
      "Epoch 5 (104.41s)\tAverage loss=0.463126, Current loss=0.321971\n",
      "Epoch 5 (104.46s)\tAverage loss=0.462869, Current loss=0.367190\n",
      "Epoch 5 (104.50s)\tAverage loss=0.462631, Current loss=0.373884\n",
      "Epoch 5 (104.52s)\tAverage loss=0.464226, Current loss=1.060941\n",
      "Epoch 5 (104.56s)\tAverage loss=0.464106, Current loss=0.418794\n",
      "Epoch 5 (104.61s)\tAverage loss=0.462952, Current loss=0.029385\n",
      "Epoch 5 (104.62s)\tAverage loss=0.462786, Current loss=0.400119\n",
      "Epoch 5 (104.66s)\tAverage loss=0.461876, Current loss=0.117656\n",
      "Epoch 5 (104.69s)\tAverage loss=0.461064, Current loss=0.153448\n",
      "Epoch 5 (104.71s)\tAverage loss=0.460441, Current loss=0.223684\n",
      "Epoch 5 (104.75s)\tAverage loss=0.459474, Current loss=0.090947\n",
      "Epoch 5 (104.79s)\tAverage loss=0.459165, Current loss=0.341094\n",
      "Epoch 5 (105.00s)\tAverage loss=0.460882, Current loss=1.118582\n",
      "Epoch 5 (105.03s)\tAverage loss=0.460018, Current loss=0.128418\n",
      "Epoch 5 (105.08s)\tAverage loss=0.459140, Current loss=0.121209\n",
      "Epoch 5 (105.12s)\tAverage loss=0.458084, Current loss=0.050299\n",
      "Epoch 5 (105.17s)\tAverage loss=0.457373, Current loss=0.182037\n",
      "Epoch 5 (105.19s)\tAverage loss=0.461500, Current loss=2.062834\n",
      "Epoch 5 (105.24s)\tAverage loss=0.468128, Current loss=3.046632\n",
      "Epoch 5 (105.26s)\tAverage loss=0.469794, Current loss=1.119352\n",
      "Epoch 5 (105.30s)\tAverage loss=0.472338, Current loss=1.467044\n",
      "Epoch 5 (105.33s)\tAverage loss=0.472261, Current loss=0.442290\n",
      "Epoch 5 (105.36s)\tAverage loss=0.471283, Current loss=0.086703\n",
      "Epoch 5 (105.38s)\tAverage loss=0.470693, Current loss=0.238171\n",
      "Epoch 5 (105.43s)\tAverage loss=0.474032, Current loss=1.793201\n",
      "Epoch 5 (105.46s)\tAverage loss=0.473119, Current loss=0.111479\n",
      "Epoch 5 (105.48s)\tAverage loss=0.472293, Current loss=0.144289\n",
      "Epoch 5 (105.51s)\tAverage loss=0.471651, Current loss=0.216069\n",
      "Epoch 5 (105.54s)\tAverage loss=0.471710, Current loss=0.495214\n",
      "Epoch 5 (105.56s)\tAverage loss=0.471112, Current loss=0.232052\n",
      "Epoch 5 (105.58s)\tAverage loss=0.473807, Current loss=1.554669\n",
      "Epoch 5 (105.61s)\tAverage loss=0.475046, Current loss=0.972962\n",
      "Epoch 5 (105.64s)\tAverage loss=0.474271, Current loss=0.162066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (105.68s)\tAverage loss=0.473444, Current loss=0.139244\n",
      "Epoch 5 (105.70s)\tAverage loss=0.472830, Current loss=0.224164\n",
      "Epoch 5 (105.75s)\tAverage loss=0.472484, Current loss=0.331927\n",
      "Epoch 5 (105.77s)\tAverage loss=0.472491, Current loss=0.475414\n",
      "Epoch 5 (105.81s)\tAverage loss=0.472558, Current loss=0.499915\n",
      "Epoch 5 (105.86s)\tAverage loss=0.472223, Current loss=0.335145\n",
      "Epoch 5 (105.93s)\tAverage loss=0.471284, Current loss=0.086362\n",
      "Epoch 5 (105.96s)\tAverage loss=0.470502, Current loss=0.149187\n",
      "Epoch 5 (105.99s)\tAverage loss=0.471712, Current loss=0.970038\n",
      "Epoch 5 (106.04s)\tAverage loss=0.470914, Current loss=0.141527\n",
      "Epoch 5 (106.08s)\tAverage loss=0.470969, Current loss=0.493746\n",
      "Epoch 5 (106.10s)\tAverage loss=0.474284, Current loss=1.849958\n",
      "Epoch 5 (106.18s)\tAverage loss=0.473331, Current loss=0.076814\n",
      "Epoch 5 (106.25s)\tAverage loss=0.472283, Current loss=0.035369\n",
      "Epoch 5 (106.27s)\tAverage loss=0.471650, Current loss=0.206870\n",
      "Epoch 5 (106.32s)\tAverage loss=0.471165, Current loss=0.267949\n",
      "Epoch 5 (106.35s)\tAverage loss=0.470401, Current loss=0.149742\n",
      "Epoch 5 (106.38s)\tAverage loss=0.470280, Current loss=0.419179\n",
      "Epoch 5 (106.41s)\tAverage loss=0.470232, Current loss=0.450048\n",
      "Epoch 5 (106.44s)\tAverage loss=0.471345, Current loss=0.942257\n",
      "Epoch 5 (106.48s)\tAverage loss=0.471358, Current loss=0.476646\n",
      "Epoch 5 (106.51s)\tAverage loss=0.470818, Current loss=0.241321\n",
      "Epoch 5 (106.56s)\tAverage loss=0.470212, Current loss=0.212163\n",
      "Epoch 5 (106.58s)\tAverage loss=0.469837, Current loss=0.309783\n",
      "Epoch 5 (106.61s)\tAverage loss=0.469704, Current loss=0.412583\n",
      "Epoch 5 (106.63s)\tAverage loss=0.469464, Current loss=0.366611\n",
      "Epoch 5 (106.67s)\tAverage loss=0.468812, Current loss=0.188317\n",
      "Epoch 5 (106.71s)\tAverage loss=0.468562, Current loss=0.360809\n",
      "Epoch 5 (106.74s)\tAverage loss=0.467933, Current loss=0.196507\n",
      "Epoch 5 (106.77s)\tAverage loss=0.469277, Current loss=1.051030\n",
      "Epoch 5 (106.84s)\tAverage loss=0.468272, Current loss=0.032334\n",
      "Epoch 5 (106.86s)\tAverage loss=0.467775, Current loss=0.251298\n",
      "Epoch 5 (106.90s)\tAverage loss=0.467638, Current loss=0.407914\n",
      "Epoch 5 (106.94s)\tAverage loss=0.466861, Current loss=0.127571\n",
      "Epoch 5 (107.01s)\tAverage loss=0.466761, Current loss=0.422649\n",
      "Epoch 5 (107.19s)\tAverage loss=0.465986, Current loss=0.125978\n",
      "Epoch 5 (107.25s)\tAverage loss=0.464988, Current loss=0.025735\n",
      "Epoch 5 (107.28s)\tAverage loss=0.464415, Current loss=0.211574\n",
      "Epoch 5 (107.30s)\tAverage loss=0.466403, Current loss=1.345404\n",
      "Epoch 5 (107.33s)\tAverage loss=0.465636, Current loss=0.125894\n",
      "Epoch 5 (107.35s)\tAverage loss=0.465249, Current loss=0.293243\n",
      "Epoch 5 (107.40s)\tAverage loss=0.464450, Current loss=0.108735\n",
      "Epoch 5 (107.43s)\tAverage loss=0.466712, Current loss=1.475734\n",
      "Epoch 5 (107.45s)\tAverage loss=0.465908, Current loss=0.106656\n",
      "Epoch 5 (107.47s)\tAverage loss=0.465072, Current loss=0.090499\n",
      "Epoch 5 (107.50s)\tAverage loss=0.464665, Current loss=0.281802\n",
      "Epoch 5 (107.52s)\tAverage loss=0.466375, Current loss=1.235866\n",
      "Epoch 5 (107.54s)\tAverage loss=0.465532, Current loss=0.085575\n",
      "Epoch 5 (107.58s)\tAverage loss=0.464672, Current loss=0.075626\n",
      "Epoch 5 (107.62s)\tAverage loss=0.463822, Current loss=0.079076\n",
      "Epoch 5 (107.65s)\tAverage loss=0.462965, Current loss=0.073501\n",
      "Epoch 5 (107.69s)\tAverage loss=0.462118, Current loss=0.076890\n",
      "Epoch 5 (107.73s)\tAverage loss=0.461270, Current loss=0.074628\n",
      "Epoch 5 (107.81s)\tAverage loss=0.460302, Current loss=0.018090\n",
      "Epoch 5 (107.83s)\tAverage loss=0.462543, Current loss=1.488775\n",
      "Epoch 5 (107.86s)\tAverage loss=0.461725, Current loss=0.086281\n",
      "Epoch 5 (107.89s)\tAverage loss=0.461150, Current loss=0.196604\n",
      "Epoch 5 (107.91s)\tAverage loss=0.465979, Current loss=2.692285\n",
      "Epoch 5 (107.99s)\tAverage loss=0.465048, Current loss=0.034600\n",
      "Epoch 5 (108.01s)\tAverage loss=0.464232, Current loss=0.086697\n",
      "Epoch 5 (108.04s)\tAverage loss=0.463811, Current loss=0.268175\n",
      "Epoch 5 (108.08s)\tAverage loss=0.463358, Current loss=0.252781\n",
      "Epoch 5 (108.11s)\tAverage loss=0.462785, Current loss=0.195729\n",
      "Epoch 5 (108.15s)\tAverage loss=0.462353, Current loss=0.260720\n",
      "Epoch 5 (108.17s)\tAverage loss=0.461890, Current loss=0.245243\n",
      "Epoch 5 (108.22s)\tAverage loss=0.461092, Current loss=0.087012\n",
      "Epoch 5 (108.24s)\tAverage loss=0.460300, Current loss=0.088058\n",
      "Epoch 5 (108.28s)\tAverage loss=0.460095, Current loss=0.363615\n",
      "Epoch 5 (108.31s)\tAverage loss=0.459574, Current loss=0.213646\n",
      "Epoch 5 (108.36s)\tAverage loss=0.459234, Current loss=0.298064\n",
      "Epoch 5 (108.39s)\tAverage loss=0.458994, Current loss=0.345279\n",
      "Epoch 5 (108.43s)\tAverage loss=0.458372, Current loss=0.162875\n",
      "Epoch 5 (108.45s)\tAverage loss=0.457969, Current loss=0.266435\n",
      "Epoch 5 (108.52s)\tAverage loss=0.457168, Current loss=0.074997\n",
      "Epoch 5 (108.54s)\tAverage loss=0.458659, Current loss=1.171409\n",
      "Epoch 5 (108.59s)\tAverage loss=0.457821, Current loss=0.056319\n",
      "Epoch 5 (108.62s)\tAverage loss=0.457022, Current loss=0.073468\n",
      "Epoch 5 (108.69s)\tAverage loss=0.456108, Current loss=0.016537\n",
      "Epoch 5 (108.74s)\tAverage loss=0.455404, Current loss=0.116193\n",
      "Epoch 5 (108.79s)\tAverage loss=0.454580, Current loss=0.056556\n",
      "Epoch 5 (108.83s)\tAverage loss=0.453741, Current loss=0.047655\n",
      "Epoch 5 (108.86s)\tAverage loss=0.452907, Current loss=0.048178\n",
      "Epoch 5 (108.91s)\tAverage loss=0.458391, Current loss=3.123672\n",
      "Epoch 5 (108.93s)\tAverage loss=0.457744, Current loss=0.142644\n",
      "Epoch 5 (108.97s)\tAverage loss=0.456903, Current loss=0.046633\n",
      "Epoch 5 (108.99s)\tAverage loss=0.461326, Current loss=2.624271\n",
      "Epoch 5 (109.02s)\tAverage loss=0.460518, Current loss=0.064321\n",
      "Epoch 5 (109.04s)\tAverage loss=0.459813, Current loss=0.113776\n",
      "Epoch 5 (109.07s)\tAverage loss=0.458999, Current loss=0.058345\n",
      "Epoch 5 (109.09s)\tAverage loss=0.458208, Current loss=0.068212\n",
      "Epoch 5 (109.11s)\tAverage loss=0.457409, Current loss=0.063090\n",
      "Epoch 5 (109.33s)\tAverage loss=0.456730, Current loss=0.120439\n",
      "Epoch 5 (109.38s)\tAverage loss=0.455938, Current loss=0.063360\n",
      "Epoch 5 (109.40s)\tAverage loss=0.460118, Current loss=2.537174\n",
      "Epoch 5 (109.47s)\tAverage loss=0.459280, Current loss=0.042030\n",
      "Epoch 5 (109.51s)\tAverage loss=0.458568, Current loss=0.103345\n",
      "Epoch 5 (109.56s)\tAverage loss=0.457891, Current loss=0.119271\n",
      "Epoch 5 (109.63s)\tAverage loss=0.457070, Current loss=0.046022\n",
      "Epoch 5 (109.66s)\tAverage loss=0.456311, Current loss=0.075304\n",
      "Epoch 5 (109.71s)\tAverage loss=0.455646, Current loss=0.120972\n",
      "Epoch 5 (109.74s)\tAverage loss=0.454903, Current loss=0.080628\n",
      "Epoch 5 (109.77s)\tAverage loss=0.454219, Current loss=0.108896\n",
      "Epoch 5 (109.80s)\tAverage loss=0.453475, Current loss=0.076671\n",
      "Epoch 5 (109.84s)\tAverage loss=0.452772, Current loss=0.096557\n",
      "Epoch 5 (109.91s)\tAverage loss=0.457046, Current loss=2.627991\n",
      "Epoch 5 (109.97s)\tAverage loss=0.456248, Current loss=0.050365\n",
      "Epoch 5 (110.02s)\tAverage loss=0.455673, Current loss=0.162097\n",
      "Epoch 5 (110.06s)\tAverage loss=0.455039, Current loss=0.131202\n",
      "Epoch 5 (110.09s)\tAverage loss=0.454522, Current loss=0.189711\n",
      "Epoch 5 (110.13s)\tAverage loss=0.453873, Current loss=0.121062\n",
      "Epoch 5 (110.20s)\tAverage loss=0.453074, Current loss=0.042469\n",
      "Epoch 5 (110.27s)\tAverage loss=0.452280, Current loss=0.043328\n",
      "Epoch 5 (110.30s)\tAverage loss=0.455794, Current loss=2.269018\n",
      "Epoch 5 (110.35s)\tAverage loss=0.455129, Current loss=0.111114\n",
      "Epoch 5 (110.38s)\tAverage loss=0.458307, Current loss=2.104720\n",
      "Epoch 5 (110.41s)\tAverage loss=0.457903, Current loss=0.248380\n",
      "Epoch 5 (110.43s)\tAverage loss=0.459461, Current loss=1.269310\n",
      "Epoch 5 (110.45s)\tAverage loss=0.460947, Current loss=1.235489\n",
      "Epoch 5 (110.47s)\tAverage loss=0.462844, Current loss=1.452967\n",
      "Epoch 5 (110.50s)\tAverage loss=0.463883, Current loss=1.007147\n",
      "Epoch 5 (110.54s)\tAverage loss=0.466958, Current loss=2.078413\n",
      "Epoch 5 (110.60s)\tAverage loss=0.466264, Current loss=0.101784\n",
      "Epoch 5 (110.64s)\tAverage loss=0.466084, Current loss=0.371471\n",
      "Epoch 5 (110.67s)\tAverage loss=0.465806, Current loss=0.319163\n",
      "Epoch 5 (110.69s)\tAverage loss=0.465640, Current loss=0.378152\n",
      "Epoch 5 (110.72s)\tAverage loss=0.466358, Current loss=0.845947\n",
      "Epoch 5 (110.76s)\tAverage loss=0.466392, Current loss=0.484402\n",
      "Epoch 5 (110.83s)\tAverage loss=0.466359, Current loss=0.448840\n",
      "Epoch 5 (110.86s)\tAverage loss=0.467560, Current loss=1.106810\n",
      "Epoch 5 (110.89s)\tAverage loss=0.467396, Current loss=0.379858\n",
      "Epoch 5 (110.93s)\tAverage loss=0.467168, Current loss=0.345450\n",
      "Epoch 5 (110.98s)\tAverage loss=0.466945, Current loss=0.347778\n",
      "Epoch 5 (111.02s)\tAverage loss=0.466857, Current loss=0.419449\n",
      "Epoch 5 (111.07s)\tAverage loss=0.466848, Current loss=0.461844\n",
      "Epoch 5 (111.10s)\tAverage loss=0.468142, Current loss=1.164576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (111.15s)\tAverage loss=0.467777, Current loss=0.271191\n",
      "Epoch 5 (111.17s)\tAverage loss=0.467929, Current loss=0.549922\n",
      "Epoch 5 (111.22s)\tAverage loss=0.467547, Current loss=0.260658\n",
      "Epoch 5 (111.29s)\tAverage loss=0.466919, Current loss=0.126682\n",
      "Epoch 5 (111.50s)\tAverage loss=0.468617, Current loss=1.390670\n",
      "Epoch 5 (111.53s)\tAverage loss=0.468631, Current loss=0.475999\n",
      "Epoch 5 (111.55s)\tAverage loss=0.469623, Current loss=1.010521\n",
      "Epoch 5 (111.57s)\tAverage loss=0.469626, Current loss=0.471431\n",
      "Epoch 5 (111.59s)\tAverage loss=0.471153, Current loss=1.306054\n",
      "Epoch 5 (111.62s)\tAverage loss=0.470993, Current loss=0.383385\n",
      "Epoch 5 (111.68s)\tAverage loss=0.470623, Current loss=0.267733\n",
      "Epoch 5 (111.70s)\tAverage loss=0.470733, Current loss=0.530987\n",
      "Epoch 5 (111.78s)\tAverage loss=0.470102, Current loss=0.122706\n",
      "Epoch 5 (111.80s)\tAverage loss=0.470072, Current loss=0.453127\n",
      "Epoch 5 (111.85s)\tAverage loss=0.469763, Current loss=0.298849\n",
      "Epoch 5 (111.88s)\tAverage loss=0.469617, Current loss=0.388699\n",
      "Epoch 5 (111.94s)\tAverage loss=0.469092, Current loss=0.177938\n",
      "Epoch 5 (111.98s)\tAverage loss=0.468882, Current loss=0.352026\n",
      "Epoch 5 (112.02s)\tAverage loss=0.470585, Current loss=1.419095\n",
      "Epoch 5 (112.05s)\tAverage loss=0.471452, Current loss=0.955519\n",
      "Epoch 5 (112.09s)\tAverage loss=0.471061, Current loss=0.252265\n",
      "Epoch 5 (112.13s)\tAverage loss=0.470914, Current loss=0.388752\n",
      "Epoch 5 (112.16s)\tAverage loss=0.470606, Current loss=0.297576\n",
      "Epoch 5 (112.20s)\tAverage loss=0.470400, Current loss=0.354559\n",
      "Epoch 5 (112.23s)\tAverage loss=0.470124, Current loss=0.314880\n",
      "Epoch 5 (112.28s)\tAverage loss=0.469706, Current loss=0.233794\n",
      "Epoch 5 (112.30s)\tAverage loss=0.469363, Current loss=0.275800\n",
      "Epoch 5 (112.34s)\tAverage loss=0.470278, Current loss=0.988000\n",
      "Epoch 5 (112.37s)\tAverage loss=0.469839, Current loss=0.221135\n",
      "Epoch 5 (112.40s)\tAverage loss=0.469570, Current loss=0.316830\n",
      "Epoch 5 (112.43s)\tAverage loss=0.469218, Current loss=0.268908\n",
      "Epoch 5 (112.46s)\tAverage loss=0.470669, Current loss=1.297465\n",
      "Epoch 5 (112.49s)\tAverage loss=0.470247, Current loss=0.229612\n",
      "Epoch 5 (112.53s)\tAverage loss=0.472716, Current loss=1.885122\n",
      "Epoch 5 (112.58s)\tAverage loss=0.472359, Current loss=0.267479\n",
      "Epoch 5 (112.61s)\tAverage loss=0.472424, Current loss=0.509893\n",
      "Epoch 5 (112.64s)\tAverage loss=0.474174, Current loss=1.480146\n",
      "Epoch 5 (112.68s)\tAverage loss=0.473771, Current loss=0.241829\n",
      "Epoch 5 (112.71s)\tAverage loss=0.473568, Current loss=0.356351\n",
      "Epoch 5 (112.73s)\tAverage loss=0.474338, Current loss=0.919685\n",
      "Epoch 5 (112.80s)\tAverage loss=0.473631, Current loss=0.064150\n",
      "Epoch 5 (112.82s)\tAverage loss=0.473375, Current loss=0.324784\n",
      "Epoch 5 (112.85s)\tAverage loss=0.473053, Current loss=0.286122\n",
      "Epoch 5 (112.88s)\tAverage loss=0.472632, Current loss=0.227519\n",
      "Epoch 5 (112.90s)\tAverage loss=0.472138, Current loss=0.184414\n",
      "Epoch 5 (112.99s)\tAverage loss=0.471896, Current loss=0.330437\n",
      "Epoch 5 (113.02s)\tAverage loss=0.471988, Current loss=0.525895\n",
      "Epoch 5 (113.05s)\tAverage loss=0.471539, Current loss=0.208091\n",
      "Epoch 5 (113.08s)\tAverage loss=0.471275, Current loss=0.316570\n",
      "Epoch 5 (113.11s)\tAverage loss=0.470904, Current loss=0.252461\n",
      "Epoch 5 (113.14s)\tAverage loss=0.470528, Current loss=0.249232\n",
      "Epoch 5 (113.21s)\tAverage loss=0.469820, Current loss=0.051838\n",
      "Epoch 5 (113.24s)\tAverage loss=0.469474, Current loss=0.265083\n",
      "Epoch 5 (113.29s)\tAverage loss=0.468955, Current loss=0.162069\n",
      "Epoch 5 (113.31s)\tAverage loss=0.468414, Current loss=0.147350\n",
      "Epoch 5 (113.35s)\tAverage loss=0.467915, Current loss=0.171549\n",
      "Epoch 5 (113.39s)\tAverage loss=0.467469, Current loss=0.201851\n",
      "Epoch 5 (113.43s)\tAverage loss=0.466990, Current loss=0.181911\n",
      "Epoch 5 (113.46s)\tAverage loss=0.466521, Current loss=0.186428\n",
      "Epoch 5 (113.67s)\tAverage loss=0.466004, Current loss=0.156889\n",
      "Epoch 5 (113.69s)\tAverage loss=0.468599, Current loss=2.022968\n",
      "Epoch 5 (113.76s)\tAverage loss=0.467876, Current loss=0.033710\n",
      "Epoch 5 (113.78s)\tAverage loss=0.467273, Current loss=0.104861\n",
      "Epoch 5 (113.85s)\tAverage loss=0.466566, Current loss=0.040950\n",
      "Epoch 5 (113.88s)\tAverage loss=0.466081, Current loss=0.173673\n",
      "Epoch 5 (113.91s)\tAverage loss=0.467980, Current loss=1.615284\n",
      "Epoch 5 (113.95s)\tAverage loss=0.467431, Current loss=0.135382\n",
      "Epoch 5 (113.98s)\tAverage loss=0.467377, Current loss=0.434213\n",
      "Epoch 5 (114.02s)\tAverage loss=0.466813, Current loss=0.124707\n",
      "Epoch 5 (114.09s)\tAverage loss=0.466098, Current loss=0.031658\n",
      "Epoch 5 (114.12s)\tAverage loss=0.468724, Current loss=2.067693\n",
      "Epoch 5 (114.19s)\tAverage loss=0.468054, Current loss=0.059553\n",
      "Epoch 5 (114.24s)\tAverage loss=0.467511, Current loss=0.135699\n",
      "Epoch 5 (114.27s)\tAverage loss=0.467020, Current loss=0.166557\n",
      "Epoch 5 (114.30s)\tAverage loss=0.466614, Current loss=0.217694\n",
      "Epoch 5 (114.33s)\tAverage loss=0.469015, Current loss=1.943270\n",
      "Epoch 5 (114.35s)\tAverage loss=0.468557, Current loss=0.186504\n",
      "Epoch 5 (114.38s)\tAverage loss=0.468095, Current loss=0.183600\n",
      "Epoch 5 (114.40s)\tAverage loss=0.467898, Current loss=0.346606\n",
      "Epoch 5 (114.42s)\tAverage loss=0.467374, Current loss=0.143605\n",
      "Epoch 5 (114.45s)\tAverage loss=0.466793, Current loss=0.107284\n",
      "Epoch 5 (114.50s)\tAverage loss=0.466349, Current loss=0.191052\n",
      "Epoch 5 (114.54s)\tAverage loss=0.465832, Current loss=0.144516\n",
      "Epoch 5 (114.58s)\tAverage loss=0.465330, Current loss=0.153327\n",
      "Epoch 5 (114.63s)\tAverage loss=0.467829, Current loss=2.024643\n",
      "Epoch 5 (114.66s)\tAverage loss=0.467517, Current loss=0.272406\n",
      "Epoch 5 (114.68s)\tAverage loss=0.467132, Current loss=0.226946\n",
      "Epoch 5 (114.71s)\tAverage loss=0.466594, Current loss=0.129620\n",
      "Epoch 5 (114.76s)\tAverage loss=0.466078, Current loss=0.142286\n",
      "Epoch 5 (114.80s)\tAverage loss=0.465561, Current loss=0.140918\n",
      "Epoch 5 (114.85s)\tAverage loss=0.465078, Current loss=0.161391\n",
      "Epoch 5 (114.88s)\tAverage loss=0.464480, Current loss=0.087945\n",
      "Epoch 5 (114.90s)\tAverage loss=0.465479, Current loss=1.096038\n",
      "Epoch 5 (114.93s)\tAverage loss=0.466441, Current loss=1.074044\n",
      "Epoch 5 (114.97s)\tAverage loss=0.466050, Current loss=0.218757\n",
      "Epoch 5 (114.99s)\tAverage loss=0.467026, Current loss=1.085883\n",
      "Epoch 5 (115.01s)\tAverage loss=0.466570, Current loss=0.176752\n",
      "Epoch 5 (115.05s)\tAverage loss=0.467373, Current loss=0.977923\n",
      "Epoch 5 (115.09s)\tAverage loss=0.466846, Current loss=0.131209\n",
      "Epoch 5 (115.12s)\tAverage loss=0.467588, Current loss=0.941247\n",
      "Epoch 5 (115.14s)\tAverage loss=0.467154, Current loss=0.189720\n",
      "Epoch 5 (115.16s)\tAverage loss=0.466909, Current loss=0.310191\n",
      "Epoch 5 (115.21s)\tAverage loss=0.466457, Current loss=0.176825\n",
      "Epoch 5 (115.25s)\tAverage loss=0.465981, Current loss=0.159929\n",
      "Epoch 5 (115.32s)\tAverage loss=0.469727, Current loss=2.878600\n",
      "Epoch 5 (115.35s)\tAverage loss=0.469146, Current loss=0.095209\n",
      "Epoch 5 (115.40s)\tAverage loss=0.468625, Current loss=0.132588\n",
      "Epoch 5 (115.44s)\tAverage loss=0.468210, Current loss=0.200066\n",
      "Epoch 5 (115.46s)\tAverage loss=0.470225, Current loss=1.773621\n",
      "Epoch 5 (115.51s)\tAverage loss=0.469786, Current loss=0.185581\n",
      "Epoch 5 (115.53s)\tAverage loss=0.471819, Current loss=1.791253\n",
      "Epoch 5 (115.57s)\tAverage loss=0.471273, Current loss=0.115944\n",
      "Epoch 5 (115.62s)\tAverage loss=0.470930, Current loss=0.248151\n",
      "Epoch 5 (115.83s)\tAverage loss=0.470427, Current loss=0.142427\n",
      "Epoch 5 (115.86s)\tAverage loss=0.469983, Current loss=0.180047\n",
      "Epoch 5 (115.90s)\tAverage loss=0.469458, Current loss=0.126232\n",
      "Epoch 5 (115.94s)\tAverage loss=0.468938, Current loss=0.127743\n",
      "Epoch 5 (116.01s)\tAverage loss=0.468397, Current loss=0.113466\n",
      "Epoch 5 (116.04s)\tAverage loss=0.469840, Current loss=1.418066\n",
      "Epoch 5 (116.07s)\tAverage loss=0.469492, Current loss=0.240699\n",
      "Epoch 5 (116.14s)\tAverage loss=0.468957, Current loss=0.116593\n",
      "Epoch 5 (116.22s)\tAverage loss=0.468422, Current loss=0.115031\n",
      "Epoch 5 (116.26s)\tAverage loss=0.467907, Current loss=0.127480\n",
      "Epoch 5 (116.28s)\tAverage loss=0.469631, Current loss=1.611036\n",
      "Epoch 5 (116.31s)\tAverage loss=0.471568, Current loss=1.756048\n",
      "Epoch 5 (116.34s)\tAverage loss=0.472200, Current loss=0.891507\n",
      "Epoch 5 (116.36s)\tAverage loss=0.471982, Current loss=0.326788\n",
      "Epoch 5 (116.40s)\tAverage loss=0.474284, Current loss=2.007621\n",
      "Epoch 5 (116.47s)\tAverage loss=0.473830, Current loss=0.170910\n",
      "Epoch 5 (116.51s)\tAverage loss=0.473606, Current loss=0.324173\n",
      "Epoch 5 (116.54s)\tAverage loss=0.473311, Current loss=0.276190\n",
      "Epoch 5 (116.57s)\tAverage loss=0.474277, Current loss=1.121052\n",
      "Epoch 5 (116.65s)\tAverage loss=0.473844, Current loss=0.183321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 (116.68s)\tAverage loss=0.473419, Current loss=0.188069\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "emb_lstm.run_training(X_train, Y_train, n_epochs=5, lr=0.01, batch_size=32, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def marginals(model, X, batch_size=None):\n",
    "    \"\"\"\n",
    "    Computes class probabilities for input data.\n",
    "\n",
    "    :param X: Collection of candidates to predict labels for\n",
    "    :param batch_size: Number of candidates to label at a time. Does not affect\n",
    "         predicted probabilities, but useful if all candidates do not fit on GPU.\n",
    "         If None, all candidates will be labeled at same time.\n",
    "    :return: 1-d numpy array of probabilities that each candidate has positive label\n",
    "         if class cardinality is 2. Else, a 2-d numpy array of probabilities where\n",
    "         dim. 0 corresponds to candidates and dim. 1 corresponds to class labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    marginals = model._pytorch_outputs(X, batch_size).detach()\n",
    "    return F.sigmoid(marginals).numpy() if model.cardinality == 2 else F.softmax(marginals).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eczech/anaconda3/envs/pubmed-nlp/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "Y_pred = marginals(emb_lstm, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3200f9dd8>]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fpr, tpr, _ = roc_curve(Y_train, Y_pred)\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>30830</td>\n",
       "      <td>1</td>\n",
       "      <td>The current understanding is that [[ interleukin ( IL)-6 ]] in combination with transforming growth factor-β ( TGF-β ) leads to generation of &lt;&lt; T helper-17 &gt;&gt; ( Th17 ) lineage cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>30834</td>\n",
       "      <td>1</td>\n",
       "      <td>The current understanding is that interleukin ( IL)-6 in combination with transforming growth factor-β ( [[ TGF-β ]] ) leads to generation of &lt;&lt; T helper-17 &gt;&gt; ( Th17 ) lineage cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>30832</td>\n",
       "      <td>1</td>\n",
       "      <td>The current understanding is that interleukin ( IL)-6 in combination with [[ transforming growth factor-β ]] ( TGF-β ) leads to generation of &lt;&lt; T helper-17 &gt;&gt; ( Th17 ) lineage cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>30249</td>\n",
       "      <td>1</td>\n",
       "      <td>Altogether , &lt;&lt; Th17 &gt;&gt; cells require IL23 , TGFβ , [[ IL6 ]] , and IL1 for their generation .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>30201</td>\n",
       "      <td>0</td>\n",
       "      <td>Plasticity within this subset is suggested by the existence of IL-17 secreting cells , which , can also secrete [[ interferon-γ ]] , the signature cytokine for Th1 cells or , can co - express the anti - inflammatory transcription factor forkhead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>30199</td>\n",
       "      <td>0</td>\n",
       "      <td>Plasticity within this subset is suggested by the existence of [[ IL-17 ]] secreting cells , which , can also secrete interferon-γ , the signature cytokine for Th1 cells or , can co - express the anti - inflammatory transcription factor forkhead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>30301</td>\n",
       "      <td>0</td>\n",
       "      <td>Furthermore , [[ IL-33 ]] controlled Bcl6 function at the chromatin level and consequently , augmented cytokine production in memory &lt;&lt; TH2 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>30247</td>\n",
       "      <td>1</td>\n",
       "      <td>Altogether , &lt;&lt; Th17 &gt;&gt; cells require [[ IL23 ]] , TGFβ , IL6 , and IL1 for their generation .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>30248</td>\n",
       "      <td>1</td>\n",
       "      <td>Altogether , &lt;&lt; Th17 &gt;&gt; cells require IL23 , [[ TGFβ ]] , IL6 , and IL1 for their generation .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>30482</td>\n",
       "      <td>0</td>\n",
       "      <td>In addition , [[ IL-6 ]] also antagonizes the IL-12– mediated differentiation of &lt;&lt; Th1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>30709</td>\n",
       "      <td>0</td>\n",
       "      <td>In this study , we demonstrate that TCF-1 is required to epigenetically maintain the [[ IL-17 ]] gene locus in a repressive state . Deletion of TCF-1 opens the IL-17 locus during T cell development , resulting in a high potential for differentiat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>30710</td>\n",
       "      <td>0</td>\n",
       "      <td>In this study , we demonstrate that TCF-1 is required to epigenetically maintain the IL-17 gene locus in a repressive state . Deletion of TCF-1 opens the [[ IL-17 ]] locus during T cell development , resulting in a high potential for differentiat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>30420</td>\n",
       "      <td>1</td>\n",
       "      <td>Interleukin ( IL)-4 is the most potent factor that causes naive CD4 + T cells to differentiate to the T helper cell ( Th ) 2 phenotype , while IL-12 and [[ interferon γ ]] trigger the differentiation of &lt;&lt; Th1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>30418</td>\n",
       "      <td>1</td>\n",
       "      <td>Interleukin ( IL)-4 is the most potent factor that causes naive CD4 + T cells to differentiate to the T helper cell ( Th ) 2 phenotype , while [[ IL-12 ]] and interferon γ trigger the differentiation of &lt;&lt; Th1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>30416</td>\n",
       "      <td>0</td>\n",
       "      <td>[[ Interleukin ( IL)-4 ]] is the most potent factor that causes naive CD4 + T cells to differentiate to the T helper cell ( Th ) 2 phenotype , while IL-12 and interferon γ trigger the differentiation of &lt;&lt; Th1 &gt;&gt; cells .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>30291</td>\n",
       "      <td>1</td>\n",
       "      <td>Here we report that &lt;&lt; TH17 &gt;&gt; cells also highly express another related nuclear receptor RORα , which is induced by [[ TGFβ ]] and IL-6 in a STAT3-dependent manner .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>30292</td>\n",
       "      <td>1</td>\n",
       "      <td>Here we report that &lt;&lt; TH17 &gt;&gt; cells also highly express another related nuclear receptor RORα , which is induced by TGFβ and [[ IL-6 ]] in a STAT3-dependent manner .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>30607</td>\n",
       "      <td>1</td>\n",
       "      <td>Regulatory DCs that express [[ IL-10 ]] , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( TGF-β ) and argininase suppress effector T cell activation and promote regulatory T cell ( &lt;&lt; Treg &gt;&gt; ) differentiation to enforce immune ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>30611</td>\n",
       "      <td>1</td>\n",
       "      <td>Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( [[ TGF-β ]] ) and argininase suppress effector T cell activation and promote regulatory T cell ( &lt;&lt; Treg &gt;&gt; ) differentiation to enforce immune ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>30609</td>\n",
       "      <td>1</td>\n",
       "      <td>Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , [[ transforming growth factor-β ]] ( TGF-β ) and argininase suppress effector T cell activation and promote regulatory T cell ( &lt;&lt; Treg &gt;&gt; ) differentiation to enforce immune ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>30606</td>\n",
       "      <td>1</td>\n",
       "      <td>Regulatory DCs that express [[ IL-10 ]] , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( TGF-β ) and argininase suppress effector T cell activation and promote &lt;&lt; regulatory T &gt;&gt; cell ( Treg ) differentiation to enforce immune ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>30608</td>\n",
       "      <td>1</td>\n",
       "      <td>Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , [[ transforming growth factor-β ]] ( TGF-β ) and argininase suppress effector T cell activation and promote &lt;&lt; regulatory T &gt;&gt; cell ( Treg ) differentiation to enforce immune ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>30610</td>\n",
       "      <td>1</td>\n",
       "      <td>Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( [[ TGF-β ]] ) and argininase suppress effector T cell activation and promote &lt;&lt; regulatory T &gt;&gt; cell ( Treg ) differentiation to enforce immune ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30176</td>\n",
       "      <td>1</td>\n",
       "      <td>Furthermore , a key inducer of both Th17 and &lt;&lt; iTreg &gt;&gt; cell differentiation , [[ transforming growth factor β ]] , repressed Gfi-1 expression , implying a reciprocal negative regulation of CD4 T cell fate determination .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30175</td>\n",
       "      <td>1</td>\n",
       "      <td>Furthermore , a key inducer of both &lt;&lt; Th17 &gt;&gt; and iTreg cell differentiation , [[ transforming growth factor β ]] , repressed Gfi-1 expression , implying a reciprocal negative regulation of CD4 T cell fate determination .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      guid  label  \\\n",
       "663  30830      1   \n",
       "667  30834      1   \n",
       "665  30832      1   \n",
       "82   30249      1   \n",
       "34   30201      0   \n",
       "32   30199      0   \n",
       "134  30301      0   \n",
       "80   30247      1   \n",
       "81   30248      1   \n",
       "315  30482      0   \n",
       "542  30709      0   \n",
       "543  30710      0   \n",
       "253  30420      1   \n",
       "251  30418      1   \n",
       "249  30416      0   \n",
       "124  30291      1   \n",
       "125  30292      1   \n",
       "440  30607      1   \n",
       "444  30611      1   \n",
       "442  30609      1   \n",
       "439  30606      1   \n",
       "441  30608      1   \n",
       "443  30610      1   \n",
       "9    30176      1   \n",
       "8    30175      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                          text  \n",
       "663                                                                   The current understanding is that [[ interleukin ( IL)-6 ]] in combination with transforming growth factor-β ( TGF-β ) leads to generation of << T helper-17 >> ( Th17 ) lineage cells .  \n",
       "667                                                                   The current understanding is that interleukin ( IL)-6 in combination with transforming growth factor-β ( [[ TGF-β ]] ) leads to generation of << T helper-17 >> ( Th17 ) lineage cells .  \n",
       "665                                                                   The current understanding is that interleukin ( IL)-6 in combination with [[ transforming growth factor-β ]] ( TGF-β ) leads to generation of << T helper-17 >> ( Th17 ) lineage cells .  \n",
       "82                                                                                                                                                              Altogether , << Th17 >> cells require IL23 , TGFβ , [[ IL6 ]] , and IL1 for their generation .  \n",
       "34   Plasticity within this subset is suggested by the existence of IL-17 secreting cells , which , can also secrete [[ interferon-γ ]] , the signature cytokine for Th1 cells or , can co - express the anti - inflammatory transcription factor forkhead ...  \n",
       "32   Plasticity within this subset is suggested by the existence of [[ IL-17 ]] secreting cells , which , can also secrete interferon-γ , the signature cytokine for Th1 cells or , can co - express the anti - inflammatory transcription factor forkhead ...  \n",
       "134                                                                                                     Furthermore , [[ IL-33 ]] controlled Bcl6 function at the chromatin level and consequently , augmented cytokine production in memory << TH2 >> cells .  \n",
       "80                                                                                                                                                              Altogether , << Th17 >> cells require [[ IL23 ]] , TGFβ , IL6 , and IL1 for their generation .  \n",
       "81                                                                                                                                                              Altogether , << Th17 >> cells require IL23 , [[ TGFβ ]] , IL6 , and IL1 for their generation .  \n",
       "315                                                                                                                                                         In addition , [[ IL-6 ]] also antagonizes the IL-12– mediated differentiation of << Th1 >> cells .  \n",
       "542  In this study , we demonstrate that TCF-1 is required to epigenetically maintain the [[ IL-17 ]] gene locus in a repressive state . Deletion of TCF-1 opens the IL-17 locus during T cell development , resulting in a high potential for differentiat...  \n",
       "543  In this study , we demonstrate that TCF-1 is required to epigenetically maintain the IL-17 gene locus in a repressive state . Deletion of TCF-1 opens the [[ IL-17 ]] locus during T cell development , resulting in a high potential for differentiat...  \n",
       "253                               Interleukin ( IL)-4 is the most potent factor that causes naive CD4 + T cells to differentiate to the T helper cell ( Th ) 2 phenotype , while IL-12 and [[ interferon γ ]] trigger the differentiation of << Th1 >> cells .  \n",
       "251                               Interleukin ( IL)-4 is the most potent factor that causes naive CD4 + T cells to differentiate to the T helper cell ( Th ) 2 phenotype , while [[ IL-12 ]] and interferon γ trigger the differentiation of << Th1 >> cells .  \n",
       "249                               [[ Interleukin ( IL)-4 ]] is the most potent factor that causes naive CD4 + T cells to differentiate to the T helper cell ( Th ) 2 phenotype , while IL-12 and interferon γ trigger the differentiation of << Th1 >> cells .  \n",
       "124                                                                                     Here we report that << TH17 >> cells also highly express another related nuclear receptor RORα , which is induced by [[ TGFβ ]] and IL-6 in a STAT3-dependent manner .  \n",
       "125                                                                                     Here we report that << TH17 >> cells also highly express another related nuclear receptor RORα , which is induced by TGFβ and [[ IL-6 ]] in a STAT3-dependent manner .  \n",
       "440  Regulatory DCs that express [[ IL-10 ]] , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( TGF-β ) and argininase suppress effector T cell activation and promote regulatory T cell ( << Treg >> ) differentiation to enforce immune ...  \n",
       "444  Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( [[ TGF-β ]] ) and argininase suppress effector T cell activation and promote regulatory T cell ( << Treg >> ) differentiation to enforce immune ...  \n",
       "442  Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , [[ transforming growth factor-β ]] ( TGF-β ) and argininase suppress effector T cell activation and promote regulatory T cell ( << Treg >> ) differentiation to enforce immune ...  \n",
       "439  Regulatory DCs that express [[ IL-10 ]] , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( TGF-β ) and argininase suppress effector T cell activation and promote << regulatory T >> cell ( Treg ) differentiation to enforce immune ...  \n",
       "441  Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , [[ transforming growth factor-β ]] ( TGF-β ) and argininase suppress effector T cell activation and promote << regulatory T >> cell ( Treg ) differentiation to enforce immune ...  \n",
       "443  Regulatory DCs that express IL-10 , programmed cell death-1 ( PD-1 ) , transforming growth factor-β ( [[ TGF-β ]] ) and argininase suppress effector T cell activation and promote << regulatory T >> cell ( Treg ) differentiation to enforce immune ...  \n",
       "9                               Furthermore , a key inducer of both Th17 and << iTreg >> cell differentiation , [[ transforming growth factor β ]] , repressed Gfi-1 expression , implying a reciprocal negative regulation of CD4 T cell fate determination .  \n",
       "8                               Furthermore , a key inducer of both << Th17 >> and iTreg cell differentiation , [[ transforming growth factor β ]] , repressed Gfi-1 expression , implying a reciprocal negative regulation of CD4 T cell fate determination .  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = np.argsort(Y_pred)\n",
    "df.iloc[o].tail(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
